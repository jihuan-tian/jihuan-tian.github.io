<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>General theory about the construction of preconditioning bilinear form in BEM | 止于至善</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="General theory about the construction of preconditioning bilinear form in BEM" />
<meta name="author" content="Jihuan Tian" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Contents  1 Inverse operator of the preconditioner  2 Orthogonal decomposition of the domain \(V^s(\Gamma ,A)\) of \(A\)  3 Define the preconditioning bilinear form  4 Discretization of the preconditioning bilinear form  5 Preconditioners for Laplace problem In BEM, the preconditioning technique is indispensable for solving a large scale PDE, when an iterative solver, such as CG, GMRES, MinRES, BiCGStab, is adopted. Without a preconditioner, the number of iterations will significantly increase with the deterioration of the system matrix’s condition number. A large condition number can be caused by a large number of DoFs, distinct mesh element size, sharp corners in the geometry, etc. The procedure for constructing the Galerkin matrix for a preconditioner is summarized into four steps. Find a preconditioning operator having an opposite order with respect to the key operator in the original PDE in the theoretical framework of pseudodifferential operators (see Pseudodifferential operator, Operator preconditioning based on pseudodifferential operator of opposite orders). In BEM, this is a trivial task thanks to the properties of boundary integral operators (see Boundary integral operators considered as pseudodifferential operators). Construct an inverse operator for the preconditioner obtained from the previous step. If the preconditioner is not elliptic on its whole domain, we should construct a Moore-Penrose pseudo inverse or generalized inverse operator instead. Alternatively, we can build an approximate operator for the preconditioner, which is elliptic on the whole domain. Then its inverse operator is available. Construct a bilinear form for the above inverse operator, which is spectrally equivalent to the bilinear form in the variational form of the PDE. The lower bound and upper bound constants in this spectral equivalence condition do not depend on the geometric discretization and finite element (function space) discretization. Discretize the above bilinear form, whose inverse matrix is to be multiplied to the discretized linear system for the variational form of the PDE. Since the inverse operator associated with this bilinear form cannot be directly evaluated, we will compute an approximate version which is based on the inverse matrix of the Galerkin matrix for the preconditioning operator. A general matrix spectral equivalence theorem guarantees the equivalence between this approximate version and the exact version. In the following, we’ll introduce the basic ideas and theoretical details. Finally, we present the preconditioners used in the Laplace problem with either Dirichlet or Neumann boundary condition. 1 Inverse operator of the preconditioner The operator \(A: V^s(\Gamma ,A):= H^s(\Gamma )_{/\mathrm {ker}(A)} \rightarrow H^{s-2\alpha }(\Gamma )\) in the original PDE is self-adjoint (in the sense of adjointness in Banach spaces), bounded and \(V^s(\Gamma ,A)\)-elliptic. As a pseudodifferential operator, its order is \(2\alpha \). The preconditioning operator \(B: H^{s-2\alpha }(\Gamma ) \rightarrow H^s(\Gamma )\) is self-adjoint, bounded and \(V^{s-2\alpha ,0}(\Gamma ,B) := H^{s-2\alpha }(\Gamma )_{/\mathrm {ker}(B)}\)-elliptic. Its order is \(-2\alpha \). Define the inverse operator \(B^{-1}: H^s(\Gamma ) \rightarrow H^{s-2\alpha }(\Gamma )\) of \(B\) which should be spectrally equivalent to \(A\). Assume \(B\) has a closed range and also note \(B\) is self-adjoint, then \begin{equation} \mathrm {Im}(B) = (\mathrm {ker}(B&#39;))^{\circ } = (\mathrm {ker}(B))^{\circ }. \end{equation} If the operator \(B\) has a non-trivial kernel and let \(H^s(\Gamma )\) be decomposed as \(H^s(\Gamma ) = (\mathrm {ker}(B))^{\circ } \oplus [(\mathrm {ker}(B))^{\circ }]^{\mathrm {c}}\), a generalized inverse \(B^{+}: H^s(\Gamma ) \rightarrow H^{s-2\alpha }(\Gamma )\) can be defined \begin{equation} B^{+} = \begin {cases} \dot {B}^{-1}(y) &amp; y\in (\mathrm {ker}(B))^{\circ } \\ 0 &amp; y\in [(\mathrm {ker}(B))^{\circ }]^{\mathrm {c}} \end {cases}, \end{equation} where \((\mathrm {ker}(B))^{\mathrm {c}}\) is the complement of \(\mathrm {ker}(B)\) within \(H^{s-2\alpha }(\Gamma )\) and \(\dot {B}^{-1}: (\mathrm {ker}(B))^{\circ } \rightarrow (\mathrm {ker}(B))^{\mathrm {c}}\) is a bijective map. Even though the above adjointness and generalized inverse are discussed in the context of Banach spaces, the Sobolev space \(H^{s-2\alpha }(\Gamma )\) is actually a Hilbert space. Therefore, the complement subspace \((\mathrm {ker}(B))^{\mathrm {c}}\) is also the orthogonal complement subspace \((\mathrm {ker}(B))^{\perp }\). The domain and range spaces of \(\dot {B}^{-1}\) are defined as (Steinbach and Wendland) \begin{equation} \begin{aligned} V^{s,0}(\Gamma ,B) &amp;:= (\mathrm {ker}(B))^{\circ } = \left \{ v\in H^s(\Gamma ): \langle v,v_p \rangle _{H^{s-\alpha }(\Gamma )} = 0 \right \} \\ V^{s-2\alpha ,0}(\Gamma ,B) &amp;:= (\mathrm {ker}(B))^{\perp } = \left \{ v\in H^{s-2\alpha }(\Gamma ): \langle v,v_p \rangle _{H^{s-2\alpha }(\Gamma )}=0 \right \} \end{aligned} \quad \forall v_p\in \mathrm {ker}(B). \end{equation} To simultaneously ensure \(V^s(\Gamma ,A)\)-ellipticity of \(A\) and \(V^{s,0}(\Gamma ,B)\)-ellipticity of \(\dot {B}^{-1}\) which are required by the spectral equivalence condition, the effective domain for \(A\) and \(\dot {B}^{-1}\) should be the intersection \(V^s(\Gamma ,A) \cap V^{s,0}(\Gamma ,B)\). 2 Orthogonal decomposition of the domain \(V^s(\Gamma ,A)\) of \(A\) \(V^s(\Gamma ,A)\) is a subspace of \(H^s(\Gamma )\). \(H^s(\Gamma )\) is a Hilbert space and has orthogonal decomposition with respect to its subspace \(V^{s,0}(\Gamma ,B)\): \begin{equation} H^s(\Gamma )=V^{s,0}(\Gamma ,B) \oplus [ V^{s,0}(\Gamma ,B) ]^{\perp }. \end{equation} As defined above, \(V^{s,0}(\Gamma ,B)\) is the annihilator of \(\mathrm {ker}(B)\) in the sense of duality pairing, which can be considered as a generalization of the concept of orthogonality (based on inner product) in a Hilbert space. Its orthogonal complement subspace \([ V^{s,0}(\Gamma ,B) ]^{\perp }\) can then be considered as the counterpart of \(\mathrm {ker}(B) \subset H^{s-2\alpha }(\Gamma )\) but in the dual space \(H^s(\Gamma )\). For any \(u(x)\in V^s(\Gamma ,A)\), it can be uniquely decomposed as \begin{equation} u(x) = u_0(x) + u_1(x), \end{equation} where \(u_0(x)\in V^{s,0}(\Gamma ,B)\) and \(u_1(x)\in [ V^{s,0}(\Gamma ,B) ]^{\perp }\). Obviously, \(u_0(x)\in V^s(\Gamma ,A) \cap V^{s,0}(\Gamma ,B)\). Because \([ V^{s,0}(\Gamma ,B) ]^{\perp }\) is the counterpart of \(\mathrm {ker}(B)\) in \(H^s(\Gamma )\), if we let \(\{ v_p \}_{p=1}^m\) be the orthonormal basis of \(\mathrm {ker}(B)\), \(u_1(x)\) can be constructed by “projecting” \(u(x)\) to the basis \(\{ v_p \}_{p=1}^m\) in the sense of duality pairing as below. \begin{equation} u_1(x)=\sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} (\mathcal {J}^{-2\alpha }v_p)(x). \end{equation} In this representation, \(\langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}\) is the expansion coefficient and \(\mathcal {J}^{-2\alpha }v_p\) is the basis element for \([ V^{s,0}(\Gamma ,B) ]^{\perp }\) in \(H^s(\Gamma )\), which is the counterpart of the basis element \(v_p\) for \(\mathrm {ker}(B)\) in \(H^{s-2\alpha }(\Gamma )\). \(\mathcal {J}: \mathcal {S}(\mathbb {R}^d) \rightarrow \mathcal {S}(\mathbb {R}^d)\) is the Bessel potential operator on rapidly decreasing functions. \(\mathcal {J}\) with an order \(s\) is defined as (Steinbach, page 32) \begin{equation} \label {eq:bessel-potential-op} \mathcal {J}^su(x) := ( 2\pi )^{-d/2} \int _{\mathbb {R}^d} ( 1+\lvert \xi \rvert ^2 )^{s/2} \hat {u}(\xi ) \mathrm {e}^{\mathrm {i}\langle x,\xi \rangle } \mathrm {d}\xi , \end{equation} where \(\hat {u}(\xi )\) is the Fourier transform of \(u(x)\). Obviously, the effect of \(\mathcal {J}^s\) on \(u(x)\) is modifying its frequency spectrum by multiplying a polynomial about \(\xi \) with an order \(s\). According to the pseudodifferential operator theory, \(\mathcal {J}^s\) is equivalent to a partial differential operator with an order \(s\). If we restrict the domain of \(\mathcal {J}^s\) to a smaller space instead of \(\mathcal {S}(\mathbb {R}^d)\), such as the Sobolev space \(H^t(\Gamma )\), \(\mathcal {J}^s\) maps from \(H^t(\Gamma )\) to \(H^{t-s}(\Gamma )\), which lowers the Sobolev space order by \(s\). In the above representation for \(u_1(x)\), \(\mathcal {J}^{-2\alpha }\) maps from \(H^{s-2\alpha }(\Gamma )\) to \(H^s(\Gamma )\). According to (McLean, page 75), for any \(s,t\in \mathbb {R}\) and \(u,v\in \mathcal {S}(\mathbb {R}^d)\), Bessel polynomial operators have these properties: \(\mathcal {J}^{s+t} = \mathcal {J}^s \mathcal {J}^t\) \(( \mathcal {J}^s )^{-1} = \mathcal {J}^{-s}\) \(\mathcal {J}^0 = \text {identity operator}\) \(\langle \mathcal {J}^s u, v \rangle =\langle u,\mathcal {J}^s v \rangle \) \(\langle \cdot ,\cdot \rangle \) can be either considered as an \(L_2\) inner product of two functions in \(\mathcal {S}(\mathbb {R}^d)\) or as applying a tempered distribution in \(\mathcal {S}&#39;(\mathbb {R}^d)\) to a rapidly decreasing function in \(\mathcal {S}(\mathbb {R}^d)\). For example, we can treat \(u\) as a tempered distribution whose kernel function is \(u\). Then \(\mathcal {J}^s\) in \(\langle \mathcal {J}^s u,v \rangle \) is the Bessel potential operator on \(\mathcal {S}&#39;(\mathbb {R}^d)\) and \(\mathcal {J}^s\) in \(\langle u,\mathcal {J}^s v \rangle \) is the Bessel potential operator on \(\mathcal {S}(\mathbb {R})^d\). The Sobolev space \(H^s(\mathbb {R}^d)\) is defined as (Steinbach, page 73) \begin{equation} H^s(\mathbb {R}^d) := \left \{ v\in \mathcal {S}&#39;(\mathbb {R}^d): \mathcal {J}^sv \in L_2(\mathbb {R}^d) \right \}. \end{equation} The inner product is for any \(u,v\in H^s(\mathbb {R}^d)\), \begin{equation} \langle u,v \rangle _{H^s(\mathbb {R}^d)} := \langle \mathcal {J}^su, \mathcal {J}^sv \rangle _{L_2(\mathbb {R}^d)}. \end{equation} The norm definition is \begin{equation} \lVert u \rVert _{H^s(\mathbb {R}^d)} := \lVert \mathcal {J}^su \rVert _{L_2(\mathbb {R}^d)}. \end{equation} From the above, we also have \begin{equation} \langle u,v \rangle _{H^s(\mathbb {R}^d)} = \langle \mathcal {J}^su, \mathcal {J}^sv \rangle _{L_2(\mathbb {R}^d)} = \langle \mathcal {J}^s \mathcal {J}^s u,v \rangle = \langle \mathcal {J}^{2s}u,v \rangle . \end{equation} The above \(\mathcal {J}^{-2\alpha }\) is actually the Riesz map from \(H^{s-2\alpha }(\Gamma )\) to \(H^s(\Gamma )\), which satisfies \begin{equation} \langle \mathcal {J}^{-2\alpha }u,v \rangle _{H^{s-\alpha }(\Gamma )} = \langle u,v \rangle _{H^{s-2\alpha }(\Gamma )} \quad \forall u,v\in H^{s-2\alpha }(\Gamma ). \end{equation} This can be proved by using the properties of \(\mathcal {J}\). The left hand side of the above equation is \begin{equation} \langle \mathcal {J}^{-2\alpha }u,v \rangle _{H^{s-\alpha }(\Gamma )} = \langle \mathcal {J}^{-\alpha }u, \mathcal {J}^{-\alpha }v \rangle _{H^{s-\alpha }(\Gamma )}, \end{equation} where \(\langle \cdot ,\cdot \rangle _{H^{s-\alpha }(\Gamma )}\) on the left hand side is the duality pairing between \(H^s(\Gamma )\) and \(H^{s-2\alpha }(\Gamma )\), while on the right hand side it is the inner product in \(H^{s-\alpha }(\Gamma )\). Because both \(\mathcal {J}^{-\alpha }u\) and \(\mathcal {J}^{-\alpha }v\) belong to \(H^{s-\alpha }(\Gamma )\), according to the norm definition, \begin{equation} \langle \mathcal {J}^{-\alpha }u, \mathcal {J}^{-\alpha }v \rangle = \langle \mathcal {J}^{s-\alpha }\mathcal {J}^{-\alpha }u, \mathcal {J}^{s-\alpha }\mathcal {J}^{-\alpha }v \rangle _{L_2(\Gamma )} = \langle \mathcal {J}^{s-2\alpha }u,\mathcal {J}^{s-2\alpha }v \rangle _{L_2(\Gamma )}. \end{equation} This is just the same as \(\langle u,v \rangle _{H^{s-2\alpha }(\Gamma )}\). We can also show that \(u_0(x)\) is orthogonal to \(u_1(x)\). \begin{equation} \begin{aligned} \langle u_0,u_1 \rangle _{H^s(\Gamma )} &amp;= \langle u_0, \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \mathcal {J}^{-2\alpha }v_p \rangle _{H^s(\Gamma )} \\ &amp;= \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle u_0,\mathcal {J}^{-2\alpha }v_p \rangle _{H^s(\Gamma )} \\ &amp;= \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle \mathcal {J}^su_0,\mathcal {J}^s \mathcal {J}^{-2\alpha }v_p \rangle _{L_2(\Gamma )} \\ &amp;= \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle \mathcal {J}^su_0,\mathcal {J}^{s-2\alpha }v_p \rangle _{L_2(\Gamma )}, \end{aligned} \end{equation} where \begin{equation} \langle \mathcal {J}^su_0,\mathcal {J}^{s-2\alpha }v_p \rangle _{L_2(\Gamma )} = \langle u_0,v_p \rangle _{H^{s-\alpha }(\Gamma )}. \end{equation} Because \(u_0\in V^{s,0}(\Gamma ,B)\), which is the annihilator of \(\mathrm {ker}(B)\), the above expression is zero. 3 Define the preconditioning bilinear form Now we define the preconditioning bilinear form \(c(u,w)\), which is spectrally equivalent to \(a(u,w)=\langle Au,w \rangle \) &lt;ul class=&#39;itemize1&#39;&gt; &lt;li class=&#39;itemize&#39;&gt; &lt;!-- l. 169 --&gt;&lt;p class=&#39;noindent&#39;&gt;Method 1: Directly construct \(c(u,w)\) based on the generalized inverse \(B^{+}\). &lt;/p&gt;&lt;!-- l. 171 --&gt;&lt;p class=&#39;noindent&#39;&gt;&lt;span class=&#39;p1xb-x-x-109&#39;&gt;Basic idea&lt;/span&gt;: use the above orthogonal decomposition and construct the bilinear form \(c(u,u)\), which is spectrally equivalent to \(a(u,u)=\langle Au,u \rangle \). &lt;/p&gt;&lt;!-- l. 173 --&gt;&lt;p class=&#39;noindent&#39;&gt;Use the boundedness of \(a(\cdot ,\cdot )\) or \(A\), for all \(u\in V^s(\Gamma ,A)\) \begin{equation*} \langle Au,u \rangle _{H^{s-\alpha }(\Gamma )} \leq c_2^A \lVert u \rVert _{H^s(\Gamma )}^2 \end{equation*} Substitute the orthogonal decomposition \(u=u_0+u_1\) \begin{equation*} = c_2^A \lVert u_0 + u_1 \rVert _{H^s(\Gamma )}^2 \end{equation*} Because we are dealing with Hilbert space, \begin{equation*} = c_2^A (\langle u_0,u_0 \rangle + \langle u_0,u_1 \rangle + \langle u_1,u_0 \rangle + \langle u_1,u_1 \rangle ) \end{equation*} Because \(u_0\) is orthogonal to \(u_1\), \begin{align*} &amp;amp;= c_2^A (\lVert u_0 \rVert _{H^s(\Gamma )}^2 + \lVert u_1 \rVert _{H^s(\Gamma )}^2) \\ &amp;amp;= c_2^A \left ( \lVert u_0 \rVert _{H^s(\Gamma )}^2 + \left \langle \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \mathcal {J}^{-2\alpha }v_p, \sum _{q=1}^m \langle u,v_q \rangle _{H^{s-\alpha }(\Gamma )} \mathcal {J}^{-2\alpha }v_q \right \rangle \right ) \\ &amp;amp;= c_2^A \left ( \lVert u_0 \rVert _{H^s(\Gamma )}^2 + \sum _{p=1}^m\sum _{q=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle u,v_q \rangle _{H^{s-\alpha }(\Gamma )} \langle \mathcal {J}^{-2\alpha }v_p, \mathcal {J}^{-2\alpha }v_q \rangle _{H^s(\Gamma )} \right ) \\ &amp;amp;= c_2^A \left ( \lVert u_0 \rVert _{H^s(\Gamma )}^2 + \sum _{p=1}^m\sum _{q=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle u,v_q \rangle _{H^{s-\alpha }(\Gamma )} \langle v_p,v_q \rangle _{H^{s-2\alpha }(\Gamma )} \right ) \end{align*} &lt;/p&gt;&lt;!-- l. 201 --&gt;&lt;p class=&#39;noindent&#39;&gt;Because \(\{ v_p \}_{p=1}^m\) is an orthonormal basis, \begin{equation*} = c_2^A \left ( \lVert u_0 \rVert _{H^s(\Gamma )}^2 + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right ) \end{equation*} Use the \(V^{s,0}(\Gamma,B)\)-ellipticity of \(\dot {B}^{-1}\) and the reciprocal relationship between its spectrum and that of \(B\), \begin{align*} &amp;amp;\leq c_2^A \left ( c_2^B \langle \dot {B}^{-1}u_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right ) \\ &amp;amp;\leq c_2^A \max \{ c_2^B,1 \} \left ( \langle \dot {B}^{-1}u_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{k=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right ). \end{align*} &lt;/p&gt;&lt;!-- l. 214 --&gt;&lt;p class=&#39;noindent&#39;&gt;If we define the bilinear form \(c(u,w)\) as \begin{equation} \label {eq:bilinear-form-c} \begin{aligned} c(u,w) &amp;amp;:= \langle B^{\dagger }u,w \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle w,v_p \rangle _{H^{s-\alpha }(\Gamma )} \\ &amp;amp;= \langle \dot {B}^{-1}u_0,w_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle w,v_p \rangle _{H^{s-\alpha }(\Gamma )} \quad \forall u,w\in H^s(\Gamma ) \end{aligned}, \end{equation}&lt;a id=&#39;x1-4001r17&#39;&gt;&lt;/a&gt; it satisfies the upper part of the spectral equivalence condition \begin{equation} \langle Au,u \rangle _{H^{s-\alpha }(\Gamma )} \leq c_2^A \max \{ c_2^{B},1 \} c(u,u). \end{equation}&lt;a id=&#39;x1-4002r18&#39;&gt;&lt;/a&gt; &lt;/p&gt;&lt;!-- l. 230 --&gt;&lt;p class=&#39;noindent&#39;&gt;Then we prove the lower part of the spectral equivalence condition. Use the \(V^s(\Gamma,A)\)-ellipticity of \(A\), for all \(u\in V^s(\Gamma ,A)\) \begin{equation*} \langle Au,u \rangle _{H^{s-\alpha }(\Gamma )} \geq c_1^A \lVert u \rVert _{H^s(\Gamma )}^2 \end{equation*} Substitute the orthogonal decomposition \(u=u_0+u_1\) \begin{align*} &amp;amp;= c_1^A \lVert u_0+u_1 \rVert _{H^s(\Gamma )}^2 \\ &amp;amp;= c_1^A \left ( \lVert u_0 \rVert _{H^s(\Gamma )}^2 + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right ) \end{align*} &lt;/p&gt;&lt;!-- l. 240 --&gt;&lt;p class=&#39;noindent&#39;&gt;Use the boundedness of \(\dot {B}^{-1}\) and the reciprocal relationship between its spectrum and that of \(B\) \begin{align*} &amp;amp;\geq c_1^A \left ( c_1^B \langle \dot {B}^{-1}u_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right ) \\ &amp;amp;\geq c_1^A \min \{ c_1^B, 1 \} \left ( \langle \dot {B}^{-1}u_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right ). \end{align*} &lt;/p&gt;&lt;!-- l. 248 --&gt;&lt;p class=&#39;noindent&#39;&gt;Therefore, we have \begin{equation} c_1^A \min \{ c_1^B,1 \} c(u,u) \leq \langle Au,u \rangle _{H^{s-\alpha }(\Gamma )}. \end{equation}&lt;a id=&#39;x1-4003r19&#39;&gt;&lt;/a&gt; &lt;/p&gt;&lt;!-- l. 253 --&gt;&lt;p class=&#39;noindent&#39;&gt;In these two special cases &lt;/p&gt;&lt;ol class=&#39;enumerate1&#39;&gt; when \(V^s(\Gamma ,A) \subseteq V^{s,0}(\Gamma ,B)\), the orthogonal decomposition of \(u\) degenerates to \(u=u_0\), \(\mathrm {ker}(B) = \{ 0 \}\) &lt;/ol&gt; &lt;!-- l. 258 --&gt;&lt;p class=&#39;noindent&#39;&gt;the bilinear form \(c(u,u)\) degenerates to \(\langle \dot {B}^{-1}u,u \rangle \). &lt;/p&gt;&lt;/li&gt; &lt;li class=&#39;itemize&#39;&gt; &lt;!-- l. 260 --&gt;&lt;p class=&#39;noindent&#39;&gt;Method 2: Build an approximate operator \(\tilde {B}\) for \(B\), which is elliptic on the whole domain \(H^{s-2\alpha }(\Gamma )\). Then use its inverse to build the bilinear form \(c(u,w) = \langle \tilde {B}^{-1}u,w \rangle _{H^{s-\alpha }(\Gamma )}\). &lt;/p&gt;&lt;!-- l. 262 --&gt;&lt;p class=&#39;noindent&#39;&gt;For any \(u\in H^{s-2\alpha }(\Gamma )\), it also has an orthogonal decomposition in \(V^{s-2\alpha }(\Gamma ,B) \oplus \mathrm {ker}(B)\) as \begin{equation} u = u_0 + u_1 = u_0 + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )} v_p(x), \end{equation}&lt;a id=&#39;x1-4008r20&#39;&gt;&lt;/a&gt; where the inner product \(\langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}\) is the expansion coefficient for \(u_1\) and \(v_p\) is the corresponding basis element. Then we try to construct \(\tilde {B}\) which is \(H^{s-2\alpha }(\Gamma )\)-elliptic. \begin{align*} \lVert u \rVert _{H^{s-2\alpha }(\Gamma )}^2 &amp;amp;= \lVert u_0 + u_1 \rVert _{H^{s-2\alpha }(\Gamma )}^2 \\ &amp;amp;= \left ( \lVert u_0 \rVert _{H^{s-2\alpha }(\Gamma )}^2 + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ) \end{align*} &lt;/p&gt;&lt;!-- l. 273 --&gt;&lt;p class=&#39;noindent&#39;&gt;Use the \(V^{s-2\alpha}(\Gamma,B)\)-ellipticity of \(B\) \begin{align*} &amp;amp;\leq \left ( \frac {1}{c_1^B} \langle Bu_0, u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ) \\ &amp;amp;\leq \max \{ \frac {1}{c_1^B},1 \} \left ( \langle Bu_0, u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ). \end{align*} &lt;/p&gt;&lt;!-- l. 281 --&gt;&lt;p class=&#39;noindent&#39;&gt;It is equivalent to \begin{equation} c_1^{\tilde {B}} \lVert u \rVert _{H^{s-2\alpha }(\Gamma )}^2 \leq \left ( \langle Bu_0, u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ), \end{equation}&lt;a id=&#39;x1-4009r21&#39;&gt;&lt;/a&gt; where \(c_1^{\tilde {B}} = 1/\max \{ \frac {1}{c_1^B},1 \} = \min \{ c_1^B,1 \}\). &lt;/p&gt;&lt;!-- l. 289 --&gt;&lt;p class=&#39;noindent&#39;&gt;On the other hand, we start from the boundedness of \(B\): \begin{align*} \lVert u \rVert _{H^{s-2\alpha }(\Gamma )}^2 &amp;amp;= \lVert u_0 + u_1 \rVert _{H^{s-2\alpha }(\Gamma )}^2 \\ &amp;amp;= \left ( \lVert u_0 \rVert _{H^{s-2\alpha }(\Gamma )}^2 + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ) \\ &amp;amp;\geq \left ( \frac {1}{c_2^B} \langle Bu_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_0 \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ) \\ &amp;amp;\geq \min \{ \frac {1}{c_2^B},1 \} \left ( \langle Bu_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_0 \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ). \end{align*} &lt;/p&gt;&lt;!-- l. 300 --&gt;&lt;p class=&#39;noindent&#39;&gt;So we have \begin{equation} \left ( \langle Bu_0, u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ) \leq c_2^{\tilde {B}} \lVert u \rVert _{H^{s-2\alpha }(\Gamma )}^2, \end{equation}&lt;a id=&#39;x1-4010r22&#39;&gt;&lt;/a&gt; where \(c_2^{\tilde {B}} = \max \{ c_2^B,1 \}\). &lt;/p&gt;&lt;!-- l. 309 --&gt;&lt;p class=&#39;noindent&#39;&gt;Therefore, we can define the bilinear form related to \(\tilde {B}\) as \begin{equation} \label {eq:bilinear-form-b} \begin{aligned} \langle \tilde {B}u,w \rangle _{H^{s-\alpha }(\Gamma )} &amp;amp;= \langle Bu,w \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle v,v_p \rangle _{H^{s-2\alpha }(\Gamma )} \langle w,v_p \rangle _{H^{s-2\alpha }(\Gamma )} \\ &amp;amp;= \langle Bu_0,w_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle v,v_p \rangle _{H^{s-2\alpha }(\Gamma )} \langle w,v_p \rangle _{H^{s-2\alpha }(\Gamma )} \quad \forall u,w\in H^{s-2\alpha }(\Gamma ). \end{aligned} \end{equation}&lt;a id=&#39;x1-4011r23&#39;&gt;&lt;/a&gt; It is both bounded and \(H^{s-2\alpha }(\Gamma )\)-elliptic. &lt;/p&gt;&lt;!-- l. 323 --&gt;&lt;p class=&#39;noindent&#39;&gt;The differences between the bilinear forms in Equation (&lt;a href=&#39;#x1-4001r17&#39;&gt;17&lt;!-- tex4ht:ref: eq:bilinear-form-c --&gt;&lt;/a&gt;) and (&lt;a href=&#39;#x1-4011r23&#39;&gt;23&lt;!-- tex4ht:ref: eq:bilinear-form-b --&gt;&lt;/a&gt;) are: &lt;/p&gt;&lt;ol class=&#39;enumerate1&#39;&gt; In Equation (23), \(u\) and \(w\) are projected to \(v_p\) via inner product, which is a true projection. In Equation (17), \(u\) and \(w\) are in a different space from \(v_p\). They are “projected” to \(v_p\) via duality pairing, which can be considered as a generalization of projection. In Equation (23), the basis used for function expansion is \(\{ v_p \}_{p=1}^m\). In Equation (17), the basis is \(\{ \mathcal {J}v_p \}_{p=1}^m\). &lt;/ol&gt; &lt;/li&gt;&lt;/ul&gt; &lt;p class=&#39;noindent&#39;&gt; &lt;/p&gt; 4 Discretization of the preconditioning bilinear form According to (Steinbach and Wendland, section 4), Method 1 in Section 3 requires coordinate transformation between the finite element basis and the orthonormal basis adopted for the orthogonal decomposition \(V_h^s(\Gamma ,A) = V_h^0 \oplus \{ \mathcal {J}v_p \}_{p=1}^m\), which is more complicated than Method 2. Meanwhile, (Steinbach, section 13.2 page 299) only introduces Method 2. So we stick to this method. Usually, the inverse operator \(\tilde {B}^{-1}\) cannot be directly evaluated or discretized. We need to compute the discretized matrix of \(\tilde {B}\) first, then use its inverse matrix to compute an approximation to the discretized matrix for the inverse operator \(\tilde {B}^{-1}\). This can be visualized in the following diagram. Therefore, we have \begin{equation} \underline {\tilde {B}^{-1}} \approx \mathcal {M}_{\tilde {B}}^{\mathrm {T}} \tilde {\mathcal {B}}^{-1} \mathcal {M}_{\tilde {B}}. \end{equation} The matrix to be multiplied to both sides of the equation is \begin{equation} \mathcal {M}_{\tilde {B}}^{-1} \tilde {\mathcal {B}} \mathcal {M}_{\tilde {B}}^{-\mathrm {T}}. \end{equation} The spectral equivalence between the above two matrices \(\underline {\tilde {B}^{-1}}\) and \(\mathcal {M}_{\tilde {B}}^{\mathrm {T}} \tilde {\mathcal {B}}^{-1} \mathcal {M}_{\tilde {B}}\) can be proved using the theory given in (Steinbach and Wendland, section 3). This theory is described in a general form as below. \(V\) and \(W\) are two Hilbert spaces. \(A: V \rightarrow V&#39;\) is a \(V\)-elliptic, self-adjoint and bounded operator, while \(B: W \rightarrow V&#39;\) is any bounded operator. Define an operator \(T: W \rightarrow W&#39;\) as \(T = B&#39; A^{-1} B\). In principle, it has a Galerkin matrix \(\mathcal {T}\). In reality, this matrix cannot be directly computed. The Galerkin matrices for \(A\) and \(B\) are \(\mathcal {A}\) and \(\mathcal {B}\) respectively. Then we have another matrix \begin{equation} \tilde {\mathcal {T}} = \mathcal {B}^{\mathrm {T}} \mathcal {A}^{-1} \mathcal {B}. \end{equation} It can be proved that for any \(w_h\in W_h\), \begin{equation} ( \tilde {\mathcal {T}} \underline {w}, \underline {w} ) \leq ( \mathcal {T} \underline {w}, \underline {w} ). \end{equation} If the stability condition or \(\inf \sup \) condition is satisfied \begin{equation} \inf _{0\neq w_h\in W_h} \sup _{0\neq v_h\in V_h} \frac {\lvert \langle Bw_h,v_h \rangle _{V} \rvert }{\lVert w_h \rVert _W \lVert v_h \rVert _V} \geq c, \end{equation} we also have \begin{equation} \gamma ( \mathcal {T} \underline {w}, \underline {w} ) \leq ( \tilde {\mathcal {T}} \underline {w}, \underline {w} ). \end{equation} Therefore, \(\mathcal {T}\) and \(\tilde {\mathcal {T}}\) are spectrally equivalent. The relationship between the spaces, operators and matrices are shown below. Coming back to the spectral equivalence between the two matrices \(\underline {\tilde {B}^{-1}}\) and \(\mathcal {M}_{\tilde {B}}^{\mathrm {T}} \tilde {\mathcal {B}}^{-1} \mathcal {M}_{\tilde {B}}\), we can see that it is the special case when \(W\) is selected to be \(V&#39;\) and \(B\) is the identity operator on \(V&#39;\). The mass matrix \(M_A\) for the duality pairing from \(V&#39;\) to \(V\) is just the Galerkin matrix of \(B\). This is shown in the following figure. 5 Preconditioners for Laplace problem The operator equation for the Laplace problem with Dirichlet boundary condition is \begin{equation} \langle Vt, \tau \rangle _{\Gamma } = \langle (\frac {1}{2}I + K)g, \tau \rangle _{\Gamma } \quad \forall \tau \in H^{-1/2}(\Gamma ). \end{equation} The hypersingular operator \(D\) has an opposite order with respect to \(V\), which is naturally the preconditioner for this equation. However, according to here, \(D\) is not \(H^{1/2}(\Gamma )\)-elliptic, but only \(H_{\ast }^{1/2}(\Gamma )\)-elliptic or \(H_{\ast \ast }^{1/2}(\Gamma )\)-elliptic depending on which kind of inner product is desired. To avoid the complex of computing the natural density \(w_{\mathrm {eq}}\), we choose \(H_{\ast \ast }^{1/2}(\Gamma )\)-ellipticity. Then the bilinear form associated with the regularized or gauged operator \(\tilde {D}\) is \begin{equation} \langle \tilde {D}u,w \rangle = \langle Du,w \rangle + \langle u,1 \rangle _{\Gamma } \langle w,1 \rangle _{\Gamma } \quad \forall u,w\in H^{1/2}(\Gamma ). \end{equation} Then the inverse of the preconditioning matrix is \begin{equation} \mathcal {M}_{\tilde {D}}^{-1} \tilde {\mathcal {D}} \mathcal {M}_{\tilde {D}}^{-\mathrm {T}}, \end{equation} where \(\mathcal {M}_{\tilde {D}}\) is the mass matrix for the duality pairing from \(H^{-1/2}(\Gamma )\) to \(H^{1/2}(\Gamma )\). The operator equation for the Laplace problem with Neumann boundary condition is \begin{equation} \langle Du, v \rangle _{\Gamma } + \alpha \langle u,1 \rangle _{\Gamma } \langle v,1 \rangle _{\Gamma } = \langle (\frac {1}{2}I - K&#39;)g,v \rangle _{\Gamma } \quad \forall v\in H^{1/2}(\Gamma ). \end{equation} The single layer potential operator \(V\) has an opposite order with respect to \(D\), which is the preconditioner. When \(d=3\), \(V\) is \(H^{-1/2}(\Gamma )\)-elliptic, so we directly obtain the inverse of the preconditioning matrix \begin{equation} \mathcal {M}_V^{-1} \mathcal {V} \mathcal {M}_V^{-\mathrm {T}}, \end{equation} where \(\mathcal {M}_V\) is the mass matrix for the duality pairing from \(H^{1/2}(\Gamma )\) to \(H^{-1/2}(\Gamma )\). References    William Charles Hector McLean. Strongly Elliptic Systems and Boundary Integral Equations. Cambridge University Press. ISBN 978-0-521-66375-5.    Olaf Steinbach. Numerical Approximation Methods for Elliptic Boundary Value Problems: Finite and Boundary Elements. Springer Science &amp; Business Media. ISBN 978-0-387-31312-2.    Olaf Steinbach and Wolfgang L. Wendland. The construction of some efficient preconditioners in the boundary element method. 9(1-2):191–216. URL http://link.springer.com/article/10.1023/A:1018937506719." />
<meta property="og:description" content="Contents  1 Inverse operator of the preconditioner  2 Orthogonal decomposition of the domain \(V^s(\Gamma ,A)\) of \(A\)  3 Define the preconditioning bilinear form  4 Discretization of the preconditioning bilinear form  5 Preconditioners for Laplace problem In BEM, the preconditioning technique is indispensable for solving a large scale PDE, when an iterative solver, such as CG, GMRES, MinRES, BiCGStab, is adopted. Without a preconditioner, the number of iterations will significantly increase with the deterioration of the system matrix’s condition number. A large condition number can be caused by a large number of DoFs, distinct mesh element size, sharp corners in the geometry, etc. The procedure for constructing the Galerkin matrix for a preconditioner is summarized into four steps. Find a preconditioning operator having an opposite order with respect to the key operator in the original PDE in the theoretical framework of pseudodifferential operators (see Pseudodifferential operator, Operator preconditioning based on pseudodifferential operator of opposite orders). In BEM, this is a trivial task thanks to the properties of boundary integral operators (see Boundary integral operators considered as pseudodifferential operators). Construct an inverse operator for the preconditioner obtained from the previous step. If the preconditioner is not elliptic on its whole domain, we should construct a Moore-Penrose pseudo inverse or generalized inverse operator instead. Alternatively, we can build an approximate operator for the preconditioner, which is elliptic on the whole domain. Then its inverse operator is available. Construct a bilinear form for the above inverse operator, which is spectrally equivalent to the bilinear form in the variational form of the PDE. The lower bound and upper bound constants in this spectral equivalence condition do not depend on the geometric discretization and finite element (function space) discretization. Discretize the above bilinear form, whose inverse matrix is to be multiplied to the discretized linear system for the variational form of the PDE. Since the inverse operator associated with this bilinear form cannot be directly evaluated, we will compute an approximate version which is based on the inverse matrix of the Galerkin matrix for the preconditioning operator. A general matrix spectral equivalence theorem guarantees the equivalence between this approximate version and the exact version. In the following, we’ll introduce the basic ideas and theoretical details. Finally, we present the preconditioners used in the Laplace problem with either Dirichlet or Neumann boundary condition. 1 Inverse operator of the preconditioner The operator \(A: V^s(\Gamma ,A):= H^s(\Gamma )_{/\mathrm {ker}(A)} \rightarrow H^{s-2\alpha }(\Gamma )\) in the original PDE is self-adjoint (in the sense of adjointness in Banach spaces), bounded and \(V^s(\Gamma ,A)\)-elliptic. As a pseudodifferential operator, its order is \(2\alpha \). The preconditioning operator \(B: H^{s-2\alpha }(\Gamma ) \rightarrow H^s(\Gamma )\) is self-adjoint, bounded and \(V^{s-2\alpha ,0}(\Gamma ,B) := H^{s-2\alpha }(\Gamma )_{/\mathrm {ker}(B)}\)-elliptic. Its order is \(-2\alpha \). Define the inverse operator \(B^{-1}: H^s(\Gamma ) \rightarrow H^{s-2\alpha }(\Gamma )\) of \(B\) which should be spectrally equivalent to \(A\). Assume \(B\) has a closed range and also note \(B\) is self-adjoint, then \begin{equation} \mathrm {Im}(B) = (\mathrm {ker}(B&#39;))^{\circ } = (\mathrm {ker}(B))^{\circ }. \end{equation} If the operator \(B\) has a non-trivial kernel and let \(H^s(\Gamma )\) be decomposed as \(H^s(\Gamma ) = (\mathrm {ker}(B))^{\circ } \oplus [(\mathrm {ker}(B))^{\circ }]^{\mathrm {c}}\), a generalized inverse \(B^{+}: H^s(\Gamma ) \rightarrow H^{s-2\alpha }(\Gamma )\) can be defined \begin{equation} B^{+} = \begin {cases} \dot {B}^{-1}(y) &amp; y\in (\mathrm {ker}(B))^{\circ } \\ 0 &amp; y\in [(\mathrm {ker}(B))^{\circ }]^{\mathrm {c}} \end {cases}, \end{equation} where \((\mathrm {ker}(B))^{\mathrm {c}}\) is the complement of \(\mathrm {ker}(B)\) within \(H^{s-2\alpha }(\Gamma )\) and \(\dot {B}^{-1}: (\mathrm {ker}(B))^{\circ } \rightarrow (\mathrm {ker}(B))^{\mathrm {c}}\) is a bijective map. Even though the above adjointness and generalized inverse are discussed in the context of Banach spaces, the Sobolev space \(H^{s-2\alpha }(\Gamma )\) is actually a Hilbert space. Therefore, the complement subspace \((\mathrm {ker}(B))^{\mathrm {c}}\) is also the orthogonal complement subspace \((\mathrm {ker}(B))^{\perp }\). The domain and range spaces of \(\dot {B}^{-1}\) are defined as (Steinbach and Wendland) \begin{equation} \begin{aligned} V^{s,0}(\Gamma ,B) &amp;:= (\mathrm {ker}(B))^{\circ } = \left \{ v\in H^s(\Gamma ): \langle v,v_p \rangle _{H^{s-\alpha }(\Gamma )} = 0 \right \} \\ V^{s-2\alpha ,0}(\Gamma ,B) &amp;:= (\mathrm {ker}(B))^{\perp } = \left \{ v\in H^{s-2\alpha }(\Gamma ): \langle v,v_p \rangle _{H^{s-2\alpha }(\Gamma )}=0 \right \} \end{aligned} \quad \forall v_p\in \mathrm {ker}(B). \end{equation} To simultaneously ensure \(V^s(\Gamma ,A)\)-ellipticity of \(A\) and \(V^{s,0}(\Gamma ,B)\)-ellipticity of \(\dot {B}^{-1}\) which are required by the spectral equivalence condition, the effective domain for \(A\) and \(\dot {B}^{-1}\) should be the intersection \(V^s(\Gamma ,A) \cap V^{s,0}(\Gamma ,B)\). 2 Orthogonal decomposition of the domain \(V^s(\Gamma ,A)\) of \(A\) \(V^s(\Gamma ,A)\) is a subspace of \(H^s(\Gamma )\). \(H^s(\Gamma )\) is a Hilbert space and has orthogonal decomposition with respect to its subspace \(V^{s,0}(\Gamma ,B)\): \begin{equation} H^s(\Gamma )=V^{s,0}(\Gamma ,B) \oplus [ V^{s,0}(\Gamma ,B) ]^{\perp }. \end{equation} As defined above, \(V^{s,0}(\Gamma ,B)\) is the annihilator of \(\mathrm {ker}(B)\) in the sense of duality pairing, which can be considered as a generalization of the concept of orthogonality (based on inner product) in a Hilbert space. Its orthogonal complement subspace \([ V^{s,0}(\Gamma ,B) ]^{\perp }\) can then be considered as the counterpart of \(\mathrm {ker}(B) \subset H^{s-2\alpha }(\Gamma )\) but in the dual space \(H^s(\Gamma )\). For any \(u(x)\in V^s(\Gamma ,A)\), it can be uniquely decomposed as \begin{equation} u(x) = u_0(x) + u_1(x), \end{equation} where \(u_0(x)\in V^{s,0}(\Gamma ,B)\) and \(u_1(x)\in [ V^{s,0}(\Gamma ,B) ]^{\perp }\). Obviously, \(u_0(x)\in V^s(\Gamma ,A) \cap V^{s,0}(\Gamma ,B)\). Because \([ V^{s,0}(\Gamma ,B) ]^{\perp }\) is the counterpart of \(\mathrm {ker}(B)\) in \(H^s(\Gamma )\), if we let \(\{ v_p \}_{p=1}^m\) be the orthonormal basis of \(\mathrm {ker}(B)\), \(u_1(x)\) can be constructed by “projecting” \(u(x)\) to the basis \(\{ v_p \}_{p=1}^m\) in the sense of duality pairing as below. \begin{equation} u_1(x)=\sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} (\mathcal {J}^{-2\alpha }v_p)(x). \end{equation} In this representation, \(\langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}\) is the expansion coefficient and \(\mathcal {J}^{-2\alpha }v_p\) is the basis element for \([ V^{s,0}(\Gamma ,B) ]^{\perp }\) in \(H^s(\Gamma )\), which is the counterpart of the basis element \(v_p\) for \(\mathrm {ker}(B)\) in \(H^{s-2\alpha }(\Gamma )\). \(\mathcal {J}: \mathcal {S}(\mathbb {R}^d) \rightarrow \mathcal {S}(\mathbb {R}^d)\) is the Bessel potential operator on rapidly decreasing functions. \(\mathcal {J}\) with an order \(s\) is defined as (Steinbach, page 32) \begin{equation} \label {eq:bessel-potential-op} \mathcal {J}^su(x) := ( 2\pi )^{-d/2} \int _{\mathbb {R}^d} ( 1+\lvert \xi \rvert ^2 )^{s/2} \hat {u}(\xi ) \mathrm {e}^{\mathrm {i}\langle x,\xi \rangle } \mathrm {d}\xi , \end{equation} where \(\hat {u}(\xi )\) is the Fourier transform of \(u(x)\). Obviously, the effect of \(\mathcal {J}^s\) on \(u(x)\) is modifying its frequency spectrum by multiplying a polynomial about \(\xi \) with an order \(s\). According to the pseudodifferential operator theory, \(\mathcal {J}^s\) is equivalent to a partial differential operator with an order \(s\). If we restrict the domain of \(\mathcal {J}^s\) to a smaller space instead of \(\mathcal {S}(\mathbb {R}^d)\), such as the Sobolev space \(H^t(\Gamma )\), \(\mathcal {J}^s\) maps from \(H^t(\Gamma )\) to \(H^{t-s}(\Gamma )\), which lowers the Sobolev space order by \(s\). In the above representation for \(u_1(x)\), \(\mathcal {J}^{-2\alpha }\) maps from \(H^{s-2\alpha }(\Gamma )\) to \(H^s(\Gamma )\). According to (McLean, page 75), for any \(s,t\in \mathbb {R}\) and \(u,v\in \mathcal {S}(\mathbb {R}^d)\), Bessel polynomial operators have these properties: \(\mathcal {J}^{s+t} = \mathcal {J}^s \mathcal {J}^t\) \(( \mathcal {J}^s )^{-1} = \mathcal {J}^{-s}\) \(\mathcal {J}^0 = \text {identity operator}\) \(\langle \mathcal {J}^s u, v \rangle =\langle u,\mathcal {J}^s v \rangle \) \(\langle \cdot ,\cdot \rangle \) can be either considered as an \(L_2\) inner product of two functions in \(\mathcal {S}(\mathbb {R}^d)\) or as applying a tempered distribution in \(\mathcal {S}&#39;(\mathbb {R}^d)\) to a rapidly decreasing function in \(\mathcal {S}(\mathbb {R}^d)\). For example, we can treat \(u\) as a tempered distribution whose kernel function is \(u\). Then \(\mathcal {J}^s\) in \(\langle \mathcal {J}^s u,v \rangle \) is the Bessel potential operator on \(\mathcal {S}&#39;(\mathbb {R}^d)\) and \(\mathcal {J}^s\) in \(\langle u,\mathcal {J}^s v \rangle \) is the Bessel potential operator on \(\mathcal {S}(\mathbb {R})^d\). The Sobolev space \(H^s(\mathbb {R}^d)\) is defined as (Steinbach, page 73) \begin{equation} H^s(\mathbb {R}^d) := \left \{ v\in \mathcal {S}&#39;(\mathbb {R}^d): \mathcal {J}^sv \in L_2(\mathbb {R}^d) \right \}. \end{equation} The inner product is for any \(u,v\in H^s(\mathbb {R}^d)\), \begin{equation} \langle u,v \rangle _{H^s(\mathbb {R}^d)} := \langle \mathcal {J}^su, \mathcal {J}^sv \rangle _{L_2(\mathbb {R}^d)}. \end{equation} The norm definition is \begin{equation} \lVert u \rVert _{H^s(\mathbb {R}^d)} := \lVert \mathcal {J}^su \rVert _{L_2(\mathbb {R}^d)}. \end{equation} From the above, we also have \begin{equation} \langle u,v \rangle _{H^s(\mathbb {R}^d)} = \langle \mathcal {J}^su, \mathcal {J}^sv \rangle _{L_2(\mathbb {R}^d)} = \langle \mathcal {J}^s \mathcal {J}^s u,v \rangle = \langle \mathcal {J}^{2s}u,v \rangle . \end{equation} The above \(\mathcal {J}^{-2\alpha }\) is actually the Riesz map from \(H^{s-2\alpha }(\Gamma )\) to \(H^s(\Gamma )\), which satisfies \begin{equation} \langle \mathcal {J}^{-2\alpha }u,v \rangle _{H^{s-\alpha }(\Gamma )} = \langle u,v \rangle _{H^{s-2\alpha }(\Gamma )} \quad \forall u,v\in H^{s-2\alpha }(\Gamma ). \end{equation} This can be proved by using the properties of \(\mathcal {J}\). The left hand side of the above equation is \begin{equation} \langle \mathcal {J}^{-2\alpha }u,v \rangle _{H^{s-\alpha }(\Gamma )} = \langle \mathcal {J}^{-\alpha }u, \mathcal {J}^{-\alpha }v \rangle _{H^{s-\alpha }(\Gamma )}, \end{equation} where \(\langle \cdot ,\cdot \rangle _{H^{s-\alpha }(\Gamma )}\) on the left hand side is the duality pairing between \(H^s(\Gamma )\) and \(H^{s-2\alpha }(\Gamma )\), while on the right hand side it is the inner product in \(H^{s-\alpha }(\Gamma )\). Because both \(\mathcal {J}^{-\alpha }u\) and \(\mathcal {J}^{-\alpha }v\) belong to \(H^{s-\alpha }(\Gamma )\), according to the norm definition, \begin{equation} \langle \mathcal {J}^{-\alpha }u, \mathcal {J}^{-\alpha }v \rangle = \langle \mathcal {J}^{s-\alpha }\mathcal {J}^{-\alpha }u, \mathcal {J}^{s-\alpha }\mathcal {J}^{-\alpha }v \rangle _{L_2(\Gamma )} = \langle \mathcal {J}^{s-2\alpha }u,\mathcal {J}^{s-2\alpha }v \rangle _{L_2(\Gamma )}. \end{equation} This is just the same as \(\langle u,v \rangle _{H^{s-2\alpha }(\Gamma )}\). We can also show that \(u_0(x)\) is orthogonal to \(u_1(x)\). \begin{equation} \begin{aligned} \langle u_0,u_1 \rangle _{H^s(\Gamma )} &amp;= \langle u_0, \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \mathcal {J}^{-2\alpha }v_p \rangle _{H^s(\Gamma )} \\ &amp;= \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle u_0,\mathcal {J}^{-2\alpha }v_p \rangle _{H^s(\Gamma )} \\ &amp;= \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle \mathcal {J}^su_0,\mathcal {J}^s \mathcal {J}^{-2\alpha }v_p \rangle _{L_2(\Gamma )} \\ &amp;= \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle \mathcal {J}^su_0,\mathcal {J}^{s-2\alpha }v_p \rangle _{L_2(\Gamma )}, \end{aligned} \end{equation} where \begin{equation} \langle \mathcal {J}^su_0,\mathcal {J}^{s-2\alpha }v_p \rangle _{L_2(\Gamma )} = \langle u_0,v_p \rangle _{H^{s-\alpha }(\Gamma )}. \end{equation} Because \(u_0\in V^{s,0}(\Gamma ,B)\), which is the annihilator of \(\mathrm {ker}(B)\), the above expression is zero. 3 Define the preconditioning bilinear form Now we define the preconditioning bilinear form \(c(u,w)\), which is spectrally equivalent to \(a(u,w)=\langle Au,w \rangle \) &lt;ul class=&#39;itemize1&#39;&gt; &lt;li class=&#39;itemize&#39;&gt; &lt;!-- l. 169 --&gt;&lt;p class=&#39;noindent&#39;&gt;Method 1: Directly construct \(c(u,w)\) based on the generalized inverse \(B^{+}\). &lt;/p&gt;&lt;!-- l. 171 --&gt;&lt;p class=&#39;noindent&#39;&gt;&lt;span class=&#39;p1xb-x-x-109&#39;&gt;Basic idea&lt;/span&gt;: use the above orthogonal decomposition and construct the bilinear form \(c(u,u)\), which is spectrally equivalent to \(a(u,u)=\langle Au,u \rangle \). &lt;/p&gt;&lt;!-- l. 173 --&gt;&lt;p class=&#39;noindent&#39;&gt;Use the boundedness of \(a(\cdot ,\cdot )\) or \(A\), for all \(u\in V^s(\Gamma ,A)\) \begin{equation*} \langle Au,u \rangle _{H^{s-\alpha }(\Gamma )} \leq c_2^A \lVert u \rVert _{H^s(\Gamma )}^2 \end{equation*} Substitute the orthogonal decomposition \(u=u_0+u_1\) \begin{equation*} = c_2^A \lVert u_0 + u_1 \rVert _{H^s(\Gamma )}^2 \end{equation*} Because we are dealing with Hilbert space, \begin{equation*} = c_2^A (\langle u_0,u_0 \rangle + \langle u_0,u_1 \rangle + \langle u_1,u_0 \rangle + \langle u_1,u_1 \rangle ) \end{equation*} Because \(u_0\) is orthogonal to \(u_1\), \begin{align*} &amp;amp;= c_2^A (\lVert u_0 \rVert _{H^s(\Gamma )}^2 + \lVert u_1 \rVert _{H^s(\Gamma )}^2) \\ &amp;amp;= c_2^A \left ( \lVert u_0 \rVert _{H^s(\Gamma )}^2 + \left \langle \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \mathcal {J}^{-2\alpha }v_p, \sum _{q=1}^m \langle u,v_q \rangle _{H^{s-\alpha }(\Gamma )} \mathcal {J}^{-2\alpha }v_q \right \rangle \right ) \\ &amp;amp;= c_2^A \left ( \lVert u_0 \rVert _{H^s(\Gamma )}^2 + \sum _{p=1}^m\sum _{q=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle u,v_q \rangle _{H^{s-\alpha }(\Gamma )} \langle \mathcal {J}^{-2\alpha }v_p, \mathcal {J}^{-2\alpha }v_q \rangle _{H^s(\Gamma )} \right ) \\ &amp;amp;= c_2^A \left ( \lVert u_0 \rVert _{H^s(\Gamma )}^2 + \sum _{p=1}^m\sum _{q=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle u,v_q \rangle _{H^{s-\alpha }(\Gamma )} \langle v_p,v_q \rangle _{H^{s-2\alpha }(\Gamma )} \right ) \end{align*} &lt;/p&gt;&lt;!-- l. 201 --&gt;&lt;p class=&#39;noindent&#39;&gt;Because \(\{ v_p \}_{p=1}^m\) is an orthonormal basis, \begin{equation*} = c_2^A \left ( \lVert u_0 \rVert _{H^s(\Gamma )}^2 + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right ) \end{equation*} Use the \(V^{s,0}(\Gamma,B)\)-ellipticity of \(\dot {B}^{-1}\) and the reciprocal relationship between its spectrum and that of \(B\), \begin{align*} &amp;amp;\leq c_2^A \left ( c_2^B \langle \dot {B}^{-1}u_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right ) \\ &amp;amp;\leq c_2^A \max \{ c_2^B,1 \} \left ( \langle \dot {B}^{-1}u_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{k=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right ). \end{align*} &lt;/p&gt;&lt;!-- l. 214 --&gt;&lt;p class=&#39;noindent&#39;&gt;If we define the bilinear form \(c(u,w)\) as \begin{equation} \label {eq:bilinear-form-c} \begin{aligned} c(u,w) &amp;amp;:= \langle B^{\dagger }u,w \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle w,v_p \rangle _{H^{s-\alpha }(\Gamma )} \\ &amp;amp;= \langle \dot {B}^{-1}u_0,w_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle w,v_p \rangle _{H^{s-\alpha }(\Gamma )} \quad \forall u,w\in H^s(\Gamma ) \end{aligned}, \end{equation}&lt;a id=&#39;x1-4001r17&#39;&gt;&lt;/a&gt; it satisfies the upper part of the spectral equivalence condition \begin{equation} \langle Au,u \rangle _{H^{s-\alpha }(\Gamma )} \leq c_2^A \max \{ c_2^{B},1 \} c(u,u). \end{equation}&lt;a id=&#39;x1-4002r18&#39;&gt;&lt;/a&gt; &lt;/p&gt;&lt;!-- l. 230 --&gt;&lt;p class=&#39;noindent&#39;&gt;Then we prove the lower part of the spectral equivalence condition. Use the \(V^s(\Gamma,A)\)-ellipticity of \(A\), for all \(u\in V^s(\Gamma ,A)\) \begin{equation*} \langle Au,u \rangle _{H^{s-\alpha }(\Gamma )} \geq c_1^A \lVert u \rVert _{H^s(\Gamma )}^2 \end{equation*} Substitute the orthogonal decomposition \(u=u_0+u_1\) \begin{align*} &amp;amp;= c_1^A \lVert u_0+u_1 \rVert _{H^s(\Gamma )}^2 \\ &amp;amp;= c_1^A \left ( \lVert u_0 \rVert _{H^s(\Gamma )}^2 + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right ) \end{align*} &lt;/p&gt;&lt;!-- l. 240 --&gt;&lt;p class=&#39;noindent&#39;&gt;Use the boundedness of \(\dot {B}^{-1}\) and the reciprocal relationship between its spectrum and that of \(B\) \begin{align*} &amp;amp;\geq c_1^A \left ( c_1^B \langle \dot {B}^{-1}u_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right ) \\ &amp;amp;\geq c_1^A \min \{ c_1^B, 1 \} \left ( \langle \dot {B}^{-1}u_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right ). \end{align*} &lt;/p&gt;&lt;!-- l. 248 --&gt;&lt;p class=&#39;noindent&#39;&gt;Therefore, we have \begin{equation} c_1^A \min \{ c_1^B,1 \} c(u,u) \leq \langle Au,u \rangle _{H^{s-\alpha }(\Gamma )}. \end{equation}&lt;a id=&#39;x1-4003r19&#39;&gt;&lt;/a&gt; &lt;/p&gt;&lt;!-- l. 253 --&gt;&lt;p class=&#39;noindent&#39;&gt;In these two special cases &lt;/p&gt;&lt;ol class=&#39;enumerate1&#39;&gt; when \(V^s(\Gamma ,A) \subseteq V^{s,0}(\Gamma ,B)\), the orthogonal decomposition of \(u\) degenerates to \(u=u_0\), \(\mathrm {ker}(B) = \{ 0 \}\) &lt;/ol&gt; &lt;!-- l. 258 --&gt;&lt;p class=&#39;noindent&#39;&gt;the bilinear form \(c(u,u)\) degenerates to \(\langle \dot {B}^{-1}u,u \rangle \). &lt;/p&gt;&lt;/li&gt; &lt;li class=&#39;itemize&#39;&gt; &lt;!-- l. 260 --&gt;&lt;p class=&#39;noindent&#39;&gt;Method 2: Build an approximate operator \(\tilde {B}\) for \(B\), which is elliptic on the whole domain \(H^{s-2\alpha }(\Gamma )\). Then use its inverse to build the bilinear form \(c(u,w) = \langle \tilde {B}^{-1}u,w \rangle _{H^{s-\alpha }(\Gamma )}\). &lt;/p&gt;&lt;!-- l. 262 --&gt;&lt;p class=&#39;noindent&#39;&gt;For any \(u\in H^{s-2\alpha }(\Gamma )\), it also has an orthogonal decomposition in \(V^{s-2\alpha }(\Gamma ,B) \oplus \mathrm {ker}(B)\) as \begin{equation} u = u_0 + u_1 = u_0 + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )} v_p(x), \end{equation}&lt;a id=&#39;x1-4008r20&#39;&gt;&lt;/a&gt; where the inner product \(\langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}\) is the expansion coefficient for \(u_1\) and \(v_p\) is the corresponding basis element. Then we try to construct \(\tilde {B}\) which is \(H^{s-2\alpha }(\Gamma )\)-elliptic. \begin{align*} \lVert u \rVert _{H^{s-2\alpha }(\Gamma )}^2 &amp;amp;= \lVert u_0 + u_1 \rVert _{H^{s-2\alpha }(\Gamma )}^2 \\ &amp;amp;= \left ( \lVert u_0 \rVert _{H^{s-2\alpha }(\Gamma )}^2 + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ) \end{align*} &lt;/p&gt;&lt;!-- l. 273 --&gt;&lt;p class=&#39;noindent&#39;&gt;Use the \(V^{s-2\alpha}(\Gamma,B)\)-ellipticity of \(B\) \begin{align*} &amp;amp;\leq \left ( \frac {1}{c_1^B} \langle Bu_0, u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ) \\ &amp;amp;\leq \max \{ \frac {1}{c_1^B},1 \} \left ( \langle Bu_0, u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ). \end{align*} &lt;/p&gt;&lt;!-- l. 281 --&gt;&lt;p class=&#39;noindent&#39;&gt;It is equivalent to \begin{equation} c_1^{\tilde {B}} \lVert u \rVert _{H^{s-2\alpha }(\Gamma )}^2 \leq \left ( \langle Bu_0, u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ), \end{equation}&lt;a id=&#39;x1-4009r21&#39;&gt;&lt;/a&gt; where \(c_1^{\tilde {B}} = 1/\max \{ \frac {1}{c_1^B},1 \} = \min \{ c_1^B,1 \}\). &lt;/p&gt;&lt;!-- l. 289 --&gt;&lt;p class=&#39;noindent&#39;&gt;On the other hand, we start from the boundedness of \(B\): \begin{align*} \lVert u \rVert _{H^{s-2\alpha }(\Gamma )}^2 &amp;amp;= \lVert u_0 + u_1 \rVert _{H^{s-2\alpha }(\Gamma )}^2 \\ &amp;amp;= \left ( \lVert u_0 \rVert _{H^{s-2\alpha }(\Gamma )}^2 + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ) \\ &amp;amp;\geq \left ( \frac {1}{c_2^B} \langle Bu_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_0 \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ) \\ &amp;amp;\geq \min \{ \frac {1}{c_2^B},1 \} \left ( \langle Bu_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_0 \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ). \end{align*} &lt;/p&gt;&lt;!-- l. 300 --&gt;&lt;p class=&#39;noindent&#39;&gt;So we have \begin{equation} \left ( \langle Bu_0, u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ) \leq c_2^{\tilde {B}} \lVert u \rVert _{H^{s-2\alpha }(\Gamma )}^2, \end{equation}&lt;a id=&#39;x1-4010r22&#39;&gt;&lt;/a&gt; where \(c_2^{\tilde {B}} = \max \{ c_2^B,1 \}\). &lt;/p&gt;&lt;!-- l. 309 --&gt;&lt;p class=&#39;noindent&#39;&gt;Therefore, we can define the bilinear form related to \(\tilde {B}\) as \begin{equation} \label {eq:bilinear-form-b} \begin{aligned} \langle \tilde {B}u,w \rangle _{H^{s-\alpha }(\Gamma )} &amp;amp;= \langle Bu,w \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle v,v_p \rangle _{H^{s-2\alpha }(\Gamma )} \langle w,v_p \rangle _{H^{s-2\alpha }(\Gamma )} \\ &amp;amp;= \langle Bu_0,w_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle v,v_p \rangle _{H^{s-2\alpha }(\Gamma )} \langle w,v_p \rangle _{H^{s-2\alpha }(\Gamma )} \quad \forall u,w\in H^{s-2\alpha }(\Gamma ). \end{aligned} \end{equation}&lt;a id=&#39;x1-4011r23&#39;&gt;&lt;/a&gt; It is both bounded and \(H^{s-2\alpha }(\Gamma )\)-elliptic. &lt;/p&gt;&lt;!-- l. 323 --&gt;&lt;p class=&#39;noindent&#39;&gt;The differences between the bilinear forms in Equation (&lt;a href=&#39;#x1-4001r17&#39;&gt;17&lt;!-- tex4ht:ref: eq:bilinear-form-c --&gt;&lt;/a&gt;) and (&lt;a href=&#39;#x1-4011r23&#39;&gt;23&lt;!-- tex4ht:ref: eq:bilinear-form-b --&gt;&lt;/a&gt;) are: &lt;/p&gt;&lt;ol class=&#39;enumerate1&#39;&gt; In Equation (23), \(u\) and \(w\) are projected to \(v_p\) via inner product, which is a true projection. In Equation (17), \(u\) and \(w\) are in a different space from \(v_p\). They are “projected” to \(v_p\) via duality pairing, which can be considered as a generalization of projection. In Equation (23), the basis used for function expansion is \(\{ v_p \}_{p=1}^m\). In Equation (17), the basis is \(\{ \mathcal {J}v_p \}_{p=1}^m\). &lt;/ol&gt; &lt;/li&gt;&lt;/ul&gt; &lt;p class=&#39;noindent&#39;&gt; &lt;/p&gt; 4 Discretization of the preconditioning bilinear form According to (Steinbach and Wendland, section 4), Method 1 in Section 3 requires coordinate transformation between the finite element basis and the orthonormal basis adopted for the orthogonal decomposition \(V_h^s(\Gamma ,A) = V_h^0 \oplus \{ \mathcal {J}v_p \}_{p=1}^m\), which is more complicated than Method 2. Meanwhile, (Steinbach, section 13.2 page 299) only introduces Method 2. So we stick to this method. Usually, the inverse operator \(\tilde {B}^{-1}\) cannot be directly evaluated or discretized. We need to compute the discretized matrix of \(\tilde {B}\) first, then use its inverse matrix to compute an approximation to the discretized matrix for the inverse operator \(\tilde {B}^{-1}\). This can be visualized in the following diagram. Therefore, we have \begin{equation} \underline {\tilde {B}^{-1}} \approx \mathcal {M}_{\tilde {B}}^{\mathrm {T}} \tilde {\mathcal {B}}^{-1} \mathcal {M}_{\tilde {B}}. \end{equation} The matrix to be multiplied to both sides of the equation is \begin{equation} \mathcal {M}_{\tilde {B}}^{-1} \tilde {\mathcal {B}} \mathcal {M}_{\tilde {B}}^{-\mathrm {T}}. \end{equation} The spectral equivalence between the above two matrices \(\underline {\tilde {B}^{-1}}\) and \(\mathcal {M}_{\tilde {B}}^{\mathrm {T}} \tilde {\mathcal {B}}^{-1} \mathcal {M}_{\tilde {B}}\) can be proved using the theory given in (Steinbach and Wendland, section 3). This theory is described in a general form as below. \(V\) and \(W\) are two Hilbert spaces. \(A: V \rightarrow V&#39;\) is a \(V\)-elliptic, self-adjoint and bounded operator, while \(B: W \rightarrow V&#39;\) is any bounded operator. Define an operator \(T: W \rightarrow W&#39;\) as \(T = B&#39; A^{-1} B\). In principle, it has a Galerkin matrix \(\mathcal {T}\). In reality, this matrix cannot be directly computed. The Galerkin matrices for \(A\) and \(B\) are \(\mathcal {A}\) and \(\mathcal {B}\) respectively. Then we have another matrix \begin{equation} \tilde {\mathcal {T}} = \mathcal {B}^{\mathrm {T}} \mathcal {A}^{-1} \mathcal {B}. \end{equation} It can be proved that for any \(w_h\in W_h\), \begin{equation} ( \tilde {\mathcal {T}} \underline {w}, \underline {w} ) \leq ( \mathcal {T} \underline {w}, \underline {w} ). \end{equation} If the stability condition or \(\inf \sup \) condition is satisfied \begin{equation} \inf _{0\neq w_h\in W_h} \sup _{0\neq v_h\in V_h} \frac {\lvert \langle Bw_h,v_h \rangle _{V} \rvert }{\lVert w_h \rVert _W \lVert v_h \rVert _V} \geq c, \end{equation} we also have \begin{equation} \gamma ( \mathcal {T} \underline {w}, \underline {w} ) \leq ( \tilde {\mathcal {T}} \underline {w}, \underline {w} ). \end{equation} Therefore, \(\mathcal {T}\) and \(\tilde {\mathcal {T}}\) are spectrally equivalent. The relationship between the spaces, operators and matrices are shown below. Coming back to the spectral equivalence between the two matrices \(\underline {\tilde {B}^{-1}}\) and \(\mathcal {M}_{\tilde {B}}^{\mathrm {T}} \tilde {\mathcal {B}}^{-1} \mathcal {M}_{\tilde {B}}\), we can see that it is the special case when \(W\) is selected to be \(V&#39;\) and \(B\) is the identity operator on \(V&#39;\). The mass matrix \(M_A\) for the duality pairing from \(V&#39;\) to \(V\) is just the Galerkin matrix of \(B\). This is shown in the following figure. 5 Preconditioners for Laplace problem The operator equation for the Laplace problem with Dirichlet boundary condition is \begin{equation} \langle Vt, \tau \rangle _{\Gamma } = \langle (\frac {1}{2}I + K)g, \tau \rangle _{\Gamma } \quad \forall \tau \in H^{-1/2}(\Gamma ). \end{equation} The hypersingular operator \(D\) has an opposite order with respect to \(V\), which is naturally the preconditioner for this equation. However, according to here, \(D\) is not \(H^{1/2}(\Gamma )\)-elliptic, but only \(H_{\ast }^{1/2}(\Gamma )\)-elliptic or \(H_{\ast \ast }^{1/2}(\Gamma )\)-elliptic depending on which kind of inner product is desired. To avoid the complex of computing the natural density \(w_{\mathrm {eq}}\), we choose \(H_{\ast \ast }^{1/2}(\Gamma )\)-ellipticity. Then the bilinear form associated with the regularized or gauged operator \(\tilde {D}\) is \begin{equation} \langle \tilde {D}u,w \rangle = \langle Du,w \rangle + \langle u,1 \rangle _{\Gamma } \langle w,1 \rangle _{\Gamma } \quad \forall u,w\in H^{1/2}(\Gamma ). \end{equation} Then the inverse of the preconditioning matrix is \begin{equation} \mathcal {M}_{\tilde {D}}^{-1} \tilde {\mathcal {D}} \mathcal {M}_{\tilde {D}}^{-\mathrm {T}}, \end{equation} where \(\mathcal {M}_{\tilde {D}}\) is the mass matrix for the duality pairing from \(H^{-1/2}(\Gamma )\) to \(H^{1/2}(\Gamma )\). The operator equation for the Laplace problem with Neumann boundary condition is \begin{equation} \langle Du, v \rangle _{\Gamma } + \alpha \langle u,1 \rangle _{\Gamma } \langle v,1 \rangle _{\Gamma } = \langle (\frac {1}{2}I - K&#39;)g,v \rangle _{\Gamma } \quad \forall v\in H^{1/2}(\Gamma ). \end{equation} The single layer potential operator \(V\) has an opposite order with respect to \(D\), which is the preconditioner. When \(d=3\), \(V\) is \(H^{-1/2}(\Gamma )\)-elliptic, so we directly obtain the inverse of the preconditioning matrix \begin{equation} \mathcal {M}_V^{-1} \mathcal {V} \mathcal {M}_V^{-\mathrm {T}}, \end{equation} where \(\mathcal {M}_V\) is the mass matrix for the duality pairing from \(H^{1/2}(\Gamma )\) to \(H^{-1/2}(\Gamma )\). References    William Charles Hector McLean. Strongly Elliptic Systems and Boundary Integral Equations. Cambridge University Press. ISBN 978-0-521-66375-5.    Olaf Steinbach. Numerical Approximation Methods for Elliptic Boundary Value Problems: Finite and Boundary Elements. Springer Science &amp; Business Media. ISBN 978-0-387-31312-2.    Olaf Steinbach and Wolfgang L. Wendland. The construction of some efficient preconditioners in the boundary element method. 9(1-2):191–216. URL http://link.springer.com/article/10.1023/A:1018937506719." />
<link rel="canonical" href="https://jihuan-tian.github.io/math/2024/11/16/general-theory-about-the-construction-of-preconditioning-bilinear-form-in-bem.html" />
<meta property="og:url" content="https://jihuan-tian.github.io/math/2024/11/16/general-theory-about-the-construction-of-preconditioning-bilinear-form-in-bem.html" />
<meta property="og:site_name" content="止于至善" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-11-16T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="General theory about the construction of preconditioning bilinear form in BEM" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Jihuan Tian"},"dateModified":"2024-11-16T00:00:00+08:00","datePublished":"2024-11-16T00:00:00+08:00","description":"Contents  1 Inverse operator of the preconditioner  2 Orthogonal decomposition of the domain \\(V^s(\\Gamma ,A)\\) of \\(A\\)  3 Define the preconditioning bilinear form  4 Discretization of the preconditioning bilinear form  5 Preconditioners for Laplace problem In BEM, the preconditioning technique is indispensable for solving a large scale PDE, when an iterative solver, such as CG, GMRES, MinRES, BiCGStab, is adopted. Without a preconditioner, the number of iterations will significantly increase with the deterioration of the system matrix’s condition number. A large condition number can be caused by a large number of DoFs, distinct mesh element size, sharp corners in the geometry, etc. The procedure for constructing the Galerkin matrix for a preconditioner is summarized into four steps. Find a preconditioning operator having an opposite order with respect to the key operator in the original PDE in the theoretical framework of pseudodifferential operators (see Pseudodifferential operator, Operator preconditioning based on pseudodifferential operator of opposite orders). In BEM, this is a trivial task thanks to the properties of boundary integral operators (see Boundary integral operators considered as pseudodifferential operators). Construct an inverse operator for the preconditioner obtained from the previous step. If the preconditioner is not elliptic on its whole domain, we should construct a Moore-Penrose pseudo inverse or generalized inverse operator instead. Alternatively, we can build an approximate operator for the preconditioner, which is elliptic on the whole domain. Then its inverse operator is available. Construct a bilinear form for the above inverse operator, which is spectrally equivalent to the bilinear form in the variational form of the PDE. The lower bound and upper bound constants in this spectral equivalence condition do not depend on the geometric discretization and finite element (function space) discretization. Discretize the above bilinear form, whose inverse matrix is to be multiplied to the discretized linear system for the variational form of the PDE. Since the inverse operator associated with this bilinear form cannot be directly evaluated, we will compute an approximate version which is based on the inverse matrix of the Galerkin matrix for the preconditioning operator. A general matrix spectral equivalence theorem guarantees the equivalence between this approximate version and the exact version. In the following, we’ll introduce the basic ideas and theoretical details. Finally, we present the preconditioners used in the Laplace problem with either Dirichlet or Neumann boundary condition. 1 Inverse operator of the preconditioner The operator \\(A: V^s(\\Gamma ,A):= H^s(\\Gamma )_{/\\mathrm {ker}(A)} \\rightarrow H^{s-2\\alpha }(\\Gamma )\\) in the original PDE is self-adjoint (in the sense of adjointness in Banach spaces), bounded and \\(V^s(\\Gamma ,A)\\)-elliptic. As a pseudodifferential operator, its order is \\(2\\alpha \\). The preconditioning operator \\(B: H^{s-2\\alpha }(\\Gamma ) \\rightarrow H^s(\\Gamma )\\) is self-adjoint, bounded and \\(V^{s-2\\alpha ,0}(\\Gamma ,B) := H^{s-2\\alpha }(\\Gamma )_{/\\mathrm {ker}(B)}\\)-elliptic. Its order is \\(-2\\alpha \\). Define the inverse operator \\(B^{-1}: H^s(\\Gamma ) \\rightarrow H^{s-2\\alpha }(\\Gamma )\\) of \\(B\\) which should be spectrally equivalent to \\(A\\). Assume \\(B\\) has a closed range and also note \\(B\\) is self-adjoint, then \\begin{equation} \\mathrm {Im}(B) = (\\mathrm {ker}(B&#39;))^{\\circ } = (\\mathrm {ker}(B))^{\\circ }. \\end{equation} If the operator \\(B\\) has a non-trivial kernel and let \\(H^s(\\Gamma )\\) be decomposed as \\(H^s(\\Gamma ) = (\\mathrm {ker}(B))^{\\circ } \\oplus [(\\mathrm {ker}(B))^{\\circ }]^{\\mathrm {c}}\\), a generalized inverse \\(B^{+}: H^s(\\Gamma ) \\rightarrow H^{s-2\\alpha }(\\Gamma )\\) can be defined \\begin{equation} B^{+} = \\begin {cases} \\dot {B}^{-1}(y) &amp; y\\in (\\mathrm {ker}(B))^{\\circ } \\\\ 0 &amp; y\\in [(\\mathrm {ker}(B))^{\\circ }]^{\\mathrm {c}} \\end {cases}, \\end{equation} where \\((\\mathrm {ker}(B))^{\\mathrm {c}}\\) is the complement of \\(\\mathrm {ker}(B)\\) within \\(H^{s-2\\alpha }(\\Gamma )\\) and \\(\\dot {B}^{-1}: (\\mathrm {ker}(B))^{\\circ } \\rightarrow (\\mathrm {ker}(B))^{\\mathrm {c}}\\) is a bijective map. Even though the above adjointness and generalized inverse are discussed in the context of Banach spaces, the Sobolev space \\(H^{s-2\\alpha }(\\Gamma )\\) is actually a Hilbert space. Therefore, the complement subspace \\((\\mathrm {ker}(B))^{\\mathrm {c}}\\) is also the orthogonal complement subspace \\((\\mathrm {ker}(B))^{\\perp }\\). The domain and range spaces of \\(\\dot {B}^{-1}\\) are defined as (Steinbach and Wendland) \\begin{equation} \\begin{aligned} V^{s,0}(\\Gamma ,B) &amp;:= (\\mathrm {ker}(B))^{\\circ } = \\left \\{ v\\in H^s(\\Gamma ): \\langle v,v_p \\rangle _{H^{s-\\alpha }(\\Gamma )} = 0 \\right \\} \\\\ V^{s-2\\alpha ,0}(\\Gamma ,B) &amp;:= (\\mathrm {ker}(B))^{\\perp } = \\left \\{ v\\in H^{s-2\\alpha }(\\Gamma ): \\langle v,v_p \\rangle _{H^{s-2\\alpha }(\\Gamma )}=0 \\right \\} \\end{aligned} \\quad \\forall v_p\\in \\mathrm {ker}(B). \\end{equation} To simultaneously ensure \\(V^s(\\Gamma ,A)\\)-ellipticity of \\(A\\) and \\(V^{s,0}(\\Gamma ,B)\\)-ellipticity of \\(\\dot {B}^{-1}\\) which are required by the spectral equivalence condition, the effective domain for \\(A\\) and \\(\\dot {B}^{-1}\\) should be the intersection \\(V^s(\\Gamma ,A) \\cap V^{s,0}(\\Gamma ,B)\\). 2 Orthogonal decomposition of the domain \\(V^s(\\Gamma ,A)\\) of \\(A\\) \\(V^s(\\Gamma ,A)\\) is a subspace of \\(H^s(\\Gamma )\\). \\(H^s(\\Gamma )\\) is a Hilbert space and has orthogonal decomposition with respect to its subspace \\(V^{s,0}(\\Gamma ,B)\\): \\begin{equation} H^s(\\Gamma )=V^{s,0}(\\Gamma ,B) \\oplus [ V^{s,0}(\\Gamma ,B) ]^{\\perp }. \\end{equation} As defined above, \\(V^{s,0}(\\Gamma ,B)\\) is the annihilator of \\(\\mathrm {ker}(B)\\) in the sense of duality pairing, which can be considered as a generalization of the concept of orthogonality (based on inner product) in a Hilbert space. Its orthogonal complement subspace \\([ V^{s,0}(\\Gamma ,B) ]^{\\perp }\\) can then be considered as the counterpart of \\(\\mathrm {ker}(B) \\subset H^{s-2\\alpha }(\\Gamma )\\) but in the dual space \\(H^s(\\Gamma )\\). For any \\(u(x)\\in V^s(\\Gamma ,A)\\), it can be uniquely decomposed as \\begin{equation} u(x) = u_0(x) + u_1(x), \\end{equation} where \\(u_0(x)\\in V^{s,0}(\\Gamma ,B)\\) and \\(u_1(x)\\in [ V^{s,0}(\\Gamma ,B) ]^{\\perp }\\). Obviously, \\(u_0(x)\\in V^s(\\Gamma ,A) \\cap V^{s,0}(\\Gamma ,B)\\). Because \\([ V^{s,0}(\\Gamma ,B) ]^{\\perp }\\) is the counterpart of \\(\\mathrm {ker}(B)\\) in \\(H^s(\\Gamma )\\), if we let \\(\\{ v_p \\}_{p=1}^m\\) be the orthonormal basis of \\(\\mathrm {ker}(B)\\), \\(u_1(x)\\) can be constructed by “projecting” \\(u(x)\\) to the basis \\(\\{ v_p \\}_{p=1}^m\\) in the sense of duality pairing as below. \\begin{equation} u_1(x)=\\sum _{p=1}^m \\langle u,v_p \\rangle _{H^{s-\\alpha }(\\Gamma )} (\\mathcal {J}^{-2\\alpha }v_p)(x). \\end{equation} In this representation, \\(\\langle u,v_p \\rangle _{H^{s-\\alpha }(\\Gamma )}\\) is the expansion coefficient and \\(\\mathcal {J}^{-2\\alpha }v_p\\) is the basis element for \\([ V^{s,0}(\\Gamma ,B) ]^{\\perp }\\) in \\(H^s(\\Gamma )\\), which is the counterpart of the basis element \\(v_p\\) for \\(\\mathrm {ker}(B)\\) in \\(H^{s-2\\alpha }(\\Gamma )\\). \\(\\mathcal {J}: \\mathcal {S}(\\mathbb {R}^d) \\rightarrow \\mathcal {S}(\\mathbb {R}^d)\\) is the Bessel potential operator on rapidly decreasing functions. \\(\\mathcal {J}\\) with an order \\(s\\) is defined as (Steinbach, page 32) \\begin{equation} \\label {eq:bessel-potential-op} \\mathcal {J}^su(x) := ( 2\\pi )^{-d/2} \\int _{\\mathbb {R}^d} ( 1+\\lvert \\xi \\rvert ^2 )^{s/2} \\hat {u}(\\xi ) \\mathrm {e}^{\\mathrm {i}\\langle x,\\xi \\rangle } \\mathrm {d}\\xi , \\end{equation} where \\(\\hat {u}(\\xi )\\) is the Fourier transform of \\(u(x)\\). Obviously, the effect of \\(\\mathcal {J}^s\\) on \\(u(x)\\) is modifying its frequency spectrum by multiplying a polynomial about \\(\\xi \\) with an order \\(s\\). According to the pseudodifferential operator theory, \\(\\mathcal {J}^s\\) is equivalent to a partial differential operator with an order \\(s\\). If we restrict the domain of \\(\\mathcal {J}^s\\) to a smaller space instead of \\(\\mathcal {S}(\\mathbb {R}^d)\\), such as the Sobolev space \\(H^t(\\Gamma )\\), \\(\\mathcal {J}^s\\) maps from \\(H^t(\\Gamma )\\) to \\(H^{t-s}(\\Gamma )\\), which lowers the Sobolev space order by \\(s\\). In the above representation for \\(u_1(x)\\), \\(\\mathcal {J}^{-2\\alpha }\\) maps from \\(H^{s-2\\alpha }(\\Gamma )\\) to \\(H^s(\\Gamma )\\). According to (McLean, page 75), for any \\(s,t\\in \\mathbb {R}\\) and \\(u,v\\in \\mathcal {S}(\\mathbb {R}^d)\\), Bessel polynomial operators have these properties: \\(\\mathcal {J}^{s+t} = \\mathcal {J}^s \\mathcal {J}^t\\) \\(( \\mathcal {J}^s )^{-1} = \\mathcal {J}^{-s}\\) \\(\\mathcal {J}^0 = \\text {identity operator}\\) \\(\\langle \\mathcal {J}^s u, v \\rangle =\\langle u,\\mathcal {J}^s v \\rangle \\) \\(\\langle \\cdot ,\\cdot \\rangle \\) can be either considered as an \\(L_2\\) inner product of two functions in \\(\\mathcal {S}(\\mathbb {R}^d)\\) or as applying a tempered distribution in \\(\\mathcal {S}&#39;(\\mathbb {R}^d)\\) to a rapidly decreasing function in \\(\\mathcal {S}(\\mathbb {R}^d)\\). For example, we can treat \\(u\\) as a tempered distribution whose kernel function is \\(u\\). Then \\(\\mathcal {J}^s\\) in \\(\\langle \\mathcal {J}^s u,v \\rangle \\) is the Bessel potential operator on \\(\\mathcal {S}&#39;(\\mathbb {R}^d)\\) and \\(\\mathcal {J}^s\\) in \\(\\langle u,\\mathcal {J}^s v \\rangle \\) is the Bessel potential operator on \\(\\mathcal {S}(\\mathbb {R})^d\\). The Sobolev space \\(H^s(\\mathbb {R}^d)\\) is defined as (Steinbach, page 73) \\begin{equation} H^s(\\mathbb {R}^d) := \\left \\{ v\\in \\mathcal {S}&#39;(\\mathbb {R}^d): \\mathcal {J}^sv \\in L_2(\\mathbb {R}^d) \\right \\}. \\end{equation} The inner product is for any \\(u,v\\in H^s(\\mathbb {R}^d)\\), \\begin{equation} \\langle u,v \\rangle _{H^s(\\mathbb {R}^d)} := \\langle \\mathcal {J}^su, \\mathcal {J}^sv \\rangle _{L_2(\\mathbb {R}^d)}. \\end{equation} The norm definition is \\begin{equation} \\lVert u \\rVert _{H^s(\\mathbb {R}^d)} := \\lVert \\mathcal {J}^su \\rVert _{L_2(\\mathbb {R}^d)}. \\end{equation} From the above, we also have \\begin{equation} \\langle u,v \\rangle _{H^s(\\mathbb {R}^d)} = \\langle \\mathcal {J}^su, \\mathcal {J}^sv \\rangle _{L_2(\\mathbb {R}^d)} = \\langle \\mathcal {J}^s \\mathcal {J}^s u,v \\rangle = \\langle \\mathcal {J}^{2s}u,v \\rangle . \\end{equation} The above \\(\\mathcal {J}^{-2\\alpha }\\) is actually the Riesz map from \\(H^{s-2\\alpha }(\\Gamma )\\) to \\(H^s(\\Gamma )\\), which satisfies \\begin{equation} \\langle \\mathcal {J}^{-2\\alpha }u,v \\rangle _{H^{s-\\alpha }(\\Gamma )} = \\langle u,v \\rangle _{H^{s-2\\alpha }(\\Gamma )} \\quad \\forall u,v\\in H^{s-2\\alpha }(\\Gamma ). \\end{equation} This can be proved by using the properties of \\(\\mathcal {J}\\). The left hand side of the above equation is \\begin{equation} \\langle \\mathcal {J}^{-2\\alpha }u,v \\rangle _{H^{s-\\alpha }(\\Gamma )} = \\langle \\mathcal {J}^{-\\alpha }u, \\mathcal {J}^{-\\alpha }v \\rangle _{H^{s-\\alpha }(\\Gamma )}, \\end{equation} where \\(\\langle \\cdot ,\\cdot \\rangle _{H^{s-\\alpha }(\\Gamma )}\\) on the left hand side is the duality pairing between \\(H^s(\\Gamma )\\) and \\(H^{s-2\\alpha }(\\Gamma )\\), while on the right hand side it is the inner product in \\(H^{s-\\alpha }(\\Gamma )\\). Because both \\(\\mathcal {J}^{-\\alpha }u\\) and \\(\\mathcal {J}^{-\\alpha }v\\) belong to \\(H^{s-\\alpha }(\\Gamma )\\), according to the norm definition, \\begin{equation} \\langle \\mathcal {J}^{-\\alpha }u, \\mathcal {J}^{-\\alpha }v \\rangle = \\langle \\mathcal {J}^{s-\\alpha }\\mathcal {J}^{-\\alpha }u, \\mathcal {J}^{s-\\alpha }\\mathcal {J}^{-\\alpha }v \\rangle _{L_2(\\Gamma )} = \\langle \\mathcal {J}^{s-2\\alpha }u,\\mathcal {J}^{s-2\\alpha }v \\rangle _{L_2(\\Gamma )}. \\end{equation} This is just the same as \\(\\langle u,v \\rangle _{H^{s-2\\alpha }(\\Gamma )}\\). We can also show that \\(u_0(x)\\) is orthogonal to \\(u_1(x)\\). \\begin{equation} \\begin{aligned} \\langle u_0,u_1 \\rangle _{H^s(\\Gamma )} &amp;= \\langle u_0, \\sum _{p=1}^m \\langle u,v_p \\rangle _{H^{s-\\alpha }(\\Gamma )} \\mathcal {J}^{-2\\alpha }v_p \\rangle _{H^s(\\Gamma )} \\\\ &amp;= \\sum _{p=1}^m \\langle u,v_p \\rangle _{H^{s-\\alpha }(\\Gamma )} \\langle u_0,\\mathcal {J}^{-2\\alpha }v_p \\rangle _{H^s(\\Gamma )} \\\\ &amp;= \\sum _{p=1}^m \\langle u,v_p \\rangle _{H^{s-\\alpha }(\\Gamma )} \\langle \\mathcal {J}^su_0,\\mathcal {J}^s \\mathcal {J}^{-2\\alpha }v_p \\rangle _{L_2(\\Gamma )} \\\\ &amp;= \\sum _{p=1}^m \\langle u,v_p \\rangle _{H^{s-\\alpha }(\\Gamma )} \\langle \\mathcal {J}^su_0,\\mathcal {J}^{s-2\\alpha }v_p \\rangle _{L_2(\\Gamma )}, \\end{aligned} \\end{equation} where \\begin{equation} \\langle \\mathcal {J}^su_0,\\mathcal {J}^{s-2\\alpha }v_p \\rangle _{L_2(\\Gamma )} = \\langle u_0,v_p \\rangle _{H^{s-\\alpha }(\\Gamma )}. \\end{equation} Because \\(u_0\\in V^{s,0}(\\Gamma ,B)\\), which is the annihilator of \\(\\mathrm {ker}(B)\\), the above expression is zero. 3 Define the preconditioning bilinear form Now we define the preconditioning bilinear form \\(c(u,w)\\), which is spectrally equivalent to \\(a(u,w)=\\langle Au,w \\rangle \\) &lt;ul class=&#39;itemize1&#39;&gt; &lt;li class=&#39;itemize&#39;&gt; &lt;!-- l. 169 --&gt;&lt;p class=&#39;noindent&#39;&gt;Method 1: Directly construct \\(c(u,w)\\) based on the generalized inverse \\(B^{+}\\). &lt;/p&gt;&lt;!-- l. 171 --&gt;&lt;p class=&#39;noindent&#39;&gt;&lt;span class=&#39;p1xb-x-x-109&#39;&gt;Basic idea&lt;/span&gt;: use the above orthogonal decomposition and construct the bilinear form \\(c(u,u)\\), which is spectrally equivalent to \\(a(u,u)=\\langle Au,u \\rangle \\). &lt;/p&gt;&lt;!-- l. 173 --&gt;&lt;p class=&#39;noindent&#39;&gt;Use the boundedness of \\(a(\\cdot ,\\cdot )\\) or \\(A\\), for all \\(u\\in V^s(\\Gamma ,A)\\) \\begin{equation*} \\langle Au,u \\rangle _{H^{s-\\alpha }(\\Gamma )} \\leq c_2^A \\lVert u \\rVert _{H^s(\\Gamma )}^2 \\end{equation*} Substitute the orthogonal decomposition \\(u=u_0+u_1\\) \\begin{equation*} = c_2^A \\lVert u_0 + u_1 \\rVert _{H^s(\\Gamma )}^2 \\end{equation*} Because we are dealing with Hilbert space, \\begin{equation*} = c_2^A (\\langle u_0,u_0 \\rangle + \\langle u_0,u_1 \\rangle + \\langle u_1,u_0 \\rangle + \\langle u_1,u_1 \\rangle ) \\end{equation*} Because \\(u_0\\) is orthogonal to \\(u_1\\), \\begin{align*} &amp;amp;= c_2^A (\\lVert u_0 \\rVert _{H^s(\\Gamma )}^2 + \\lVert u_1 \\rVert _{H^s(\\Gamma )}^2) \\\\ &amp;amp;= c_2^A \\left ( \\lVert u_0 \\rVert _{H^s(\\Gamma )}^2 + \\left \\langle \\sum _{p=1}^m \\langle u,v_p \\rangle _{H^{s-\\alpha }(\\Gamma )} \\mathcal {J}^{-2\\alpha }v_p, \\sum _{q=1}^m \\langle u,v_q \\rangle _{H^{s-\\alpha }(\\Gamma )} \\mathcal {J}^{-2\\alpha }v_q \\right \\rangle \\right ) \\\\ &amp;amp;= c_2^A \\left ( \\lVert u_0 \\rVert _{H^s(\\Gamma )}^2 + \\sum _{p=1}^m\\sum _{q=1}^m \\langle u,v_p \\rangle _{H^{s-\\alpha }(\\Gamma )} \\langle u,v_q \\rangle _{H^{s-\\alpha }(\\Gamma )} \\langle \\mathcal {J}^{-2\\alpha }v_p, \\mathcal {J}^{-2\\alpha }v_q \\rangle _{H^s(\\Gamma )} \\right ) \\\\ &amp;amp;= c_2^A \\left ( \\lVert u_0 \\rVert _{H^s(\\Gamma )}^2 + \\sum _{p=1}^m\\sum _{q=1}^m \\langle u,v_p \\rangle _{H^{s-\\alpha }(\\Gamma )} \\langle u,v_q \\rangle _{H^{s-\\alpha }(\\Gamma )} \\langle v_p,v_q \\rangle _{H^{s-2\\alpha }(\\Gamma )} \\right ) \\end{align*} &lt;/p&gt;&lt;!-- l. 201 --&gt;&lt;p class=&#39;noindent&#39;&gt;Because \\(\\{ v_p \\}_{p=1}^m\\) is an orthonormal basis, \\begin{equation*} = c_2^A \\left ( \\lVert u_0 \\rVert _{H^s(\\Gamma )}^2 + \\sum _{p=1}^m \\langle u,v_p \\rangle _{H^{s-\\alpha }(\\Gamma )}^2 \\right ) \\end{equation*} Use the \\(V^{s,0}(\\Gamma,B)\\)-ellipticity of \\(\\dot {B}^{-1}\\) and the reciprocal relationship between its spectrum and that of \\(B\\), \\begin{align*} &amp;amp;\\leq c_2^A \\left ( c_2^B \\langle \\dot {B}^{-1}u_0,u_0 \\rangle _{H^{s-\\alpha }(\\Gamma )} + \\sum _{p=1}^m \\langle u,v_p \\rangle _{H^{s-\\alpha }(\\Gamma )}^2 \\right ) \\\\ &amp;amp;\\leq c_2^A \\max \\{ c_2^B,1 \\} \\left ( \\langle \\dot {B}^{-1}u_0,u_0 \\rangle _{H^{s-\\alpha }(\\Gamma )} + \\sum _{k=1}^m \\langle u,v_p \\rangle _{H^{s-\\alpha }(\\Gamma )}^2 \\right ). \\end{align*} &lt;/p&gt;&lt;!-- l. 214 --&gt;&lt;p class=&#39;noindent&#39;&gt;If we define the bilinear form \\(c(u,w)\\) as \\begin{equation} \\label {eq:bilinear-form-c} \\begin{aligned} c(u,w) &amp;amp;:= \\langle B^{\\dagger }u,w \\rangle _{H^{s-\\alpha }(\\Gamma )} + \\sum _{p=1}^m \\langle u,v_p \\rangle _{H^{s-\\alpha }(\\Gamma )} \\langle w,v_p \\rangle _{H^{s-\\alpha }(\\Gamma )} \\\\ &amp;amp;= \\langle \\dot {B}^{-1}u_0,w_0 \\rangle _{H^{s-\\alpha }(\\Gamma )} + \\sum _{p=1}^m \\langle u,v_p \\rangle _{H^{s-\\alpha }(\\Gamma )} \\langle w,v_p \\rangle _{H^{s-\\alpha }(\\Gamma )} \\quad \\forall u,w\\in H^s(\\Gamma ) \\end{aligned}, \\end{equation}&lt;a id=&#39;x1-4001r17&#39;&gt;&lt;/a&gt; it satisfies the upper part of the spectral equivalence condition \\begin{equation} \\langle Au,u \\rangle _{H^{s-\\alpha }(\\Gamma )} \\leq c_2^A \\max \\{ c_2^{B},1 \\} c(u,u). \\end{equation}&lt;a id=&#39;x1-4002r18&#39;&gt;&lt;/a&gt; &lt;/p&gt;&lt;!-- l. 230 --&gt;&lt;p class=&#39;noindent&#39;&gt;Then we prove the lower part of the spectral equivalence condition. Use the \\(V^s(\\Gamma,A)\\)-ellipticity of \\(A\\), for all \\(u\\in V^s(\\Gamma ,A)\\) \\begin{equation*} \\langle Au,u \\rangle _{H^{s-\\alpha }(\\Gamma )} \\geq c_1^A \\lVert u \\rVert _{H^s(\\Gamma )}^2 \\end{equation*} Substitute the orthogonal decomposition \\(u=u_0+u_1\\) \\begin{align*} &amp;amp;= c_1^A \\lVert u_0+u_1 \\rVert _{H^s(\\Gamma )}^2 \\\\ &amp;amp;= c_1^A \\left ( \\lVert u_0 \\rVert _{H^s(\\Gamma )}^2 + \\sum _{p=1}^m \\langle u,v_p \\rangle _{H^{s-\\alpha }(\\Gamma )}^2 \\right ) \\end{align*} &lt;/p&gt;&lt;!-- l. 240 --&gt;&lt;p class=&#39;noindent&#39;&gt;Use the boundedness of \\(\\dot {B}^{-1}\\) and the reciprocal relationship between its spectrum and that of \\(B\\) \\begin{align*} &amp;amp;\\geq c_1^A \\left ( c_1^B \\langle \\dot {B}^{-1}u_0,u_0 \\rangle _{H^{s-\\alpha }(\\Gamma )} + \\sum _{p=1}^m \\langle u,v_p \\rangle _{H^{s-\\alpha }(\\Gamma )}^2 \\right ) \\\\ &amp;amp;\\geq c_1^A \\min \\{ c_1^B, 1 \\} \\left ( \\langle \\dot {B}^{-1}u_0,u_0 \\rangle _{H^{s-\\alpha }(\\Gamma )} + \\sum _{p=1}^m \\langle u,v_p \\rangle _{H^{s-\\alpha }(\\Gamma )}^2 \\right ). \\end{align*} &lt;/p&gt;&lt;!-- l. 248 --&gt;&lt;p class=&#39;noindent&#39;&gt;Therefore, we have \\begin{equation} c_1^A \\min \\{ c_1^B,1 \\} c(u,u) \\leq \\langle Au,u \\rangle _{H^{s-\\alpha }(\\Gamma )}. \\end{equation}&lt;a id=&#39;x1-4003r19&#39;&gt;&lt;/a&gt; &lt;/p&gt;&lt;!-- l. 253 --&gt;&lt;p class=&#39;noindent&#39;&gt;In these two special cases &lt;/p&gt;&lt;ol class=&#39;enumerate1&#39;&gt; when \\(V^s(\\Gamma ,A) \\subseteq V^{s,0}(\\Gamma ,B)\\), the orthogonal decomposition of \\(u\\) degenerates to \\(u=u_0\\), \\(\\mathrm {ker}(B) = \\{ 0 \\}\\) &lt;/ol&gt; &lt;!-- l. 258 --&gt;&lt;p class=&#39;noindent&#39;&gt;the bilinear form \\(c(u,u)\\) degenerates to \\(\\langle \\dot {B}^{-1}u,u \\rangle \\). &lt;/p&gt;&lt;/li&gt; &lt;li class=&#39;itemize&#39;&gt; &lt;!-- l. 260 --&gt;&lt;p class=&#39;noindent&#39;&gt;Method 2: Build an approximate operator \\(\\tilde {B}\\) for \\(B\\), which is elliptic on the whole domain \\(H^{s-2\\alpha }(\\Gamma )\\). Then use its inverse to build the bilinear form \\(c(u,w) = \\langle \\tilde {B}^{-1}u,w \\rangle _{H^{s-\\alpha }(\\Gamma )}\\). &lt;/p&gt;&lt;!-- l. 262 --&gt;&lt;p class=&#39;noindent&#39;&gt;For any \\(u\\in H^{s-2\\alpha }(\\Gamma )\\), it also has an orthogonal decomposition in \\(V^{s-2\\alpha }(\\Gamma ,B) \\oplus \\mathrm {ker}(B)\\) as \\begin{equation} u = u_0 + u_1 = u_0 + \\sum _{p=1}^m \\langle u,v_p \\rangle _{H^{s-2\\alpha }(\\Gamma )} v_p(x), \\end{equation}&lt;a id=&#39;x1-4008r20&#39;&gt;&lt;/a&gt; where the inner product \\(\\langle u,v_p \\rangle _{H^{s-2\\alpha }(\\Gamma )}\\) is the expansion coefficient for \\(u_1\\) and \\(v_p\\) is the corresponding basis element. Then we try to construct \\(\\tilde {B}\\) which is \\(H^{s-2\\alpha }(\\Gamma )\\)-elliptic. \\begin{align*} \\lVert u \\rVert _{H^{s-2\\alpha }(\\Gamma )}^2 &amp;amp;= \\lVert u_0 + u_1 \\rVert _{H^{s-2\\alpha }(\\Gamma )}^2 \\\\ &amp;amp;= \\left ( \\lVert u_0 \\rVert _{H^{s-2\\alpha }(\\Gamma )}^2 + \\sum _{p=1}^m \\langle u,v_p \\rangle _{H^{s-2\\alpha }(\\Gamma )}^2 \\right ) \\end{align*} &lt;/p&gt;&lt;!-- l. 273 --&gt;&lt;p class=&#39;noindent&#39;&gt;Use the \\(V^{s-2\\alpha}(\\Gamma,B)\\)-ellipticity of \\(B\\) \\begin{align*} &amp;amp;\\leq \\left ( \\frac {1}{c_1^B} \\langle Bu_0, u_0 \\rangle _{H^{s-\\alpha }(\\Gamma )} + \\sum _{p=1}^m \\langle u,v_p \\rangle _{H^{s-2\\alpha }(\\Gamma )}^2 \\right ) \\\\ &amp;amp;\\leq \\max \\{ \\frac {1}{c_1^B},1 \\} \\left ( \\langle Bu_0, u_0 \\rangle _{H^{s-\\alpha }(\\Gamma )} + \\sum _{p=1}^m \\langle u,v_p \\rangle _{H^{s-2\\alpha }(\\Gamma )}^2 \\right ). \\end{align*} &lt;/p&gt;&lt;!-- l. 281 --&gt;&lt;p class=&#39;noindent&#39;&gt;It is equivalent to \\begin{equation} c_1^{\\tilde {B}} \\lVert u \\rVert _{H^{s-2\\alpha }(\\Gamma )}^2 \\leq \\left ( \\langle Bu_0, u_0 \\rangle _{H^{s-\\alpha }(\\Gamma )} + \\sum _{p=1}^m \\langle u,v_p \\rangle _{H^{s-2\\alpha }(\\Gamma )}^2 \\right ), \\end{equation}&lt;a id=&#39;x1-4009r21&#39;&gt;&lt;/a&gt; where \\(c_1^{\\tilde {B}} = 1/\\max \\{ \\frac {1}{c_1^B},1 \\} = \\min \\{ c_1^B,1 \\}\\). &lt;/p&gt;&lt;!-- l. 289 --&gt;&lt;p class=&#39;noindent&#39;&gt;On the other hand, we start from the boundedness of \\(B\\): \\begin{align*} \\lVert u \\rVert _{H^{s-2\\alpha }(\\Gamma )}^2 &amp;amp;= \\lVert u_0 + u_1 \\rVert _{H^{s-2\\alpha }(\\Gamma )}^2 \\\\ &amp;amp;= \\left ( \\lVert u_0 \\rVert _{H^{s-2\\alpha }(\\Gamma )}^2 + \\sum _{p=1}^m \\langle u,v_p \\rangle _{H^{s-2\\alpha }(\\Gamma )}^2 \\right ) \\\\ &amp;amp;\\geq \\left ( \\frac {1}{c_2^B} \\langle Bu_0,u_0 \\rangle _{H^{s-\\alpha }(\\Gamma )} + \\sum _{p=1}^m \\langle u,v_0 \\rangle _{H^{s-2\\alpha }(\\Gamma )}^2 \\right ) \\\\ &amp;amp;\\geq \\min \\{ \\frac {1}{c_2^B},1 \\} \\left ( \\langle Bu_0,u_0 \\rangle _{H^{s-\\alpha }(\\Gamma )} + \\sum _{p=1}^m \\langle u,v_0 \\rangle _{H^{s-2\\alpha }(\\Gamma )}^2 \\right ). \\end{align*} &lt;/p&gt;&lt;!-- l. 300 --&gt;&lt;p class=&#39;noindent&#39;&gt;So we have \\begin{equation} \\left ( \\langle Bu_0, u_0 \\rangle _{H^{s-\\alpha }(\\Gamma )} + \\sum _{p=1}^m \\langle u,v_p \\rangle _{H^{s-2\\alpha }(\\Gamma )}^2 \\right ) \\leq c_2^{\\tilde {B}} \\lVert u \\rVert _{H^{s-2\\alpha }(\\Gamma )}^2, \\end{equation}&lt;a id=&#39;x1-4010r22&#39;&gt;&lt;/a&gt; where \\(c_2^{\\tilde {B}} = \\max \\{ c_2^B,1 \\}\\). &lt;/p&gt;&lt;!-- l. 309 --&gt;&lt;p class=&#39;noindent&#39;&gt;Therefore, we can define the bilinear form related to \\(\\tilde {B}\\) as \\begin{equation} \\label {eq:bilinear-form-b} \\begin{aligned} \\langle \\tilde {B}u,w \\rangle _{H^{s-\\alpha }(\\Gamma )} &amp;amp;= \\langle Bu,w \\rangle _{H^{s-\\alpha }(\\Gamma )} + \\sum _{p=1}^m \\langle v,v_p \\rangle _{H^{s-2\\alpha }(\\Gamma )} \\langle w,v_p \\rangle _{H^{s-2\\alpha }(\\Gamma )} \\\\ &amp;amp;= \\langle Bu_0,w_0 \\rangle _{H^{s-\\alpha }(\\Gamma )} + \\sum _{p=1}^m \\langle v,v_p \\rangle _{H^{s-2\\alpha }(\\Gamma )} \\langle w,v_p \\rangle _{H^{s-2\\alpha }(\\Gamma )} \\quad \\forall u,w\\in H^{s-2\\alpha }(\\Gamma ). \\end{aligned} \\end{equation}&lt;a id=&#39;x1-4011r23&#39;&gt;&lt;/a&gt; It is both bounded and \\(H^{s-2\\alpha }(\\Gamma )\\)-elliptic. &lt;/p&gt;&lt;!-- l. 323 --&gt;&lt;p class=&#39;noindent&#39;&gt;The differences between the bilinear forms in Equation (&lt;a href=&#39;#x1-4001r17&#39;&gt;17&lt;!-- tex4ht:ref: eq:bilinear-form-c --&gt;&lt;/a&gt;) and (&lt;a href=&#39;#x1-4011r23&#39;&gt;23&lt;!-- tex4ht:ref: eq:bilinear-form-b --&gt;&lt;/a&gt;) are: &lt;/p&gt;&lt;ol class=&#39;enumerate1&#39;&gt; In Equation (23), \\(u\\) and \\(w\\) are projected to \\(v_p\\) via inner product, which is a true projection. In Equation (17), \\(u\\) and \\(w\\) are in a different space from \\(v_p\\). They are “projected” to \\(v_p\\) via duality pairing, which can be considered as a generalization of projection. In Equation (23), the basis used for function expansion is \\(\\{ v_p \\}_{p=1}^m\\). In Equation (17), the basis is \\(\\{ \\mathcal {J}v_p \\}_{p=1}^m\\). &lt;/ol&gt; &lt;/li&gt;&lt;/ul&gt; &lt;p class=&#39;noindent&#39;&gt; &lt;/p&gt; 4 Discretization of the preconditioning bilinear form According to (Steinbach and Wendland, section 4), Method 1 in Section 3 requires coordinate transformation between the finite element basis and the orthonormal basis adopted for the orthogonal decomposition \\(V_h^s(\\Gamma ,A) = V_h^0 \\oplus \\{ \\mathcal {J}v_p \\}_{p=1}^m\\), which is more complicated than Method 2. Meanwhile, (Steinbach, section 13.2 page 299) only introduces Method 2. So we stick to this method. Usually, the inverse operator \\(\\tilde {B}^{-1}\\) cannot be directly evaluated or discretized. We need to compute the discretized matrix of \\(\\tilde {B}\\) first, then use its inverse matrix to compute an approximation to the discretized matrix for the inverse operator \\(\\tilde {B}^{-1}\\). This can be visualized in the following diagram. Therefore, we have \\begin{equation} \\underline {\\tilde {B}^{-1}} \\approx \\mathcal {M}_{\\tilde {B}}^{\\mathrm {T}} \\tilde {\\mathcal {B}}^{-1} \\mathcal {M}_{\\tilde {B}}. \\end{equation} The matrix to be multiplied to both sides of the equation is \\begin{equation} \\mathcal {M}_{\\tilde {B}}^{-1} \\tilde {\\mathcal {B}} \\mathcal {M}_{\\tilde {B}}^{-\\mathrm {T}}. \\end{equation} The spectral equivalence between the above two matrices \\(\\underline {\\tilde {B}^{-1}}\\) and \\(\\mathcal {M}_{\\tilde {B}}^{\\mathrm {T}} \\tilde {\\mathcal {B}}^{-1} \\mathcal {M}_{\\tilde {B}}\\) can be proved using the theory given in (Steinbach and Wendland, section 3). This theory is described in a general form as below. \\(V\\) and \\(W\\) are two Hilbert spaces. \\(A: V \\rightarrow V&#39;\\) is a \\(V\\)-elliptic, self-adjoint and bounded operator, while \\(B: W \\rightarrow V&#39;\\) is any bounded operator. Define an operator \\(T: W \\rightarrow W&#39;\\) as \\(T = B&#39; A^{-1} B\\). In principle, it has a Galerkin matrix \\(\\mathcal {T}\\). In reality, this matrix cannot be directly computed. The Galerkin matrices for \\(A\\) and \\(B\\) are \\(\\mathcal {A}\\) and \\(\\mathcal {B}\\) respectively. Then we have another matrix \\begin{equation} \\tilde {\\mathcal {T}} = \\mathcal {B}^{\\mathrm {T}} \\mathcal {A}^{-1} \\mathcal {B}. \\end{equation} It can be proved that for any \\(w_h\\in W_h\\), \\begin{equation} ( \\tilde {\\mathcal {T}} \\underline {w}, \\underline {w} ) \\leq ( \\mathcal {T} \\underline {w}, \\underline {w} ). \\end{equation} If the stability condition or \\(\\inf \\sup \\) condition is satisfied \\begin{equation} \\inf _{0\\neq w_h\\in W_h} \\sup _{0\\neq v_h\\in V_h} \\frac {\\lvert \\langle Bw_h,v_h \\rangle _{V} \\rvert }{\\lVert w_h \\rVert _W \\lVert v_h \\rVert _V} \\geq c, \\end{equation} we also have \\begin{equation} \\gamma ( \\mathcal {T} \\underline {w}, \\underline {w} ) \\leq ( \\tilde {\\mathcal {T}} \\underline {w}, \\underline {w} ). \\end{equation} Therefore, \\(\\mathcal {T}\\) and \\(\\tilde {\\mathcal {T}}\\) are spectrally equivalent. The relationship between the spaces, operators and matrices are shown below. Coming back to the spectral equivalence between the two matrices \\(\\underline {\\tilde {B}^{-1}}\\) and \\(\\mathcal {M}_{\\tilde {B}}^{\\mathrm {T}} \\tilde {\\mathcal {B}}^{-1} \\mathcal {M}_{\\tilde {B}}\\), we can see that it is the special case when \\(W\\) is selected to be \\(V&#39;\\) and \\(B\\) is the identity operator on \\(V&#39;\\). The mass matrix \\(M_A\\) for the duality pairing from \\(V&#39;\\) to \\(V\\) is just the Galerkin matrix of \\(B\\). This is shown in the following figure. 5 Preconditioners for Laplace problem The operator equation for the Laplace problem with Dirichlet boundary condition is \\begin{equation} \\langle Vt, \\tau \\rangle _{\\Gamma } = \\langle (\\frac {1}{2}I + K)g, \\tau \\rangle _{\\Gamma } \\quad \\forall \\tau \\in H^{-1/2}(\\Gamma ). \\end{equation} The hypersingular operator \\(D\\) has an opposite order with respect to \\(V\\), which is naturally the preconditioner for this equation. However, according to here, \\(D\\) is not \\(H^{1/2}(\\Gamma )\\)-elliptic, but only \\(H_{\\ast }^{1/2}(\\Gamma )\\)-elliptic or \\(H_{\\ast \\ast }^{1/2}(\\Gamma )\\)-elliptic depending on which kind of inner product is desired. To avoid the complex of computing the natural density \\(w_{\\mathrm {eq}}\\), we choose \\(H_{\\ast \\ast }^{1/2}(\\Gamma )\\)-ellipticity. Then the bilinear form associated with the regularized or gauged operator \\(\\tilde {D}\\) is \\begin{equation} \\langle \\tilde {D}u,w \\rangle = \\langle Du,w \\rangle + \\langle u,1 \\rangle _{\\Gamma } \\langle w,1 \\rangle _{\\Gamma } \\quad \\forall u,w\\in H^{1/2}(\\Gamma ). \\end{equation} Then the inverse of the preconditioning matrix is \\begin{equation} \\mathcal {M}_{\\tilde {D}}^{-1} \\tilde {\\mathcal {D}} \\mathcal {M}_{\\tilde {D}}^{-\\mathrm {T}}, \\end{equation} where \\(\\mathcal {M}_{\\tilde {D}}\\) is the mass matrix for the duality pairing from \\(H^{-1/2}(\\Gamma )\\) to \\(H^{1/2}(\\Gamma )\\). The operator equation for the Laplace problem with Neumann boundary condition is \\begin{equation} \\langle Du, v \\rangle _{\\Gamma } + \\alpha \\langle u,1 \\rangle _{\\Gamma } \\langle v,1 \\rangle _{\\Gamma } = \\langle (\\frac {1}{2}I - K&#39;)g,v \\rangle _{\\Gamma } \\quad \\forall v\\in H^{1/2}(\\Gamma ). \\end{equation} The single layer potential operator \\(V\\) has an opposite order with respect to \\(D\\), which is the preconditioner. When \\(d=3\\), \\(V\\) is \\(H^{-1/2}(\\Gamma )\\)-elliptic, so we directly obtain the inverse of the preconditioning matrix \\begin{equation} \\mathcal {M}_V^{-1} \\mathcal {V} \\mathcal {M}_V^{-\\mathrm {T}}, \\end{equation} where \\(\\mathcal {M}_V\\) is the mass matrix for the duality pairing from \\(H^{1/2}(\\Gamma )\\) to \\(H^{-1/2}(\\Gamma )\\). References    William Charles Hector McLean. Strongly Elliptic Systems and Boundary Integral Equations. Cambridge University Press. ISBN 978-0-521-66375-5.    Olaf Steinbach. Numerical Approximation Methods for Elliptic Boundary Value Problems: Finite and Boundary Elements. Springer Science &amp; Business Media. ISBN 978-0-387-31312-2.    Olaf Steinbach and Wolfgang L. Wendland. The construction of some efficient preconditioners in the boundary element method. 9(1-2):191–216. URL http://link.springer.com/article/10.1023/A:1018937506719.","headline":"General theory about the construction of preconditioning bilinear form in BEM","mainEntityOfPage":{"@type":"WebPage","@id":"https://jihuan-tian.github.io/math/2024/11/16/general-theory-about-the-construction-of-preconditioning-bilinear-form-in-bem.html"},"url":"https://jihuan-tian.github.io/math/2024/11/16/general-theory-about-the-construction-of-preconditioning-bilinear-form-in-bem.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/font.css">
  <link rel="stylesheet" href="/assets/main.css">
  <link rel="stylesheet" href="/assets/css/htmlize-syntax-highlight.css">
  
    <link rel="stylesheet" href="/assets/css/make4ht.css">
  
  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon.ico?">
  <link href="https://fonts.googlefonts.cn/css?family=EB+Garamond" rel="stylesheet">
  <script type='text/javascript' src='https://platform-api.sharethis.com/js/sharethis.js#property=67ba859acde8790019eafb38&product=inline-share-buttons' async='async'></script><link type="application/atom+xml" rel="alternate" href="https://jihuan-tian.github.io/feed.xml" title="止于至善" />
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/SVG"],
    extensions: ["tex2jax.js", "TeX/AMSmath.js", "TeX/AMSsymbols.js", "TeX/noUndefined.js", "TeX/AMScd.js"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      skipTags: ["script","noscript","style","textarea","pre","code"],
      processEscapes: true,
      processEnvironments: true,
      preview: "TeX"
    },
    TeX: {
      Macros: {
        intd: "\\,{\\rm d}",
        diff: "{\\rm d}",
        Diff: "{\\rm D}",
        pdiff: "\\partial",
        DD: ["\\frac{\\diff}{\\diff #2}\\left( #1 \\right)", 2],
        Dd: ["\\frac{\\diff #1}{\\diff #2}", 2],
        PD: ["\\frac{\\pdiff}{\\pdiff #2}\\left( #1 \\right)", 2],
        Pd: ["\\frac{\\pdiff #1}{\\pdiff #2}", 2],
        rme: "{\\rm e}",
        rmi: "{\\rm i}",
        rmj: "{\\rm j}",
        vect: ["\\boldsymbol{#1}", 1],
        dform: ["\\overset{\\rightharpoonup}{\\boldsymbol{#1}}", 1],
        cochain: ["\\overset{\\rightharpoonup}{#1}", 1],
        bigabs: ["\\bigg\\lvert#1\\bigg\\rvert", 1],
        Abs: ["\\big\\lvert#1\\big\\rvert", 1],
        abs: ["\\lvert#1\\rvert", 1],
        bignorm: ["\\bigg\\lVert#1\\bigg\\rVert", 1],
        Norm: ["\\big\\lVert#1\\big\\rVert", 1],
        norm: ["\\lVert#1\\rVert", 1],
        normvect: "\\vect{n}",
        ouset: ["\\overset{#3}{\\underset{#2}{#1}}", 3],
        cscript: ["\\;\\; #1", 1],
        suchthat: "\\textit{S.T. }",
        prefstar: "\\ast",
        restrict: "\\big\\vert",
        sgn: "{\\rm sgn}",
        erf: "{\\rm erf}",
        Bd: "{\\rm Bd}",
        Int: "{\\rm Int}",
        dim: "{\\rm dim}",
        rank: "{\\rm rank}",
        range: "{\\rm range}",
        divergence: "{\\rm div}",
        curl: "{\\rm curl}",
        grad: "{\\rm grad}",
        tr: "{\\rm tr}",
        lhs: "{\\rm LHS}",
        rhs: "{\\rm RHS}",
        span: "{\\rm span}",
        diag: "{\\rm diag}",
        argmin: "{\\rm argmin}",
        argmax: "{\\rm argmax}",
        esssup: "{\\rm esssup}",
        essinf: "{\\rm essinf}",
        kernel: "{\\rm ker}",
        image: "{\\rm Im}",
        diam: "{\\rm diam}",
        dist: "{\\rm dist}",
        const: "{\\rm const}"
      },
      equationNumbers: { autoNumber: "AMS" }
    }
  });
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_SVG"></script>

  
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">止于至善</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
          <!-- Enforce a fixed order for my categories. -->
          <a class="page-link" href="/math/">Math</a>
          <a class="page-link" href="/computer/">Computer</a>
          <a class="page-link" href="/thoughts/">Thoughts</a>
          <a class="page-link" href="/tags/">Tags</a>
          <a class="page-link" href="/search/">Search</a>
          <a class="page-link" href="/about/">About</a>
        </div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">General theory about the construction of preconditioning bilinear form in BEM</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2024-11-16T00:00:00+08:00" itemprop="datePublished">Nov 16, 2024 &nbsp;

        
          Categories:
          
            <a href="/math">Math</a>
          
         &nbsp;
        
          Tags:
          
            <a href="/tags/PDE">PDE</a>
          
            <a href="/tags/BEM">BEM</a>
          
        
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
       <h3 class='likesectionHead'><a id='x1-1000'></a>Contents</h3>
   <div class='tableofcontents'>
    <span class='sectionToc'>1 <a href='#x1-20001' id='QQ2-1-2'>Inverse operator of the preconditioner</a></span>
<br />    <span class='sectionToc'>2 <a href='#x1-30002' id='QQ2-1-3'>Orthogonal decomposition of the domain \(V^s(\Gamma ,A)\) of \(A\)</a></span>
<br />    <span class='sectionToc'>3 <a href='#x1-40003' id='QQ2-1-4'>Define the preconditioning bilinear form</a></span>
<br />    <span class='sectionToc'>4 <a href='#x1-50004' id='QQ2-1-5'>Discretization of the preconditioning bilinear form</a></span>
<br />    <span class='sectionToc'>5 <a href='#x1-60005' id='QQ2-1-6'>Preconditioners for Laplace problem</a></span>
   </div>
<!-- l. 24 --><p class='indent'>   In BEM, the preconditioning technique is indispensable for solving a large scale PDE, when an iterative
solver, such as CG, GMRES, MinRES, BiCGStab, is adopted. Without a preconditioner, the number of iterations
will significantly increase with the deterioration of the system matrix’s condition number. A large condition
number can be caused by a large number of DoFs, distinct mesh element size, sharp corners in the geometry,
etc.
</p><!-- l. 26 --><p class='indent'>   The procedure for constructing the Galerkin matrix for a preconditioner is summarized into four
steps.
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-1002x1'>
     <!-- l. 28 --><p class='noindent'>Find a preconditioning operator having an opposite order with respect to the key operator in the
     original PDE in the theoretical framework of pseudodifferential operators (see <a href='/math/2024/11/11/basic-ideas-of-operator-preconditioning.html#x1-20001'>Pseudodifferential
     operator</a>, <a href='/math/2024/11/11/basic-ideas-of-operator-preconditioning.html#x1-30002'>Operator preconditioning based on pseudodifferential operator of opposite orders</a>). In
     BEM, this is a trivial task thanks to the properties of boundary integral operators (see <a href='/math/2024/11/11/basic-ideas-of-operator-preconditioning.html#x1-40003'>Boundary
     integral operators considered as pseudodifferential operators</a>).
     </p></li>
<li class='enumerate' id='x1-1004x2'>
                                                                                               
                                                                                               
     <!-- l. 29 --><p class='noindent'>Construct  an  inverse  operator  for  the  preconditioner  obtained  from  the  previous  step.  If  the
     preconditioner is not elliptic on its whole domain, we should construct a Moore-Penrose pseudo
     inverse  or  generalized  inverse  operator  instead.  Alternatively,  we  can  build  an  approximate
     operator for the preconditioner, which is elliptic on the whole domain. Then its inverse operator is
     available.
     </p></li>
<li class='enumerate' id='x1-1006x3'>
     <!-- l. 30 --><p class='noindent'>Construct  a  bilinear  form  for  the  above  inverse  operator,  which  is  spectrally  equivalent  to  the
     bilinear form in the variational form of the PDE. The lower bound and upper bound constants
     in  this  spectral  equivalence  condition  do  not  depend  on  the  geometric  discretization  and  finite
     element (function space) discretization.
     </p></li>
<li class='enumerate' id='x1-1008x4'>
     <!-- l. 31 --><p class='noindent'>Discretize the above bilinear form, whose inverse matrix is to be multiplied to the discretized linear
     system for the variational form of the PDE. Since the inverse operator associated with this bilinear
     form cannot be directly evaluated, we will compute an approximate version which is based on the
     inverse matrix of the Galerkin matrix for the preconditioning operator. A general matrix spectral
     equivalence theorem guarantees the equivalence between this approximate version and the exact
     version.</p></li></ol>
<!-- l. 34 --><p class='indent'>   In the following, we’ll introduce the basic ideas and theoretical details. Finally, we present the
preconditioners used in the Laplace problem with either Dirichlet or Neumann boundary condition.
</p>
   <h3 class='sectionHead'><span class='titlemark'>1    </span> <a id='x1-20001'></a>Inverse operator of the preconditioner</h3>
<!-- l. 37 --><p class='noindent'>The operator \(A: V^s(\Gamma ,A):= H^s(\Gamma )_{/\mathrm {ker}(A)} \rightarrow H^{s-2\alpha }(\Gamma )\) in the original PDE is self-adjoint (in the sense of adjointness in Banach spaces), bounded and
\(V^s(\Gamma ,A)\)-elliptic. As a pseudodifferential operator, its order is \(2\alpha \).
</p><!-- l. 39 --><p class='indent'>   The preconditioning operator \(B: H^{s-2\alpha }(\Gamma ) \rightarrow H^s(\Gamma )\) is self-adjoint, bounded and \(V^{s-2\alpha ,0}(\Gamma ,B) := H^{s-2\alpha }(\Gamma )_{/\mathrm {ker}(B)}\)-elliptic. Its order is \(-2\alpha \).
</p><!-- l. 41 --><p class='indent'>   Define the inverse operator \(B^{-1}: H^s(\Gamma ) \rightarrow H^{s-2\alpha }(\Gamma )\) of \(B\) which should be spectrally equivalent to \(A\). Assume \(B\) has a closed range and
also note \(B\) is self-adjoint, then \begin{equation}  \mathrm {Im}(B) = (\mathrm {ker}(B'))^{\circ } = (\mathrm {ker}(B))^{\circ }.  \end{equation}<a id='x1-2001r1'></a> If the operator \(B\) has a non-trivial kernel and let \(H^s(\Gamma )\) be decomposed as \(H^s(\Gamma ) = (\mathrm {ker}(B))^{\circ } \oplus [(\mathrm {ker}(B))^{\circ }]^{\mathrm {c}}\), a generalized
inverse \(B^{+}: H^s(\Gamma ) \rightarrow H^{s-2\alpha }(\Gamma )\) can be defined \begin{equation}  B^{+} = \begin {cases} \dot {B}^{-1}(y) &amp; y\in (\mathrm {ker}(B))^{\circ } \\ 0 &amp; y\in [(\mathrm {ker}(B))^{\circ }]^{\mathrm {c}} \end {cases},  \end{equation}<a id='x1-2002r2'></a> where \((\mathrm {ker}(B))^{\mathrm {c}}\) is the complement of \(\mathrm {ker}(B)\) within \(H^{s-2\alpha }(\Gamma )\) and \(\dot {B}^{-1}: (\mathrm {ker}(B))^{\circ } \rightarrow (\mathrm {ker}(B))^{\mathrm {c}}\) is a bijective map. Even though the above
adjointness and generalized inverse are discussed in the context of Banach spaces, the Sobolev space \(H^{s-2\alpha }(\Gamma )\) is actually
a Hilbert space. Therefore, the complement subspace \((\mathrm {ker}(B))^{\mathrm {c}}\) is also the orthogonal complement subspace
\((\mathrm {ker}(B))^{\perp }\).
</p><!-- l. 54 --><p class='indent'>   The domain and range spaces of \(\dot {B}^{-1}\) are defined as (<a href='#XSteinbachConstruction1998'>Steinbach and Wendland</a>) \begin{equation}  \begin{aligned} V^{s,0}(\Gamma ,B) &amp;:= (\mathrm {ker}(B))^{\circ } = \left \{ v\in H^s(\Gamma ): \langle v,v_p \rangle _{H^{s-\alpha }(\Gamma )} = 0 \right \} \\ V^{s-2\alpha ,0}(\Gamma ,B) &amp;:= (\mathrm {ker}(B))^{\perp } = \left \{ v\in H^{s-2\alpha }(\Gamma ): \langle v,v_p \rangle _{H^{s-2\alpha }(\Gamma )}=0 \right \} \end{aligned} \quad \forall v_p\in \mathrm {ker}(B).  \end{equation}<a id='x1-2003r3'></a>
</p><!-- l. 65 --><p class='indent'>   To simultaneously ensure \(V^s(\Gamma ,A)\)-ellipticity of \(A\) and \(V^{s,0}(\Gamma ,B)\)-ellipticity of \(\dot {B}^{-1}\) which are required by the spectral equivalence
condition, the effective domain for \(A\) and \(\dot {B}^{-1}\) should be the intersection \(V^s(\Gamma ,A) \cap V^{s,0}(\Gamma ,B)\).
                                                                                               
                                                                                               
</p><!-- l. 67 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>2    </span> <a id='x1-30002'></a>Orthogonal decomposition of the domain \(V^s(\Gamma ,A)\) of \(A\)</h3>
<!-- l. 70 --><p class='noindent'>\(V^s(\Gamma ,A)\) is a subspace of \(H^s(\Gamma )\). \(H^s(\Gamma )\) is a Hilbert space and has orthogonal decomposition with respect to its subspace \(V^{s,0}(\Gamma ,B)\): \begin{equation}  H^s(\Gamma )=V^{s,0}(\Gamma ,B) \oplus [ V^{s,0}(\Gamma ,B) ]^{\perp }.  \end{equation}<a id='x1-3001r4'></a> As
defined above, \(V^{s,0}(\Gamma ,B)\) is the annihilator of \(\mathrm {ker}(B)\) in the sense of duality pairing, which can be considered as
a generalization of the concept of orthogonality (based on inner product) in a Hilbert space. Its
orthogonal complement subspace \([ V^{s,0}(\Gamma ,B) ]^{\perp }\) can then be considered as the counterpart of \(\mathrm {ker}(B) \subset H^{s-2\alpha }(\Gamma )\) but in the dual space
\(H^s(\Gamma )\).
</p><!-- l. 76 --><p class='indent'>   For any \(u(x)\in V^s(\Gamma ,A)\), it can be uniquely decomposed as \begin{equation}  u(x) = u_0(x) + u_1(x),  \end{equation}<a id='x1-3002r5'></a> where \(u_0(x)\in V^{s,0}(\Gamma ,B)\) and \(u_1(x)\in [ V^{s,0}(\Gamma ,B) ]^{\perp }\). Obviously, \(u_0(x)\in V^s(\Gamma ,A) \cap V^{s,0}(\Gamma ,B)\). Because \([ V^{s,0}(\Gamma ,B) ]^{\perp }\) is the counterpart of \(\mathrm {ker}(B)\) in \(H^s(\Gamma )\), if we
let \(\{ v_p \}_{p=1}^m\) be the orthonormal basis of \(\mathrm {ker}(B)\), \(u_1(x)\) can be constructed by “projecting” \(u(x)\) to the basis \(\{ v_p \}_{p=1}^m\) in the sense of duality pairing
as below. \begin{equation}  u_1(x)=\sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} (\mathcal {J}^{-2\alpha }v_p)(x).  \end{equation}<a id='x1-3003r6'></a> In this representation, \(\langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}\) is the expansion coefficient and \(\mathcal {J}^{-2\alpha }v_p\) is the basis element for \([ V^{s,0}(\Gamma ,B) ]^{\perp }\) in \(H^s(\Gamma )\), which is the
counterpart of the basis element \(v_p\) for \(\mathrm {ker}(B)\) in \(H^{s-2\alpha }(\Gamma )\). \(\mathcal {J}: \mathcal {S}(\mathbb {R}^d) \rightarrow \mathcal {S}(\mathbb {R}^d)\) is the Bessel potential operator on rapidly decreasing functions. \(\mathcal {J}\) with
an order \(s\) is defined as (<a href='#XSteinbachNumerical2007'>Steinbach</a>, page 32) \begin{equation}  \label {eq:bessel-potential-op} \mathcal {J}^su(x) := ( 2\pi )^{-d/2} \int _{\mathbb {R}^d} ( 1+\lvert \xi \rvert ^2 )^{s/2} \hat {u}(\xi ) \mathrm {e}^{\mathrm {i}\langle x,\xi \rangle } \mathrm {d}\xi ,  \end{equation}<a id='x1-3004r7'></a> where \(\hat {u}(\xi )\) is the Fourier transform of \(u(x)\). Obviously, the effect of \(\mathcal {J}^s\) on \(u(x)\) is
modifying its frequency spectrum by multiplying a polynomial about \(\xi \) with an order \(s\). According to the
pseudodifferential operator theory, \(\mathcal {J}^s\) is equivalent to a partial differential operator with an order \(s\). If
we restrict the domain of \(\mathcal {J}^s\) to a smaller space instead of \(\mathcal {S}(\mathbb {R}^d)\), such as the Sobolev space \(H^t(\Gamma )\), \(\mathcal {J}^s\) maps from \(H^t(\Gamma )\)
to \(H^{t-s}(\Gamma )\), which lowers the Sobolev space order by \(s\). In the above representation for \(u_1(x)\), \(\mathcal {J}^{-2\alpha }\) maps from \(H^{s-2\alpha }(\Gamma )\) to
\(H^s(\Gamma )\).
</p><!-- l. 92 --><p class='indent'>   According to (<a href='#XMcLeanStrongly2000'>McLean</a>, page 75), for any \(s,t\in \mathbb {R}\) and \(u,v\in \mathcal {S}(\mathbb {R}^d)\), Bessel polynomial operators have these properties:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-3006x1'>
     <!-- l. 94 --><p class='noindent'>\(\mathcal {J}^{s+t} = \mathcal {J}^s \mathcal {J}^t\)
     </p></li>
<li class='enumerate' id='x1-3008x2'>
     <!-- l. 95 --><p class='noindent'>\(( \mathcal {J}^s )^{-1} = \mathcal {J}^{-s}\)
     </p></li>
<li class='enumerate' id='x1-3010x3'>
     <!-- l. 96 --><p class='noindent'>\(\mathcal {J}^0 = \text {identity operator}\)
     </p></li>
<li class='enumerate' id='x1-3012x4'>
     <!-- l. 97 --><p class='noindent'>\(\langle \mathcal {J}^s u, v \rangle =\langle u,\mathcal {J}^s v \rangle \)
     </p><!-- l. 99 --><p class='noindent'>\(\langle \cdot ,\cdot \rangle \) can  be  either  considered  as  an  \(L_2\)  inner  product  of  two  functions  in  \(\mathcal {S}(\mathbb {R}^d)\)  or  as  applying  a  tempered
     distribution  in  \(\mathcal {S}'(\mathbb {R}^d)\)  to  a  rapidly  decreasing  function  in  \(\mathcal {S}(\mathbb {R}^d)\).  For  example,  we  can  treat  \(u\)  as  a  tempered
     distribution whose kernel function is \(u\). Then \(\mathcal {J}^s\) in \(\langle \mathcal {J}^s u,v \rangle \) is the Bessel potential operator on \(\mathcal {S}'(\mathbb {R}^d)\) and \(\mathcal {J}^s\) in \(\langle u,\mathcal {J}^s v \rangle \) is the
     Bessel potential operator on \(\mathcal {S}(\mathbb {R})^d\).</p></li></ol>
<!-- l. 102 --><p class='indent'>   The Sobolev space \(H^s(\mathbb {R}^d)\) is defined as (<a href='#XSteinbachNumerical2007'>Steinbach</a>, page 73) \begin{equation}  H^s(\mathbb {R}^d) := \left \{ v\in \mathcal {S}'(\mathbb {R}^d): \mathcal {J}^sv \in L_2(\mathbb {R}^d) \right \}.  \end{equation}<a id='x1-3013r8'></a> The inner product is for any \(u,v\in H^s(\mathbb {R}^d)\), \begin{equation}  \langle u,v \rangle _{H^s(\mathbb {R}^d)} := \langle \mathcal {J}^su, \mathcal {J}^sv \rangle _{L_2(\mathbb {R}^d)}.  \end{equation}<a id='x1-3014r9'></a> The norm definition is
\begin{equation}  \lVert u \rVert _{H^s(\mathbb {R}^d)} := \lVert \mathcal {J}^su \rVert _{L_2(\mathbb {R}^d)}.  \end{equation}<a id='x1-3015r10'></a>
                                                                                               
                                                                                               
</p><!-- l. 117 --><p class='indent'>   From the above, we also have \begin{equation}  \langle u,v \rangle _{H^s(\mathbb {R}^d)} = \langle \mathcal {J}^su, \mathcal {J}^sv \rangle _{L_2(\mathbb {R}^d)} = \langle \mathcal {J}^s \mathcal {J}^s u,v \rangle = \langle \mathcal {J}^{2s}u,v \rangle .  \end{equation}<a id='x1-3016r11'></a>
</p><!-- l. 124 --><p class='indent'>   The above \(\mathcal {J}^{-2\alpha }\) is actually the Riesz map from \(H^{s-2\alpha }(\Gamma )\) to \(H^s(\Gamma )\), which satisfies \begin{equation}  \langle \mathcal {J}^{-2\alpha }u,v \rangle _{H^{s-\alpha }(\Gamma )} = \langle u,v \rangle _{H^{s-2\alpha }(\Gamma )} \quad \forall u,v\in H^{s-2\alpha }(\Gamma ).  \end{equation}<a id='x1-3017r12'></a> This can be proved by using the properties of \(\mathcal {J}\).
The left hand side of the above equation is \begin{equation}  \langle \mathcal {J}^{-2\alpha }u,v \rangle _{H^{s-\alpha }(\Gamma )} = \langle \mathcal {J}^{-\alpha }u, \mathcal {J}^{-\alpha }v \rangle _{H^{s-\alpha }(\Gamma )},  \end{equation}<a id='x1-3018r13'></a> where \(\langle \cdot ,\cdot \rangle _{H^{s-\alpha }(\Gamma )}\) on the left hand side is the duality pairing between \(H^s(\Gamma )\) and \(H^{s-2\alpha }(\Gamma )\),
while on the right hand side it is the inner product in \(H^{s-\alpha }(\Gamma )\). Because both \(\mathcal {J}^{-\alpha }u\) and \(\mathcal {J}^{-\alpha }v\) belong to \(H^{s-\alpha }(\Gamma )\), according to the norm
definition, \begin{equation}  \langle \mathcal {J}^{-\alpha }u, \mathcal {J}^{-\alpha }v \rangle = \langle \mathcal {J}^{s-\alpha }\mathcal {J}^{-\alpha }u, \mathcal {J}^{s-\alpha }\mathcal {J}^{-\alpha }v \rangle _{L_2(\Gamma )} = \langle \mathcal {J}^{s-2\alpha }u,\mathcal {J}^{s-2\alpha }v \rangle _{L_2(\Gamma )}.  \end{equation}<a id='x1-3019r14'></a> This is just the same as \(\langle u,v \rangle _{H^{s-2\alpha }(\Gamma )}\).
</p><!-- l. 142 --><p class='indent'>   We can also show that \(u_0(x)\) is orthogonal to \(u_1(x)\). \begin{equation}  \begin{aligned} \langle u_0,u_1 \rangle _{H^s(\Gamma )} &amp;= \langle u_0, \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \mathcal {J}^{-2\alpha }v_p \rangle _{H^s(\Gamma )} \\ &amp;= \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle u_0,\mathcal {J}^{-2\alpha }v_p \rangle _{H^s(\Gamma )} \\ &amp;= \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle \mathcal {J}^su_0,\mathcal {J}^s \mathcal {J}^{-2\alpha }v_p \rangle _{L_2(\Gamma )} \\ &amp;= \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle \mathcal {J}^su_0,\mathcal {J}^{s-2\alpha }v_p \rangle _{L_2(\Gamma )}, \end{aligned}  \end{equation}<a id='x1-3020r15'></a> where \begin{equation}  \langle \mathcal {J}^su_0,\mathcal {J}^{s-2\alpha }v_p \rangle _{L_2(\Gamma )} = \langle u_0,v_p \rangle _{H^{s-\alpha }(\Gamma )}.  \end{equation}<a id='x1-3021r16'></a> Because \(u_0\in V^{s,0}(\Gamma ,B)\), which is the annihilator of \(\mathrm {ker}(B)\), the above expression is
zero.
</p><!-- l. 162 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>3    </span> <a id='x1-40003'></a>Define the preconditioning bilinear form</h3>
<!-- l. 166 --><p class='noindent'>Now we define the preconditioning bilinear form \(c(u,w)\), which is spectrally equivalent to \(a(u,w)=\langle Au,w \rangle \)
</p>
     <ul class='itemize1'>
     <li class='itemize'>
     <!-- l. 169 --><p class='noindent'>Method 1: Directly construct \(c(u,w)\) based on the generalized inverse \(B^{+}\).
     </p><!-- l. 171 --><p class='noindent'><span class='p1xb-x-x-109'>Basic idea</span>:  use the above orthogonal decomposition and construct the bilinear form \(c(u,u)\),  which is
     spectrally equivalent to \(a(u,u)=\langle Au,u \rangle \).
     </p><!-- l. 173 --><p class='noindent'>Use the boundedness of \(a(\cdot ,\cdot )\) or \(A\), for all \(u\in V^s(\Gamma ,A)\) \begin{equation*}  \langle Au,u \rangle _{H^{s-\alpha }(\Gamma )} \leq c_2^A \lVert u \rVert _{H^s(\Gamma )}^2  \end{equation*} Substitute the orthogonal decomposition \(u=u_0+u_1\) \begin{equation*}  = c_2^A \lVert u_0 + u_1 \rVert _{H^s(\Gamma )}^2  \end{equation*} Because we are dealing with
     Hilbert space, \begin{equation*}  = c_2^A (\langle u_0,u_0 \rangle + \langle u_0,u_1 \rangle + \langle u_1,u_0 \rangle + \langle u_1,u_1 \rangle )  \end{equation*} Because \(u_0\) is orthogonal to \(u_1\), \begin{align*}  &amp;= c_2^A (\lVert u_0 \rVert _{H^s(\Gamma )}^2 + \lVert u_1 \rVert _{H^s(\Gamma )}^2) \\ &amp;= c_2^A \left ( \lVert u_0 \rVert _{H^s(\Gamma )}^2 + \left \langle \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \mathcal {J}^{-2\alpha }v_p, \sum _{q=1}^m \langle u,v_q \rangle _{H^{s-\alpha }(\Gamma )} \mathcal {J}^{-2\alpha }v_q \right \rangle \right ) \\ &amp;= c_2^A \left ( \lVert u_0 \rVert _{H^s(\Gamma )}^2 + \sum _{p=1}^m\sum _{q=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle u,v_q \rangle _{H^{s-\alpha }(\Gamma )} \langle \mathcal {J}^{-2\alpha }v_p, \mathcal {J}^{-2\alpha }v_q \rangle _{H^s(\Gamma )} \right ) \\ &amp;= c_2^A \left ( \lVert u_0 \rVert _{H^s(\Gamma )}^2 + \sum _{p=1}^m\sum _{q=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle u,v_q \rangle _{H^{s-\alpha }(\Gamma )} \langle v_p,v_q \rangle _{H^{s-2\alpha }(\Gamma )} \right )  \end{align*}
     </p><!-- l. 201 --><p class='noindent'>Because \(\{ v_p \}_{p=1}^m\) is an orthonormal basis, \begin{equation*}  = c_2^A \left ( \lVert u_0 \rVert _{H^s(\Gamma )}^2 + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right )  \end{equation*} Use the \(V^{s,0}(\Gamma,B)\)-ellipticity of \(\dot {B}^{-1}\) and the reciprocal relationship
     between its spectrum and that of \(B\), \begin{align*}  &amp;\leq c_2^A \left ( c_2^B \langle \dot {B}^{-1}u_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right ) \\ &amp;\leq c_2^A \max \{ c_2^B,1 \} \left ( \langle \dot {B}^{-1}u_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{k=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right ).  \end{align*}
     </p><!-- l. 214 --><p class='noindent'>If we define the bilinear form \(c(u,w)\) as \begin{equation}  \label {eq:bilinear-form-c} \begin{aligned} c(u,w) &amp;:= \langle B^{\dagger }u,w \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle w,v_p \rangle _{H^{s-\alpha }(\Gamma )} \\ &amp;= \langle \dot {B}^{-1}u_0,w_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle w,v_p \rangle _{H^{s-\alpha }(\Gamma )} \quad \forall u,w\in H^s(\Gamma ) \end{aligned},  \end{equation}<a id='x1-4001r17'></a> it satisfies the upper part of the spectral equivalence condition
     \begin{equation}  \langle Au,u \rangle _{H^{s-\alpha }(\Gamma )} \leq c_2^A \max \{ c_2^{B},1 \} c(u,u).  \end{equation}<a id='x1-4002r18'></a>
     </p><!-- l. 230 --><p class='noindent'>Then we prove the lower part of the spectral equivalence condition. Use the \(V^s(\Gamma,A)\)-ellipticity of \(A\), for all \(u\in V^s(\Gamma ,A)\)
     \begin{equation*}  \langle Au,u \rangle _{H^{s-\alpha }(\Gamma )} \geq c_1^A \lVert u \rVert _{H^s(\Gamma )}^2  \end{equation*} Substitute the orthogonal decomposition \(u=u_0+u_1\) \begin{align*}  &amp;= c_1^A \lVert u_0+u_1 \rVert _{H^s(\Gamma )}^2 \\ &amp;= c_1^A \left ( \lVert u_0 \rVert _{H^s(\Gamma )}^2 + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right )  \end{align*}
     </p><!-- l. 240 --><p class='noindent'>Use the boundedness of \(\dot {B}^{-1}\) and the reciprocal relationship between its spectrum and that of \(B\)
     \begin{align*}  &amp;\geq c_1^A \left ( c_1^B \langle \dot {B}^{-1}u_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right ) \\ &amp;\geq c_1^A \min \{ c_1^B, 1 \} \left ( \langle \dot {B}^{-1}u_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right ).  \end{align*}
     </p><!-- l. 248 --><p class='noindent'>Therefore, we have \begin{equation}  c_1^A \min \{ c_1^B,1 \} c(u,u) \leq \langle Au,u \rangle _{H^{s-\alpha }(\Gamma )}.  \end{equation}<a id='x1-4003r19'></a>
     </p><!-- l. 253 --><p class='noindent'>In these two special cases
          </p><ol class='enumerate1'>
<li class='enumerate' id='x1-4005x1'>
          <!-- l. 255 --><p class='noindent'>when \(V^s(\Gamma ,A) \subseteq V^{s,0}(\Gamma ,B)\), the orthogonal decomposition of \(u\) degenerates to \(u=u_0\),
          </p></li>
<li class='enumerate' id='x1-4007x2'>
          <!-- l. 256 --><p class='noindent'>\(\mathrm {ker}(B) = \{ 0 \}\)</p></li></ol>
                                                                                               
                                                                                               
     <!-- l. 258 --><p class='noindent'>the bilinear form \(c(u,u)\) degenerates to \(\langle \dot {B}^{-1}u,u \rangle \).
     </p></li>
     <li class='itemize'>
     <!-- l. 260 --><p class='noindent'>Method 2: Build an approximate operator \(\tilde {B}\) for \(B\), which is elliptic on the whole domain \(H^{s-2\alpha }(\Gamma )\). Then use its inverse
     to build the bilinear form \(c(u,w) = \langle \tilde {B}^{-1}u,w \rangle _{H^{s-\alpha }(\Gamma )}\).
     </p><!-- l. 262 --><p class='noindent'>For any \(u\in H^{s-2\alpha }(\Gamma )\), it also has an orthogonal decomposition in \(V^{s-2\alpha }(\Gamma ,B) \oplus \mathrm {ker}(B)\) as \begin{equation}  u = u_0 + u_1 = u_0 + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )} v_p(x),  \end{equation}<a id='x1-4008r20'></a> where the inner product \(\langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}\) is the expansion
     coefficient for \(u_1\) and \(v_p\) is the corresponding basis element. Then we try to construct \(\tilde {B}\) which is \(H^{s-2\alpha }(\Gamma )\)-elliptic.
     \begin{align*}  \lVert u \rVert _{H^{s-2\alpha }(\Gamma )}^2 &amp;= \lVert u_0 + u_1 \rVert _{H^{s-2\alpha }(\Gamma )}^2 \\ &amp;= \left ( \lVert u_0 \rVert _{H^{s-2\alpha }(\Gamma )}^2 + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right )  \end{align*}
     </p><!-- l. 273 --><p class='noindent'>Use the \(V^{s-2\alpha}(\Gamma,B)\)-ellipticity of \(B\) \begin{align*}  &amp;\leq \left ( \frac {1}{c_1^B} \langle Bu_0, u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ) \\ &amp;\leq \max \{ \frac {1}{c_1^B},1 \} \left ( \langle Bu_0, u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ).  \end{align*}
     </p><!-- l. 281 --><p class='noindent'>It is equivalent to \begin{equation}  c_1^{\tilde {B}} \lVert u \rVert _{H^{s-2\alpha }(\Gamma )}^2 \leq \left ( \langle Bu_0, u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ),  \end{equation}<a id='x1-4009r21'></a> where \(c_1^{\tilde {B}} = 1/\max \{ \frac {1}{c_1^B},1 \} = \min \{ c_1^B,1 \}\).
     </p><!-- l. 289 --><p class='noindent'>On the other hand, we start from the boundedness of \(B\): \begin{align*}  \lVert u \rVert _{H^{s-2\alpha }(\Gamma )}^2 &amp;= \lVert u_0 + u_1 \rVert _{H^{s-2\alpha }(\Gamma )}^2 \\ &amp;= \left ( \lVert u_0 \rVert _{H^{s-2\alpha }(\Gamma )}^2 + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ) \\ &amp;\geq \left ( \frac {1}{c_2^B} \langle Bu_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_0 \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ) \\ &amp;\geq \min \{ \frac {1}{c_2^B},1 \} \left ( \langle Bu_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_0 \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ).  \end{align*}
     </p><!-- l. 300 --><p class='noindent'>So we have \begin{equation}  \left ( \langle Bu_0, u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ) \leq c_2^{\tilde {B}} \lVert u \rVert _{H^{s-2\alpha }(\Gamma )}^2,  \end{equation}<a id='x1-4010r22'></a> where \(c_2^{\tilde {B}} = \max \{ c_2^B,1 \}\).
     </p><!-- l. 309 --><p class='noindent'>Therefore, we can define the bilinear form related to \(\tilde {B}\) as \begin{equation}  \label {eq:bilinear-form-b} \begin{aligned} \langle \tilde {B}u,w \rangle _{H^{s-\alpha }(\Gamma )} &amp;= \langle Bu,w \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle v,v_p \rangle _{H^{s-2\alpha }(\Gamma )} \langle w,v_p \rangle _{H^{s-2\alpha }(\Gamma )} \\ &amp;= \langle Bu_0,w_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle v,v_p \rangle _{H^{s-2\alpha }(\Gamma )} \langle w,v_p \rangle _{H^{s-2\alpha }(\Gamma )} \quad \forall u,w\in H^{s-2\alpha }(\Gamma ). \end{aligned}  \end{equation}<a id='x1-4011r23'></a> It is both bounded and \(H^{s-2\alpha }(\Gamma )\)-elliptic.
     </p><!-- l. 323 --><p class='noindent'>The differences between the bilinear forms in Equation (<a href='#x1-4001r17'>17<!-- tex4ht:ref: eq:bilinear-form-c  --></a>) and (<a href='#x1-4011r23'>23<!-- tex4ht:ref: eq:bilinear-form-b  --></a>) are:
          </p><ol class='enumerate1'>
<li class='enumerate' id='x1-4013x1'>
          <!-- l. 325 --><p class='noindent'>In Equation (<a href='#x1-4011r23'>23<!-- tex4ht:ref: eq:bilinear-form-b  --></a>), \(u\) and \(w\) are projected to \(v_p\) via inner product, which is a true projection. In Equation
          (<a href='#x1-4001r17'>17<!-- tex4ht:ref: eq:bilinear-form-c  --></a>), \(u\) and \(w\) are in a different space from \(v_p\). They are “projected” to \(v_p\) via duality pairing, which
          can be considered as a generalization of projection.
          </p></li>
<li class='enumerate' id='x1-4015x2'>
          <!-- l. 326 --><p class='noindent'>In Equation (<a href='#x1-4011r23'>23<!-- tex4ht:ref: eq:bilinear-form-b  --></a>), the basis used for function expansion is \(\{ v_p \}_{p=1}^m\). In Equation (<a href='#x1-4001r17'>17<!-- tex4ht:ref: eq:bilinear-form-c  --></a>), the basis is \(\{ \mathcal {J}v_p \}_{p=1}^m\).</p></li></ol>
     </li></ul>
<!-- l. 330 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>4    </span> <a id='x1-50004'></a>Discretization of the preconditioning bilinear form</h3>
<!-- l. 333 --><p class='noindent'>According to (<a href='#XSteinbachConstruction1998'>Steinbach and Wendland</a>, section 4), Method 1 in Section <a href='#x1-40003'>3<!-- tex4ht:ref: sec:precond-bilinear-form  --></a> requires coordinate transformation
between the finite element basis and the orthonormal basis adopted for the orthogonal decomposition \(V_h^s(\Gamma ,A) = V_h^0 \oplus \{ \mathcal {J}v_p \}_{p=1}^m\), which is
more complicated than Method 2. Meanwhile, (<a href='#XSteinbachNumerical2007'>Steinbach</a>, section 13.2 page 299) only introduces Method 2. So
we stick to this method.
</p><!-- l. 335 --><p class='indent'>   Usually, the inverse operator \(\tilde {B}^{-1}\) cannot be directly evaluated or discretized. We need to compute the discretized
matrix of \(\tilde {B}\) first, then use its inverse matrix to compute an approximation to the discretized matrix for the inverse
operator \(\tilde {B}^{-1}\). This can be visualized in the following diagram. </p>
<div class='center'>
                                                                                               
                                                                                               
<!-- l. 336 --><p class='noindent'>
</p><!-- l. 337 --><p class='noindent'><img alt='PIC'  src='/figures/2024-11-16_11-54-40-approximation-of-inverse-operator.png'  /></p></div>
<!-- l. 340 --><p class='indent'>   Therefore, we have \begin{equation}  \underline {\tilde {B}^{-1}} \approx \mathcal {M}_{\tilde {B}}^{\mathrm {T}} \tilde {\mathcal {B}}^{-1} \mathcal {M}_{\tilde {B}}.  \end{equation}<a id='x1-5001r24'></a> The matrix to be multiplied to both sides of the equation is \begin{equation}  \mathcal {M}_{\tilde {B}}^{-1} \tilde {\mathcal {B}} \mathcal {M}_{\tilde {B}}^{-\mathrm {T}}.  \end{equation}<a id='x1-5002r25'></a>
</p><!-- l. 350 --><p class='indent'>   The spectral equivalence between the above two matrices \(\underline {\tilde {B}^{-1}}\) and \(\mathcal {M}_{\tilde {B}}^{\mathrm {T}} \tilde {\mathcal {B}}^{-1} \mathcal {M}_{\tilde {B}}\) can be proved using the theory given in
(<a href='#XSteinbachConstruction1998'>Steinbach and Wendland</a>, section 3). This theory is described in a general form as below.
</p><!-- l. 352 --><p class='indent'>   \(V\) and \(W\) are two Hilbert spaces. \(A: V \rightarrow V'\) is a \(V\)-elliptic, self-adjoint and bounded operator, while \(B: W \rightarrow V'\) is any bounded operator.
Define an operator \(T: W \rightarrow W'\) as \(T = B' A^{-1} B\). In principle, it has a Galerkin matrix \(\mathcal {T}\). In reality, this matrix cannot be directly computed.
The Galerkin matrices for \(A\) and \(B\) are \(\mathcal {A}\) and \(\mathcal {B}\) respectively. Then we have another matrix \begin{equation}  \tilde {\mathcal {T}} = \mathcal {B}^{\mathrm {T}} \mathcal {A}^{-1} \mathcal {B}.  \end{equation}<a id='x1-5003r26'></a> It can be proved that for any
\(w_h\in W_h\), \begin{equation}  ( \tilde {\mathcal {T}} \underline {w}, \underline {w} ) \leq ( \mathcal {T} \underline {w}, \underline {w} ).  \end{equation}<a id='x1-5004r27'></a> If the stability condition or \(\inf \sup \) condition is satisfied \begin{equation}  \inf _{0\neq w_h\in W_h} \sup _{0\neq v_h\in V_h} \frac {\lvert \langle Bw_h,v_h \rangle _{V} \rvert }{\lVert w_h \rVert _W \lVert v_h \rVert _V} \geq c,  \end{equation}<a id='x1-5005r28'></a> we also have \begin{equation}  \gamma ( \mathcal {T} \underline {w}, \underline {w} ) \leq ( \tilde {\mathcal {T}} \underline {w}, \underline {w} ).  \end{equation}<a id='x1-5006r29'></a> Therefore, \(\mathcal {T}\) and \(\tilde {\mathcal {T}}\) are spectrally
equivalent.
</p><!-- l. 372 --><p class='indent'>   The relationship between the spaces, operators and matrices are shown below. </p>
<div class='center'>
<!-- l. 373 --><p class='noindent'>
</p><!-- l. 374 --><p class='noindent'><img alt='PIC' src='/figures/IMG_1857.JPG' /></p></div>
                                                                                               
                                                                                               
<!-- l. 377 --><p class='indent'>   Coming back to the spectral equivalence between the two matrices \(\underline {\tilde {B}^{-1}}\) and \(\mathcal {M}_{\tilde {B}}^{\mathrm {T}} \tilde {\mathcal {B}}^{-1} \mathcal {M}_{\tilde {B}}\), we can see that it
is the special case when \(W\) is selected to be \(V'\) and \(B\) is the identity operator on \(V'\). The mass matrix \(M_A\) for
the duality pairing from \(V'\) to \(V\) is just the Galerkin matrix of \(B\). This is shown in the following figure.
</p>
<div class='center'>
<!-- l. 378 --><p class='noindent'>
</p><!-- l. 379 --><p class='noindent'><img alt='PIC' src='/figures/IMG_1858.JPG' /></p></div>
<!-- l. 382 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>5    </span> <a id='x1-60005'></a>Preconditioners for Laplace problem</h3>
<!-- l. 384 --><p class='noindent'>The operator equation for the Laplace problem with Dirichlet boundary condition is \begin{equation}  \langle Vt, \tau \rangle _{\Gamma } = \langle (\frac {1}{2}I + K)g, \tau \rangle _{\Gamma } \quad \forall \tau \in H^{-1/2}(\Gamma ).  \end{equation}<a id='x1-6001r30'></a> The hypersingular operator
\(D\) has an opposite order with respect to \(V\), which is naturally the preconditioner for this equation.
                                                                                               
                                                                                               
However, according to <a href='/math/2024/11/11/ellipticity-of-boundary-integral-operators.html#x1-50004'>here</a>, \(D\) is not \(H^{1/2}(\Gamma )\)-elliptic, but only \(H_{\ast }^{1/2}(\Gamma )\)-elliptic or \(H_{\ast \ast }^{1/2}(\Gamma )\)-elliptic depending on which kind
of inner product is desired. To avoid the complex of computing the natural density \(w_{\mathrm {eq}}\), we choose
\(H_{\ast \ast }^{1/2}(\Gamma )\)-ellipticity. Then the bilinear form associated with the regularized or gauged operator \(\tilde {D}\) is \begin{equation}  \langle \tilde {D}u,w \rangle = \langle Du,w \rangle + \langle u,1 \rangle _{\Gamma } \langle w,1 \rangle _{\Gamma } \quad \forall u,w\in H^{1/2}(\Gamma ).  \end{equation}<a id='x1-6002r31'></a> Then the
inverse of the preconditioning matrix is \begin{equation}  \mathcal {M}_{\tilde {D}}^{-1} \tilde {\mathcal {D}} \mathcal {M}_{\tilde {D}}^{-\mathrm {T}},  \end{equation}<a id='x1-6003r32'></a> where \(\mathcal {M}_{\tilde {D}}\) is the mass matrix for the duality pairing from \(H^{-1/2}(\Gamma )\) to
\(H^{1/2}(\Gamma )\).
</p><!-- l. 398 --><p class='indent'>   The operator equation for the Laplace problem with Neumann boundary condition is \begin{equation}  \langle Du, v \rangle _{\Gamma } + \alpha \langle u,1 \rangle _{\Gamma } \langle v,1 \rangle _{\Gamma } = \langle (\frac {1}{2}I - K')g,v \rangle _{\Gamma } \quad \forall v\in H^{1/2}(\Gamma ).  \end{equation}<a id='x1-6004r33'></a> The single layer
potential operator \(V\) has an opposite order with respect to \(D\), which is the preconditioner. When \(d=3\), \(V\) is \(H^{-1/2}(\Gamma )\)-elliptic, so we
directly obtain the inverse of the preconditioning matrix \begin{equation}  \mathcal {M}_V^{-1} \mathcal {V} \mathcal {M}_V^{-\mathrm {T}},  \end{equation}<a id='x1-6005r34'></a> where \(\mathcal {M}_V\) is the mass matrix for the duality pairing from \(H^{1/2}(\Gamma )\)
to \(H^{-1/2}(\Gamma )\).
</p><!-- l. 1 --><p class='noindent'>
</p>
   <h3 class='likesectionHead'><a id='x1-7000'></a>References</h3>
<!-- l. 1 --><p class='noindent'>
  </p><div class='thebibliography'>
  <p class='bibitem'><span class='biblabel'>
<a id='XMcLeanStrongly2000'></a><span class='bibsp'>   </span></span>William Charles Hector McLean. <span class='p1xi-x-x-109'>Strongly Elliptic Systems and Boundary Integral Equations</span>. Cambridge
  University Press. ISBN 978-0-521-66375-5.
  </p>
  <p class='bibitem'><span class='biblabel'>
<a id='XSteinbachNumerical2007'></a><span class='bibsp'>   </span></span>Olaf  Steinbach.    <span class='p1xi-x-x-109'>Numerical  Approximation  Methods  for  Elliptic  Boundary  Value  Problems:  Finite  and
  </span><span class='p1xi-x-x-109'>Boundary Elements</span>. Springer Science &amp; Business Media. ISBN 978-0-387-31312-2.
  </p>
  <p class='bibitem'><span class='biblabel'>
<a id='XSteinbachConstruction1998'></a><span class='bibsp'>   </span></span>Olaf               Steinbach               and               Wolfgang L.               Wendland.                                      The
  construction of some efficient preconditioners in the boundary element method.  9(1-2):191–216.  URL
  <a class='url' href='http://link.springer.com/article/10.1023/A:1018937506719'><span class='t1xtt-x-x-109'>http://link.springer.com/article/10.1023/A:1018937506719</span></a>.
</p>
  </div>



  </div><div class="sharethis-inline-share-buttons"></div>

  <script src="https://utteranc.es/client.js"
          repo="jihuan-tian/jihuan-tian.github.io"
          issue-term="pathname"
          theme="github-light"
          crossorigin="anonymous"
          async>
  </script>

  <a class="u-url" href="/math/2024/11/16/general-theory-about-the-construction-of-preconditioning-bilinear-form-in-bem.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">止于至善</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Jihuan Tian</li><li><a class="u-email" href="mailto:jihuan_tian@hotmail.com">jihuan_tian@hotmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="/feed.xml"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#rss"></use></svg> <span>RSS</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>As regards numerical analysis, mathematical electromagnetism, Linux techniques and personal thoughts.</p>
      </div>
      <div class="footer-col">
        <p>The articles are under a <a href='http://creativecommons.org/licenses/by-nc-sa/4.0/'>Creative Commons Attribution License</a>. Copyright &copy; 2025 <a href="mailto:jihuan_tian@hotmail.com">Jihuan Tian</a>.</p>
      </div>
    </div>
  </div>

</footer>
</body>

</html>
