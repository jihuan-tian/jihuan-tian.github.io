<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Minimization of a functional for solving linear problems | 止于至善</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Minimization of a functional for solving linear problems" />
<meta name="author" content="Jihuan Tian" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Contents  1 General idea  2 Gradient of the functional \(\varphi (x)\) for \(Ax=b\) (real valued)  3 Wirtinger derivatives for complex functions  4 Gradient of the functional \(\varphi (x)\) for \(Ax=b\) (complex valued)  5 Gradient of the residual norm in linear least square problems  6 Summary Abstract Minimization of a functional can be considered as a reduction or simplification method for solving a linear system with many or infinite number of degree of freedoms (DoFs). The minimum point can be found by computing the critical point of the functional, which is achieved when the gradient of the functional is zero. In the complex valued case, the evaluation of the gradient requires Wirtinger derivatives, where the complex variable \(z\) and its complex conjugation \(\overline {z}\) are independent from each other. 1 General idea According to Variational problems, a linear operator equation \(Au=f\) (as a strong form problem) is equivalent to a variational equation \(\langle Au,v \rangle = \langle f,v \rangle \) (as a weak form problem) due to the boundedness of the operator \(A\). Furthermore, the variational equation is equivalent to the minimization of a functional \(\mathcal {L}(v)=\frac {1}{2}\langle Av,v \rangle - \langle f,v \rangle \), if the linear operator \(A\) is also self-adjoint (in the sense of normed space, which is also called self-dual, see Adjoint operators in functional analysis) and elliptic. The variational equation is based on the notion of measurement by projection, while the functional minimization problem is based on the notion of minimizing energy, such as the principle of least action in physics. Even though the above theory is usually introduced in a book about PDE, such as (Steinbach), it is actually a general theory, which naturally holds for linear algebra where finite dimensional spaces are involved. For example, to solve the linear system \(Ax=b\), where \(A\in \mathbb {R}^{n\times n}\) is symmetric positive definite (SPD), \(x\in \mathbb {R}^n\) and \(b\in \mathbb {R}^n\), we can search the critical point \(x\) which minimizes a functional \(\varphi (x) = \frac {1}{2} ( Ax,x ) - ( b,x )\), where \(( \cdot ,\cdot )\) is the inner product in \(\mathbb {R}^{n}\). According to Understanding about the Lagrange multiplier method and its application in PDEs, the critical point of \(\varphi (x)\) must be a minimum point, if the operator \(A\) is positive semi-definite. Of course, if \(A\) is a SPD matrix in \(\mathbb {R}^{n\times n}\), this condition is satisfied. If we use an iterative algorithm, such as the steepest descent method, we need to construct a sequence \(\{ x^{(k)} \}_{k\geq 1}\), which minimizes the functional \(\varphi (x)\) incrementally. In each iteration step, with the previous vector \(x^{(k-1)}\), we search the next vector \(x^{(k)}\) along the negative gradient direction \(-\nabla \varphi (x) \Big \vert _{x^{(k-1)}}\), the concept of which is the same as the Newton’s method in 1D search. The step size in this search direction is an unknown factor \(\alpha ^{(k)}\), which can be obtained by solving the critical point of this functional \begin{equation} \tilde {\varphi }(\alpha ^{(k)}) = \varphi \left (x^{(k)} + \alpha ^{(k)}r^{(k)} \right ), \end{equation} where \(r^{(k)}\) is the unit vector of \(-\nabla \varphi (x)\Big \vert _{x^{(k-1)}}\). Now, let’s see what on earth the gradient of the functional is. 2 Gradient of the functional \(\varphi (x)\) for \(Ax=b\) (real valued) Theorem 1 The gradient of \(\varphi (x)\) is \(Ax-b\). Proof \begin{equation} \begin{aligned} \varphi (x) &amp;= \frac {1}{2} ( Ax,x ) - ( b,x ) = \frac {1}{2} x^{\mathrm {T}}Ax - x^{\mathrm {T}}b \\ &amp;= \frac {1}{2} \sum _{i=1}^n \sum _{j=1}^n a_{ij}x_ix_j - \sum _{i=1}^n b_ix_i. \end{aligned} \end{equation} We need to enforce the conditions \begin{equation} \frac {\diff \varphi (x)}{\diff x_k} = 0 \quad k=1,\cdots ,n. \end{equation} For the first term \(\frac {1}{2} \sum _{i=1}^n \sum _{j=1}^n a_{ij}x_ix_j\) in \(\varphi (x)\), we consider the following cases about \(k\): When \(k\neq i\) and \(k\neq j\), \begin{equation} \frac {\partial a_{ij}x_ix_j}{\partial x_{k}} = 0. \end{equation} When \(k=i\) and \(k\neq j\), \begin{equation} \frac {\partial a_{ij}x_ix_j}{\partial x_{k}} = \frac {\partial a_{kj}x_kx_j}{\partial x_k} = a_{kj}x_j. \end{equation} When \(k\neq i\) and \(k=j\), \begin{equation} \frac {\partial a_{ij}x_ix_j}{\partial x_{k}} = \frac {\partial a_{ik}x_ix_k}{\partial x_k} = a_{ik}x_i. \end{equation} When \(k=i=j\), \begin{equation} \frac {\partial a_{ij}x_ix_j}{\partial x_{k}} = \frac {\partial a_{kk}x_k^2}{\partial x_k} = 2a_{kk}x_k. \end{equation} Then the partial derivative of the first term with respect to \(x_k\) is \begin{equation} \frac {1}{2} \sum _{\substack {j=1 \\ j\neq k}}^n a_{kj}x_j + \frac {1}{2} \sum _{\substack {i=1 \\ i\neq k}}^n a_{ik}x_i + a_{kk}x_k. \end{equation} Because \(A\) is SPD, \(a_{ik} = a_{ki}\), the first and second terms in the above expression can be merged, hence the above expression becomes \begin{equation} \sum _{\substack {j=1 \\ j\neq k}}^n a_{kj}x_j + a_{kk}x_k = \sum _{j=1}^n a_{kj}x_j. \end{equation} This is just the \(k\)-th component of the vector \(Ax\). The partial derivative of the second term in \(\varphi (x)\) with respect to \(x_k\) is simply \(-b_k\). Therefore, we have \(\nabla \varphi (x) = Ax-b\). Comment 1 The gradient of \((Ax,x)\) with respect to \(x\) is \(2Ax\) and the gradient of \((b,x)\) is \(b\). This is a generalization of the derivation rule for scalar values, i.e. \(\frac {\diff (ax^2)}{\diff x} = 2ax\) and \(\frac {\diff (bx)}{\diff x} = b\). 3 Wirtinger derivatives for complex functions Before we examine the gradient of the functional \(\varphi (x)\) in the complex valued case, we need to clarify the derivatives of a function with respect to a complex variable \(x\) and its complex conjugate \(\overline {x}\), both of which will appear in the functional \(\varphi (x)\). The basic idea is that even though \(x\) is related to \(\overline {x}\) via complex conjugation, these two variables are actually independent. This determines how we compute the gradient of \(\varphi (x)\). The reason for \(x\) and \(\overline {x}\) are independent is given below. Let \(f\) be a complex valued function dependent on a complex variable \(z\). If \(f\) is differentiable at \(z_0\), the variation of the function value at \(z_0 + \Delta z\) can be written as \begin{equation} \Delta f(z_0) = f(z_0 + \Delta z) - f(z_0) = \frac {\partial f}{\partial x} \Delta x + \frac {\partial f}{\partial y} \Delta y + o(\Delta z), \end{equation} where \(o(\Delta z)/\lvert \Delta z \rvert \rightarrow 0\) when \(\Delta z \rightarrow 0\). We should bear in mind that a complex function is just a complex valued function defined on a 2D plane. Therefore, its variation around the point \(z_0\) can be represented as a combination of the variations in the \(x\) direction and \(y\) direction. If we define \(\Delta z = \Delta x + \rmi \Delta y\) and \(\Delta \overline {z} = \Delta x - \rmi \Delta y\), we can represent \((\Delta x, \Delta y)\) with \((\Delta z, \Delta \overline {z})\) as \begin{equation} \Delta x = \frac {1}{2} (\Delta z + \Delta \overline {z}), \Delta y = \frac {1}{2\rmi } ( \Delta z - \Delta \overline {z} ). \end{equation} Then \begin{equation} \Delta f(z_0) = \frac {1}{2} \left ( \frac {\partial f}{\partial x} - \rmi \frac {\partial f}{\partial y} \right ) \Delta z + \frac {1}{2} \left ( \frac {\partial f}{\partial x} + \rmi \frac {\partial f}{\partial y}\right ) \Delta \overline {z} + o(\Delta z) \end{equation} and divide it by \(\Delta z\), we have \begin{equation} \begin{aligned} \frac {\mathrm {d} f}{\mathrm {d} z} \Big \vert _{z_0} &amp;= \lim _{\substack {\Delta z \rightarrow 0 \\ \Delta z \in \mathbb {C}}} \left [ \frac {1}{2} \left ( \frac {\partial f}{\partial x} - \rmi \frac {\partial f}{\partial y} \right ) + \frac {1}{2} \left (\frac {\partial f}{\partial x} + \rmi \frac {\partial f}{\partial y} \right ) \frac {\Delta \overline {z}}{\Delta z} + \frac {o(\Delta z)}{\Delta z} \right ]. \end{aligned} \end{equation} Because \(\frac {\Delta \overline {z}}{\Delta z}\) depends on the path of \(\Delta z\) approaching \(0\), it does not have a limiting value. To make \(\frac {\diff f}{\diff z}\) meaningful, we should enforce \(\frac {\partial f}{\partial x} + \rmi \frac {\partial f}{\partial y} = 0\), which is just equivalent to the Cauchy-Riemann equations. Therefore, with the differentiability of \(f\) with respect to real variables \(x\) and \(y\) as well as the Cauchy-Riemann equations, we have \begin{equation} \frac {\diff f}{\diff z} \Big \vert _{z_0} = \frac {1}{2} \left ( \frac {\partial f}{\partial x} - \rmi \frac {\partial f}{\partial y} \right ). \end{equation} Similarly, the derivative of \(f\) with respect to \(\overline {z}\) can be written as \begin{equation} \begin{aligned} \frac {\mathrm {d} f}{\mathrm {d} \overline {z}} \Big \vert _{z_0} &amp;= \lim _{\substack {\Delta z \rightarrow 0 \\ \Delta z \in \mathbb {C}}} \left [ \frac {1}{2} \left ( \frac {\partial f}{\partial x} - \rmi \frac {\partial f}{\partial y} \right ) \frac {\Delta z}{\Delta \overline {z}} + \frac {1}{2} \left (\frac {\partial f}{\partial x} + \rmi \frac {\partial f}{\partial y} \right ) + \frac {o(\Delta z)}{\Delta z} \right ]. \end{aligned} \end{equation} Enforcing \(\frac {\partial f}{\partial x} - \rmi \frac {\partial f}{\partial y} = 0\), which is the counterpart of the Cauchy-Riemann equations when \(f\) is a function of \(\overline {z}\) instead of \(z\), we have \begin{equation} \frac {\diff f}{\diff \overline {z}} \Big \vert _{z_0} = \frac {1}{2} \left ( \frac {\partial f}{\partial x} + \rmi \frac {\partial f}{\partial y} \right ). \end{equation} The above \(\frac {\diff f}{\diff z}\) and \(\frac {\diff f}{\diff \overline {z}}\) are called Wirtinger derivatives. Comment 2 Because a complex function \(f\) can be considered as a complex valued function on the \(xy\) plane, its variation in the neighborhood about a point \(z_0\) can be decomposed into real partial derivatives along two orthogonal directions \(x\) and \(y\). On the other hand, the function variation can be represented with the complex derivative with respect to \(z\), which is along the direction angle \(\mathrm {atan}(x,y)\) or with respect to \(\overline {z}\), which is along the direction angle \(\mathrm {atan}(x,-y)\). These two directions are linearly independent. Therefore, even though the two variables \(z\) and \(\overline {z}\) are linked via complex conjugation, they are two independent variables. 4 Gradient of the functional \(\varphi (x)\) for \(Ax=b\) (complex valued) When the matrix \(A\) and vectors \(x\) and \(b\) are complex valued 1, the definition of the inner product in \(\mathbb {C}^n\) involves complex conjugation. For any \(x\) and \(y\) in \(\mathbb {C}^n\), the following conditions should be satisfied: \((x,y) = \overline {(y,x)}\); \(( \alpha x,y ) = \alpha (x,y)\); \((x,\alpha y) = \overline {\alpha } (x,y)\). Meanwhile, the SPD condition of \(A\) now becomes Hermite symmetric (\(A = A^{\mathrm {H}}\)) and positive definite. In the complex valued case, the range of the functional \(\varphi (x)\) should still be \(\mathbb {R}\), since it is related to energy. Therefore, the term \(( b,x )\) in the original \(\varphi (x)\), which is not necessarily real valued, should be changed to \(\real ( b,x )\), while \(\frac {1}{2} ( Ax,x )\) must be real valued because \(A\) is SPD. Then the functional is (Sauter and Schwab, page 354) \begin{equation} \begin{aligned} \varphi (x) &amp;= \frac {1}{2} ( Ax,x )-\real (b,x) = \frac {1}{2} x^{\mathrm {H}}Ax - \real (x^{\mathrm {H}}b) \\ &amp;= \frac {1}{2} \sum _{i=1}^n \sum _{j=1}^n a_{ij}\overline {x}_ix_j - \real \left ( \sum _{i=1}^n b_i \overline {x}_i \right ). \end{aligned} \end{equation} First, let’s see the gradient of \(\varphi (x)\) with respect to \(x\). Before proceeding, we notice that the second term above does not contain \(x\) at all. If we directly evaluate \(\nabla _x \varphi (x)\) using this expression, the result will not depend on \(b\), which is obviously incorrect. Therefore, the functional should be written as below, which is still the same as before \begin{equation} \begin{aligned} \varphi (x) &amp;= \frac {1}{2} ( Ax,x )-\real (x,b) = \frac {1}{2} x^{\mathrm {H}}Ax - \real (b^{\mathrm {H}}x) \\ &amp;= \frac {1}{2} \sum _{i=1}^n \sum _{j=1}^n a_{ij}\overline {x}_ix_j - \real \left ( \sum _{i=1}^n \overline {b}_i x_i \right ). \end{aligned} \end{equation} The complex partial derivative of the first term in the above expression with respect to \(x_k\) is \begin{equation} \frac {\partial }{\partial x_k} \left ( \frac {1}{2} \sum _{i=1}^n \sum _{j=1}^n a_{ij} \overline {x}_ix_j \right ) = \frac {1}{2} \sum _{i=1}^{n} a_{ik} \overline {x}_i. \end{equation} For the second term, only when \(i=k\), the term in the sum contributes to the complex partial derivative of \(\varphi (x)\). Let \(b_k = b_{k1} + \rmi b_{k2}\) and \(x_k = x_{k1} + \rmi x_{k2}\), \begin{equation} \real (\overline {b}_k x_k) = \real (b_{k1}x_{k1} + b_{k2}x_{k2} + \rmi (b_{k1}x_{k2} - b_{k2}x_{k1})) = b_{k1}x_{k1} + b_{k2}x_{k2}. \end{equation} Using the Wirtinger derivative, the complex partial derivative of the second term with respect to \(x_k\) is \begin{equation} \begin{aligned} \frac {\partial }{\partial x_k} \real (\overline {b}_kx_k) &amp;= \frac {\partial }{\partial x_k} (b_{k1}x_{k1} + b_{k2}x_{k2}) \\ &amp;= \frac {1}{2} \left ( \frac {\partial }{\partial x_{k1}} - \rmi \frac {\partial }{\partial x_{k2}} \right ) (b_{k1}x_{k1} + b_{k2}x_{k2}) \\ &amp;= \frac {1}{2} ( b_{k1} - \rmi b_{k2} ) = \frac {1}{2} \overline {b}_k. \end{aligned} \end{equation} Then the gradient \(\nabla _x \varphi (x)\) is \begin{equation} \nabla _x\varphi (x) = \frac {1}{2} \overline {A^{\mathrm {H}}x} - \frac {1}{2} \overline {b}. \end{equation} When \(\nabla _x\varphi (x) = 0\), we have \begin{equation} \overline {A^{\mathrm {H}}x} = \overline {b} \Leftrightarrow A^{\mathrm {H}}x = b. \end{equation} Because \(A\) is Hermite symmetric, this is further equivalent to \(Ax = b\). Second, we check the gradient of \(\varphi (x)\) with respect to \(\overline {x}\). Using the same procedure, we still obtain \(Ax=b\), when \(\nabla _{\overline {x}}\varphi (x)=0\). 5 Gradient of the residual norm in linear least square problems For a linear system \(Ax=b\), where \(A\in \mathbb {R}^{m\times n}\) with \(m\geq n\), if \(x\) minimizes the 2-norm of the residual vector \(\lVert b - Ax \rVert _2\), it is called the linear least square solution. This is equivalent to solve the normal equation \(A^{\mathrm {T}}Ax=b\), or \(A^{\mathrm {H}}Ax=b\) in the complex valued case. Then we will show this equivalence. First, we consider the real valued case. The 2-norm of the residual vector can also be considered as a functional \begin{equation} \varphi (x) = \lVert b - Ax \rVert _2 = ( Ax-b,Ax-b ) = ( Ax,Ax ) + ( b,b ) - ( Ax,b ) - ( b,Ax ). \end{equation} The gradient of the first term in this functional is \begin{equation} \nabla _x ( Ax,Ax ) = \nabla _x ( A^{\mathrm {T}}Ax,x ) = 2A^{\mathrm {T}}Ax. \end{equation} The gradient of the third term in \(\varphi (x)\) is \begin{equation} \nabla _x ( Ax,b ) = \nabla _x b^{\mathrm {T}}Ax = b^{\mathrm {T}}A. \end{equation} Write it as a column vector \begin{equation} \nabla _x ( Ax,b ) = A^{\mathrm {T}}b. \end{equation} The gradient of the fourth term in \(\varphi (x)\) is \begin{equation} \nabla _x ( b,Ax ) = \nabla _x x^{\mathrm {T}}A^{\mathrm {T}}b = A^{\mathrm {T}}b. \end{equation} Therefore, \begin{equation} \nabla _x \varphi (x) = 2A^{\mathrm {T}}Ax - 2A^{\mathrm {T}}b. \end{equation} When it is 0, we have \(A^{\mathrm {T}}Ax = A^{\mathrm {T}}b\), which is the normal equation. Second, we consider the complex valued case. The functional \(\varphi (x)\) still takes its original form, since \(( Ax,Ax )\) and \(( b,b )\) are real valued for sure, while \(( Ax,b ) + ( b,Ax )\) is also real valued. For the gradient with respect to \(x\), we have \begin{equation} \nabla _x ( Ax,Ax ) = \nabla _x x^{\mathrm {H}}A^{\mathrm {H}}Ax = x^{\mathrm {H}}A^{\mathrm {H}}A, \end{equation} \begin{equation} \nabla _x ( Ax,b ) = b^{\mathrm {H}}A \end{equation} and \begin{equation} \nabla _x ( b,Ax ) = 0. \end{equation} Hence \begin{equation} \nabla _x \varphi (x) = x^{\mathrm {H}}A^{\mathrm {H}}A - b^{\mathrm {H}}A. \end{equation} When the gradient with respect to \(x\) is 0, we obtain the normal equation \begin{equation} x^{\mathrm {H}}A^{\mathrm {H}}A - b^{\mathrm {H}}A = 0 \Leftrightarrow A^{\mathrm {H}}Ax = A^{\mathrm {H}}b. \end{equation} For the gradient with respect to \(\overline {x}\), we have \begin{equation} \nabla _{\overline {x}} ( Ax,Ax ) = \nabla _{\overline {x}} x^{\mathrm {H}}A^{\mathrm {H}}Ax = A^{\mathrm {H}}Ax, \end{equation} \begin{equation} \nabla _{\overline {x}} (Ax,b ) = 0 \end{equation} and \begin{equation} \nabla _{\overline {x}} ( b,Ax ) = \nabla _{\overline {x}} x^{\mathrm {H}}A^{\mathrm {H}}b = A^{\mathrm {H}}b. \end{equation} Hence \begin{equation} \nabla _{\overline {x}} \varphi (x) = A^{\mathrm {H}}Ax - A^{\mathrm {H}}b. \end{equation} When this gradient is 0, we also obtain the normal equation \(A^{\mathrm {H}}Ax = A^{\mathrm {H}}b\). 6 Summary &lt;ul class=&#39;itemize1&#39;&gt; &lt;li class=&#39;itemize&#39;&gt; &lt;!-- l. 280 --&gt;&lt;p class=&#39;noindent&#39;&gt;The solution of a linear system \(Ax=b\) is equivalent to finding the minimum point of a functional, if the linear operator \(A\) satisfies some properties. When \(A\) is a partial differential operator, there are infinite number of DoFs to be solved. When \(A\) is a large matrix, as in FEM or BEM which is usually the discrete version of a corresponding partial differential operator, there are still a large number of unknowns to be solved. If the said functional is found, the solution of &lt;span class=&#39;p1xb-x-x-109&#39;&gt;many &lt;/span&gt;unknowns is converted to the minimization of a &lt;span class=&#39;p1xb-x-x-109&#39;&gt;single &lt;/span&gt;objective function, which can be reckoned as a kind of simplification or reduction. &lt;/p&gt;&lt;/li&gt; &lt;li class=&#39;itemize&#39;&gt; &lt;!-- l. 281 --&gt;&lt;p class=&#39;noindent&#39;&gt;The critical point of the functional \(\varphi (x) = \frac {1}{2}( Ax,x ) - ( b,x )\) in the real valued case, or the critical point of the functional \(\varphi (x) = \frac {1}{2} (Ax,x) - \real ( b,x )\) in the complex valued case, is the solution of \(Ax=b\). &lt;/p&gt;&lt;/li&gt; &lt;li class=&#39;itemize&#39;&gt; &lt;!-- l. 282 --&gt;&lt;p class=&#39;noindent&#39;&gt;The critical point of the residual norm \(\lVert b - Ax \rVert _2\) in both real and complex valued cases is the solution of the normal equation \(A^{\mathrm {T}}Ax = A^{\mathrm {T}}b\) in the real valued case or \(A^{\mathrm {H}}Ax = A^{\mathrm {H}}b\) in the complex valued case, which is also the linear least square solution of \(Ax=b\).&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt; &lt;!-- l. 1 --&gt;&lt;p class=&#39;noindent&#39;&gt; &lt;/p&gt; References    Yousef Saad. Iterative Methods for Sparse Linear Systems. Other Titles in Applied Mathematics. Society for Industrial and Applied Mathematics. ISBN 978-0-89871-534-7. doi: 10.1137/1.9780898718003.    Stefan Sauter and Christoph Schwab. Boundary Element Methods. Springer Science &amp; Business Media. ISBN 978-3-540-68093-2.    Olaf Steinbach. Numerical Approximation Methods for Elliptic Boundary Value Problems: Finite and Boundary Elements. Springer Science &amp; Business Media. ISBN 978-0-387-31312-2. 1Complex valued problems will be met, when we solve harmonic acoutics or electromagnetic equations, i.e Helmholtz equations." />
<meta property="og:description" content="Contents  1 General idea  2 Gradient of the functional \(\varphi (x)\) for \(Ax=b\) (real valued)  3 Wirtinger derivatives for complex functions  4 Gradient of the functional \(\varphi (x)\) for \(Ax=b\) (complex valued)  5 Gradient of the residual norm in linear least square problems  6 Summary Abstract Minimization of a functional can be considered as a reduction or simplification method for solving a linear system with many or infinite number of degree of freedoms (DoFs). The minimum point can be found by computing the critical point of the functional, which is achieved when the gradient of the functional is zero. In the complex valued case, the evaluation of the gradient requires Wirtinger derivatives, where the complex variable \(z\) and its complex conjugation \(\overline {z}\) are independent from each other. 1 General idea According to Variational problems, a linear operator equation \(Au=f\) (as a strong form problem) is equivalent to a variational equation \(\langle Au,v \rangle = \langle f,v \rangle \) (as a weak form problem) due to the boundedness of the operator \(A\). Furthermore, the variational equation is equivalent to the minimization of a functional \(\mathcal {L}(v)=\frac {1}{2}\langle Av,v \rangle - \langle f,v \rangle \), if the linear operator \(A\) is also self-adjoint (in the sense of normed space, which is also called self-dual, see Adjoint operators in functional analysis) and elliptic. The variational equation is based on the notion of measurement by projection, while the functional minimization problem is based on the notion of minimizing energy, such as the principle of least action in physics. Even though the above theory is usually introduced in a book about PDE, such as (Steinbach), it is actually a general theory, which naturally holds for linear algebra where finite dimensional spaces are involved. For example, to solve the linear system \(Ax=b\), where \(A\in \mathbb {R}^{n\times n}\) is symmetric positive definite (SPD), \(x\in \mathbb {R}^n\) and \(b\in \mathbb {R}^n\), we can search the critical point \(x\) which minimizes a functional \(\varphi (x) = \frac {1}{2} ( Ax,x ) - ( b,x )\), where \(( \cdot ,\cdot )\) is the inner product in \(\mathbb {R}^{n}\). According to Understanding about the Lagrange multiplier method and its application in PDEs, the critical point of \(\varphi (x)\) must be a minimum point, if the operator \(A\) is positive semi-definite. Of course, if \(A\) is a SPD matrix in \(\mathbb {R}^{n\times n}\), this condition is satisfied. If we use an iterative algorithm, such as the steepest descent method, we need to construct a sequence \(\{ x^{(k)} \}_{k\geq 1}\), which minimizes the functional \(\varphi (x)\) incrementally. In each iteration step, with the previous vector \(x^{(k-1)}\), we search the next vector \(x^{(k)}\) along the negative gradient direction \(-\nabla \varphi (x) \Big \vert _{x^{(k-1)}}\), the concept of which is the same as the Newton’s method in 1D search. The step size in this search direction is an unknown factor \(\alpha ^{(k)}\), which can be obtained by solving the critical point of this functional \begin{equation} \tilde {\varphi }(\alpha ^{(k)}) = \varphi \left (x^{(k)} + \alpha ^{(k)}r^{(k)} \right ), \end{equation} where \(r^{(k)}\) is the unit vector of \(-\nabla \varphi (x)\Big \vert _{x^{(k-1)}}\). Now, let’s see what on earth the gradient of the functional is. 2 Gradient of the functional \(\varphi (x)\) for \(Ax=b\) (real valued) Theorem 1 The gradient of \(\varphi (x)\) is \(Ax-b\). Proof \begin{equation} \begin{aligned} \varphi (x) &amp;= \frac {1}{2} ( Ax,x ) - ( b,x ) = \frac {1}{2} x^{\mathrm {T}}Ax - x^{\mathrm {T}}b \\ &amp;= \frac {1}{2} \sum _{i=1}^n \sum _{j=1}^n a_{ij}x_ix_j - \sum _{i=1}^n b_ix_i. \end{aligned} \end{equation} We need to enforce the conditions \begin{equation} \frac {\diff \varphi (x)}{\diff x_k} = 0 \quad k=1,\cdots ,n. \end{equation} For the first term \(\frac {1}{2} \sum _{i=1}^n \sum _{j=1}^n a_{ij}x_ix_j\) in \(\varphi (x)\), we consider the following cases about \(k\): When \(k\neq i\) and \(k\neq j\), \begin{equation} \frac {\partial a_{ij}x_ix_j}{\partial x_{k}} = 0. \end{equation} When \(k=i\) and \(k\neq j\), \begin{equation} \frac {\partial a_{ij}x_ix_j}{\partial x_{k}} = \frac {\partial a_{kj}x_kx_j}{\partial x_k} = a_{kj}x_j. \end{equation} When \(k\neq i\) and \(k=j\), \begin{equation} \frac {\partial a_{ij}x_ix_j}{\partial x_{k}} = \frac {\partial a_{ik}x_ix_k}{\partial x_k} = a_{ik}x_i. \end{equation} When \(k=i=j\), \begin{equation} \frac {\partial a_{ij}x_ix_j}{\partial x_{k}} = \frac {\partial a_{kk}x_k^2}{\partial x_k} = 2a_{kk}x_k. \end{equation} Then the partial derivative of the first term with respect to \(x_k\) is \begin{equation} \frac {1}{2} \sum _{\substack {j=1 \\ j\neq k}}^n a_{kj}x_j + \frac {1}{2} \sum _{\substack {i=1 \\ i\neq k}}^n a_{ik}x_i + a_{kk}x_k. \end{equation} Because \(A\) is SPD, \(a_{ik} = a_{ki}\), the first and second terms in the above expression can be merged, hence the above expression becomes \begin{equation} \sum _{\substack {j=1 \\ j\neq k}}^n a_{kj}x_j + a_{kk}x_k = \sum _{j=1}^n a_{kj}x_j. \end{equation} This is just the \(k\)-th component of the vector \(Ax\). The partial derivative of the second term in \(\varphi (x)\) with respect to \(x_k\) is simply \(-b_k\). Therefore, we have \(\nabla \varphi (x) = Ax-b\). Comment 1 The gradient of \((Ax,x)\) with respect to \(x\) is \(2Ax\) and the gradient of \((b,x)\) is \(b\). This is a generalization of the derivation rule for scalar values, i.e. \(\frac {\diff (ax^2)}{\diff x} = 2ax\) and \(\frac {\diff (bx)}{\diff x} = b\). 3 Wirtinger derivatives for complex functions Before we examine the gradient of the functional \(\varphi (x)\) in the complex valued case, we need to clarify the derivatives of a function with respect to a complex variable \(x\) and its complex conjugate \(\overline {x}\), both of which will appear in the functional \(\varphi (x)\). The basic idea is that even though \(x\) is related to \(\overline {x}\) via complex conjugation, these two variables are actually independent. This determines how we compute the gradient of \(\varphi (x)\). The reason for \(x\) and \(\overline {x}\) are independent is given below. Let \(f\) be a complex valued function dependent on a complex variable \(z\). If \(f\) is differentiable at \(z_0\), the variation of the function value at \(z_0 + \Delta z\) can be written as \begin{equation} \Delta f(z_0) = f(z_0 + \Delta z) - f(z_0) = \frac {\partial f}{\partial x} \Delta x + \frac {\partial f}{\partial y} \Delta y + o(\Delta z), \end{equation} where \(o(\Delta z)/\lvert \Delta z \rvert \rightarrow 0\) when \(\Delta z \rightarrow 0\). We should bear in mind that a complex function is just a complex valued function defined on a 2D plane. Therefore, its variation around the point \(z_0\) can be represented as a combination of the variations in the \(x\) direction and \(y\) direction. If we define \(\Delta z = \Delta x + \rmi \Delta y\) and \(\Delta \overline {z} = \Delta x - \rmi \Delta y\), we can represent \((\Delta x, \Delta y)\) with \((\Delta z, \Delta \overline {z})\) as \begin{equation} \Delta x = \frac {1}{2} (\Delta z + \Delta \overline {z}), \Delta y = \frac {1}{2\rmi } ( \Delta z - \Delta \overline {z} ). \end{equation} Then \begin{equation} \Delta f(z_0) = \frac {1}{2} \left ( \frac {\partial f}{\partial x} - \rmi \frac {\partial f}{\partial y} \right ) \Delta z + \frac {1}{2} \left ( \frac {\partial f}{\partial x} + \rmi \frac {\partial f}{\partial y}\right ) \Delta \overline {z} + o(\Delta z) \end{equation} and divide it by \(\Delta z\), we have \begin{equation} \begin{aligned} \frac {\mathrm {d} f}{\mathrm {d} z} \Big \vert _{z_0} &amp;= \lim _{\substack {\Delta z \rightarrow 0 \\ \Delta z \in \mathbb {C}}} \left [ \frac {1}{2} \left ( \frac {\partial f}{\partial x} - \rmi \frac {\partial f}{\partial y} \right ) + \frac {1}{2} \left (\frac {\partial f}{\partial x} + \rmi \frac {\partial f}{\partial y} \right ) \frac {\Delta \overline {z}}{\Delta z} + \frac {o(\Delta z)}{\Delta z} \right ]. \end{aligned} \end{equation} Because \(\frac {\Delta \overline {z}}{\Delta z}\) depends on the path of \(\Delta z\) approaching \(0\), it does not have a limiting value. To make \(\frac {\diff f}{\diff z}\) meaningful, we should enforce \(\frac {\partial f}{\partial x} + \rmi \frac {\partial f}{\partial y} = 0\), which is just equivalent to the Cauchy-Riemann equations. Therefore, with the differentiability of \(f\) with respect to real variables \(x\) and \(y\) as well as the Cauchy-Riemann equations, we have \begin{equation} \frac {\diff f}{\diff z} \Big \vert _{z_0} = \frac {1}{2} \left ( \frac {\partial f}{\partial x} - \rmi \frac {\partial f}{\partial y} \right ). \end{equation} Similarly, the derivative of \(f\) with respect to \(\overline {z}\) can be written as \begin{equation} \begin{aligned} \frac {\mathrm {d} f}{\mathrm {d} \overline {z}} \Big \vert _{z_0} &amp;= \lim _{\substack {\Delta z \rightarrow 0 \\ \Delta z \in \mathbb {C}}} \left [ \frac {1}{2} \left ( \frac {\partial f}{\partial x} - \rmi \frac {\partial f}{\partial y} \right ) \frac {\Delta z}{\Delta \overline {z}} + \frac {1}{2} \left (\frac {\partial f}{\partial x} + \rmi \frac {\partial f}{\partial y} \right ) + \frac {o(\Delta z)}{\Delta z} \right ]. \end{aligned} \end{equation} Enforcing \(\frac {\partial f}{\partial x} - \rmi \frac {\partial f}{\partial y} = 0\), which is the counterpart of the Cauchy-Riemann equations when \(f\) is a function of \(\overline {z}\) instead of \(z\), we have \begin{equation} \frac {\diff f}{\diff \overline {z}} \Big \vert _{z_0} = \frac {1}{2} \left ( \frac {\partial f}{\partial x} + \rmi \frac {\partial f}{\partial y} \right ). \end{equation} The above \(\frac {\diff f}{\diff z}\) and \(\frac {\diff f}{\diff \overline {z}}\) are called Wirtinger derivatives. Comment 2 Because a complex function \(f\) can be considered as a complex valued function on the \(xy\) plane, its variation in the neighborhood about a point \(z_0\) can be decomposed into real partial derivatives along two orthogonal directions \(x\) and \(y\). On the other hand, the function variation can be represented with the complex derivative with respect to \(z\), which is along the direction angle \(\mathrm {atan}(x,y)\) or with respect to \(\overline {z}\), which is along the direction angle \(\mathrm {atan}(x,-y)\). These two directions are linearly independent. Therefore, even though the two variables \(z\) and \(\overline {z}\) are linked via complex conjugation, they are two independent variables. 4 Gradient of the functional \(\varphi (x)\) for \(Ax=b\) (complex valued) When the matrix \(A\) and vectors \(x\) and \(b\) are complex valued 1, the definition of the inner product in \(\mathbb {C}^n\) involves complex conjugation. For any \(x\) and \(y\) in \(\mathbb {C}^n\), the following conditions should be satisfied: \((x,y) = \overline {(y,x)}\); \(( \alpha x,y ) = \alpha (x,y)\); \((x,\alpha y) = \overline {\alpha } (x,y)\). Meanwhile, the SPD condition of \(A\) now becomes Hermite symmetric (\(A = A^{\mathrm {H}}\)) and positive definite. In the complex valued case, the range of the functional \(\varphi (x)\) should still be \(\mathbb {R}\), since it is related to energy. Therefore, the term \(( b,x )\) in the original \(\varphi (x)\), which is not necessarily real valued, should be changed to \(\real ( b,x )\), while \(\frac {1}{2} ( Ax,x )\) must be real valued because \(A\) is SPD. Then the functional is (Sauter and Schwab, page 354) \begin{equation} \begin{aligned} \varphi (x) &amp;= \frac {1}{2} ( Ax,x )-\real (b,x) = \frac {1}{2} x^{\mathrm {H}}Ax - \real (x^{\mathrm {H}}b) \\ &amp;= \frac {1}{2} \sum _{i=1}^n \sum _{j=1}^n a_{ij}\overline {x}_ix_j - \real \left ( \sum _{i=1}^n b_i \overline {x}_i \right ). \end{aligned} \end{equation} First, let’s see the gradient of \(\varphi (x)\) with respect to \(x\). Before proceeding, we notice that the second term above does not contain \(x\) at all. If we directly evaluate \(\nabla _x \varphi (x)\) using this expression, the result will not depend on \(b\), which is obviously incorrect. Therefore, the functional should be written as below, which is still the same as before \begin{equation} \begin{aligned} \varphi (x) &amp;= \frac {1}{2} ( Ax,x )-\real (x,b) = \frac {1}{2} x^{\mathrm {H}}Ax - \real (b^{\mathrm {H}}x) \\ &amp;= \frac {1}{2} \sum _{i=1}^n \sum _{j=1}^n a_{ij}\overline {x}_ix_j - \real \left ( \sum _{i=1}^n \overline {b}_i x_i \right ). \end{aligned} \end{equation} The complex partial derivative of the first term in the above expression with respect to \(x_k\) is \begin{equation} \frac {\partial }{\partial x_k} \left ( \frac {1}{2} \sum _{i=1}^n \sum _{j=1}^n a_{ij} \overline {x}_ix_j \right ) = \frac {1}{2} \sum _{i=1}^{n} a_{ik} \overline {x}_i. \end{equation} For the second term, only when \(i=k\), the term in the sum contributes to the complex partial derivative of \(\varphi (x)\). Let \(b_k = b_{k1} + \rmi b_{k2}\) and \(x_k = x_{k1} + \rmi x_{k2}\), \begin{equation} \real (\overline {b}_k x_k) = \real (b_{k1}x_{k1} + b_{k2}x_{k2} + \rmi (b_{k1}x_{k2} - b_{k2}x_{k1})) = b_{k1}x_{k1} + b_{k2}x_{k2}. \end{equation} Using the Wirtinger derivative, the complex partial derivative of the second term with respect to \(x_k\) is \begin{equation} \begin{aligned} \frac {\partial }{\partial x_k} \real (\overline {b}_kx_k) &amp;= \frac {\partial }{\partial x_k} (b_{k1}x_{k1} + b_{k2}x_{k2}) \\ &amp;= \frac {1}{2} \left ( \frac {\partial }{\partial x_{k1}} - \rmi \frac {\partial }{\partial x_{k2}} \right ) (b_{k1}x_{k1} + b_{k2}x_{k2}) \\ &amp;= \frac {1}{2} ( b_{k1} - \rmi b_{k2} ) = \frac {1}{2} \overline {b}_k. \end{aligned} \end{equation} Then the gradient \(\nabla _x \varphi (x)\) is \begin{equation} \nabla _x\varphi (x) = \frac {1}{2} \overline {A^{\mathrm {H}}x} - \frac {1}{2} \overline {b}. \end{equation} When \(\nabla _x\varphi (x) = 0\), we have \begin{equation} \overline {A^{\mathrm {H}}x} = \overline {b} \Leftrightarrow A^{\mathrm {H}}x = b. \end{equation} Because \(A\) is Hermite symmetric, this is further equivalent to \(Ax = b\). Second, we check the gradient of \(\varphi (x)\) with respect to \(\overline {x}\). Using the same procedure, we still obtain \(Ax=b\), when \(\nabla _{\overline {x}}\varphi (x)=0\). 5 Gradient of the residual norm in linear least square problems For a linear system \(Ax=b\), where \(A\in \mathbb {R}^{m\times n}\) with \(m\geq n\), if \(x\) minimizes the 2-norm of the residual vector \(\lVert b - Ax \rVert _2\), it is called the linear least square solution. This is equivalent to solve the normal equation \(A^{\mathrm {T}}Ax=b\), or \(A^{\mathrm {H}}Ax=b\) in the complex valued case. Then we will show this equivalence. First, we consider the real valued case. The 2-norm of the residual vector can also be considered as a functional \begin{equation} \varphi (x) = \lVert b - Ax \rVert _2 = ( Ax-b,Ax-b ) = ( Ax,Ax ) + ( b,b ) - ( Ax,b ) - ( b,Ax ). \end{equation} The gradient of the first term in this functional is \begin{equation} \nabla _x ( Ax,Ax ) = \nabla _x ( A^{\mathrm {T}}Ax,x ) = 2A^{\mathrm {T}}Ax. \end{equation} The gradient of the third term in \(\varphi (x)\) is \begin{equation} \nabla _x ( Ax,b ) = \nabla _x b^{\mathrm {T}}Ax = b^{\mathrm {T}}A. \end{equation} Write it as a column vector \begin{equation} \nabla _x ( Ax,b ) = A^{\mathrm {T}}b. \end{equation} The gradient of the fourth term in \(\varphi (x)\) is \begin{equation} \nabla _x ( b,Ax ) = \nabla _x x^{\mathrm {T}}A^{\mathrm {T}}b = A^{\mathrm {T}}b. \end{equation} Therefore, \begin{equation} \nabla _x \varphi (x) = 2A^{\mathrm {T}}Ax - 2A^{\mathrm {T}}b. \end{equation} When it is 0, we have \(A^{\mathrm {T}}Ax = A^{\mathrm {T}}b\), which is the normal equation. Second, we consider the complex valued case. The functional \(\varphi (x)\) still takes its original form, since \(( Ax,Ax )\) and \(( b,b )\) are real valued for sure, while \(( Ax,b ) + ( b,Ax )\) is also real valued. For the gradient with respect to \(x\), we have \begin{equation} \nabla _x ( Ax,Ax ) = \nabla _x x^{\mathrm {H}}A^{\mathrm {H}}Ax = x^{\mathrm {H}}A^{\mathrm {H}}A, \end{equation} \begin{equation} \nabla _x ( Ax,b ) = b^{\mathrm {H}}A \end{equation} and \begin{equation} \nabla _x ( b,Ax ) = 0. \end{equation} Hence \begin{equation} \nabla _x \varphi (x) = x^{\mathrm {H}}A^{\mathrm {H}}A - b^{\mathrm {H}}A. \end{equation} When the gradient with respect to \(x\) is 0, we obtain the normal equation \begin{equation} x^{\mathrm {H}}A^{\mathrm {H}}A - b^{\mathrm {H}}A = 0 \Leftrightarrow A^{\mathrm {H}}Ax = A^{\mathrm {H}}b. \end{equation} For the gradient with respect to \(\overline {x}\), we have \begin{equation} \nabla _{\overline {x}} ( Ax,Ax ) = \nabla _{\overline {x}} x^{\mathrm {H}}A^{\mathrm {H}}Ax = A^{\mathrm {H}}Ax, \end{equation} \begin{equation} \nabla _{\overline {x}} (Ax,b ) = 0 \end{equation} and \begin{equation} \nabla _{\overline {x}} ( b,Ax ) = \nabla _{\overline {x}} x^{\mathrm {H}}A^{\mathrm {H}}b = A^{\mathrm {H}}b. \end{equation} Hence \begin{equation} \nabla _{\overline {x}} \varphi (x) = A^{\mathrm {H}}Ax - A^{\mathrm {H}}b. \end{equation} When this gradient is 0, we also obtain the normal equation \(A^{\mathrm {H}}Ax = A^{\mathrm {H}}b\). 6 Summary &lt;ul class=&#39;itemize1&#39;&gt; &lt;li class=&#39;itemize&#39;&gt; &lt;!-- l. 280 --&gt;&lt;p class=&#39;noindent&#39;&gt;The solution of a linear system \(Ax=b\) is equivalent to finding the minimum point of a functional, if the linear operator \(A\) satisfies some properties. When \(A\) is a partial differential operator, there are infinite number of DoFs to be solved. When \(A\) is a large matrix, as in FEM or BEM which is usually the discrete version of a corresponding partial differential operator, there are still a large number of unknowns to be solved. If the said functional is found, the solution of &lt;span class=&#39;p1xb-x-x-109&#39;&gt;many &lt;/span&gt;unknowns is converted to the minimization of a &lt;span class=&#39;p1xb-x-x-109&#39;&gt;single &lt;/span&gt;objective function, which can be reckoned as a kind of simplification or reduction. &lt;/p&gt;&lt;/li&gt; &lt;li class=&#39;itemize&#39;&gt; &lt;!-- l. 281 --&gt;&lt;p class=&#39;noindent&#39;&gt;The critical point of the functional \(\varphi (x) = \frac {1}{2}( Ax,x ) - ( b,x )\) in the real valued case, or the critical point of the functional \(\varphi (x) = \frac {1}{2} (Ax,x) - \real ( b,x )\) in the complex valued case, is the solution of \(Ax=b\). &lt;/p&gt;&lt;/li&gt; &lt;li class=&#39;itemize&#39;&gt; &lt;!-- l. 282 --&gt;&lt;p class=&#39;noindent&#39;&gt;The critical point of the residual norm \(\lVert b - Ax \rVert _2\) in both real and complex valued cases is the solution of the normal equation \(A^{\mathrm {T}}Ax = A^{\mathrm {T}}b\) in the real valued case or \(A^{\mathrm {H}}Ax = A^{\mathrm {H}}b\) in the complex valued case, which is also the linear least square solution of \(Ax=b\).&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt; &lt;!-- l. 1 --&gt;&lt;p class=&#39;noindent&#39;&gt; &lt;/p&gt; References    Yousef Saad. Iterative Methods for Sparse Linear Systems. Other Titles in Applied Mathematics. Society for Industrial and Applied Mathematics. ISBN 978-0-89871-534-7. doi: 10.1137/1.9780898718003.    Stefan Sauter and Christoph Schwab. Boundary Element Methods. Springer Science &amp; Business Media. ISBN 978-3-540-68093-2.    Olaf Steinbach. Numerical Approximation Methods for Elliptic Boundary Value Problems: Finite and Boundary Elements. Springer Science &amp; Business Media. ISBN 978-0-387-31312-2. 1Complex valued problems will be met, when we solve harmonic acoutics or electromagnetic equations, i.e Helmholtz equations." />
<link rel="canonical" href="https://jihuan-tian.github.io/math/2025/06/07/minimization-of-a-functional-for-solving-linear-problems.html" />
<meta property="og:url" content="https://jihuan-tian.github.io/math/2025/06/07/minimization-of-a-functional-for-solving-linear-problems.html" />
<meta property="og:site_name" content="止于至善" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-06-07T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Minimization of a functional for solving linear problems" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Jihuan Tian"},"dateModified":"2025-06-07T00:00:00+08:00","datePublished":"2025-06-07T00:00:00+08:00","description":"Contents  1 General idea  2 Gradient of the functional \\(\\varphi (x)\\) for \\(Ax=b\\) (real valued)  3 Wirtinger derivatives for complex functions  4 Gradient of the functional \\(\\varphi (x)\\) for \\(Ax=b\\) (complex valued)  5 Gradient of the residual norm in linear least square problems  6 Summary Abstract Minimization of a functional can be considered as a reduction or simplification method for solving a linear system with many or infinite number of degree of freedoms (DoFs). The minimum point can be found by computing the critical point of the functional, which is achieved when the gradient of the functional is zero. In the complex valued case, the evaluation of the gradient requires Wirtinger derivatives, where the complex variable \\(z\\) and its complex conjugation \\(\\overline {z}\\) are independent from each other. 1 General idea According to Variational problems, a linear operator equation \\(Au=f\\) (as a strong form problem) is equivalent to a variational equation \\(\\langle Au,v \\rangle = \\langle f,v \\rangle \\) (as a weak form problem) due to the boundedness of the operator \\(A\\). Furthermore, the variational equation is equivalent to the minimization of a functional \\(\\mathcal {L}(v)=\\frac {1}{2}\\langle Av,v \\rangle - \\langle f,v \\rangle \\), if the linear operator \\(A\\) is also self-adjoint (in the sense of normed space, which is also called self-dual, see Adjoint operators in functional analysis) and elliptic. The variational equation is based on the notion of measurement by projection, while the functional minimization problem is based on the notion of minimizing energy, such as the principle of least action in physics. Even though the above theory is usually introduced in a book about PDE, such as (Steinbach), it is actually a general theory, which naturally holds for linear algebra where finite dimensional spaces are involved. For example, to solve the linear system \\(Ax=b\\), where \\(A\\in \\mathbb {R}^{n\\times n}\\) is symmetric positive definite (SPD), \\(x\\in \\mathbb {R}^n\\) and \\(b\\in \\mathbb {R}^n\\), we can search the critical point \\(x\\) which minimizes a functional \\(\\varphi (x) = \\frac {1}{2} ( Ax,x ) - ( b,x )\\), where \\(( \\cdot ,\\cdot )\\) is the inner product in \\(\\mathbb {R}^{n}\\). According to Understanding about the Lagrange multiplier method and its application in PDEs, the critical point of \\(\\varphi (x)\\) must be a minimum point, if the operator \\(A\\) is positive semi-definite. Of course, if \\(A\\) is a SPD matrix in \\(\\mathbb {R}^{n\\times n}\\), this condition is satisfied. If we use an iterative algorithm, such as the steepest descent method, we need to construct a sequence \\(\\{ x^{(k)} \\}_{k\\geq 1}\\), which minimizes the functional \\(\\varphi (x)\\) incrementally. In each iteration step, with the previous vector \\(x^{(k-1)}\\), we search the next vector \\(x^{(k)}\\) along the negative gradient direction \\(-\\nabla \\varphi (x) \\Big \\vert _{x^{(k-1)}}\\), the concept of which is the same as the Newton’s method in 1D search. The step size in this search direction is an unknown factor \\(\\alpha ^{(k)}\\), which can be obtained by solving the critical point of this functional \\begin{equation} \\tilde {\\varphi }(\\alpha ^{(k)}) = \\varphi \\left (x^{(k)} + \\alpha ^{(k)}r^{(k)} \\right ), \\end{equation} where \\(r^{(k)}\\) is the unit vector of \\(-\\nabla \\varphi (x)\\Big \\vert _{x^{(k-1)}}\\). Now, let’s see what on earth the gradient of the functional is. 2 Gradient of the functional \\(\\varphi (x)\\) for \\(Ax=b\\) (real valued) Theorem 1 The gradient of \\(\\varphi (x)\\) is \\(Ax-b\\). Proof \\begin{equation} \\begin{aligned} \\varphi (x) &amp;= \\frac {1}{2} ( Ax,x ) - ( b,x ) = \\frac {1}{2} x^{\\mathrm {T}}Ax - x^{\\mathrm {T}}b \\\\ &amp;= \\frac {1}{2} \\sum _{i=1}^n \\sum _{j=1}^n a_{ij}x_ix_j - \\sum _{i=1}^n b_ix_i. \\end{aligned} \\end{equation} We need to enforce the conditions \\begin{equation} \\frac {\\diff \\varphi (x)}{\\diff x_k} = 0 \\quad k=1,\\cdots ,n. \\end{equation} For the first term \\(\\frac {1}{2} \\sum _{i=1}^n \\sum _{j=1}^n a_{ij}x_ix_j\\) in \\(\\varphi (x)\\), we consider the following cases about \\(k\\): When \\(k\\neq i\\) and \\(k\\neq j\\), \\begin{equation} \\frac {\\partial a_{ij}x_ix_j}{\\partial x_{k}} = 0. \\end{equation} When \\(k=i\\) and \\(k\\neq j\\), \\begin{equation} \\frac {\\partial a_{ij}x_ix_j}{\\partial x_{k}} = \\frac {\\partial a_{kj}x_kx_j}{\\partial x_k} = a_{kj}x_j. \\end{equation} When \\(k\\neq i\\) and \\(k=j\\), \\begin{equation} \\frac {\\partial a_{ij}x_ix_j}{\\partial x_{k}} = \\frac {\\partial a_{ik}x_ix_k}{\\partial x_k} = a_{ik}x_i. \\end{equation} When \\(k=i=j\\), \\begin{equation} \\frac {\\partial a_{ij}x_ix_j}{\\partial x_{k}} = \\frac {\\partial a_{kk}x_k^2}{\\partial x_k} = 2a_{kk}x_k. \\end{equation} Then the partial derivative of the first term with respect to \\(x_k\\) is \\begin{equation} \\frac {1}{2} \\sum _{\\substack {j=1 \\\\ j\\neq k}}^n a_{kj}x_j + \\frac {1}{2} \\sum _{\\substack {i=1 \\\\ i\\neq k}}^n a_{ik}x_i + a_{kk}x_k. \\end{equation} Because \\(A\\) is SPD, \\(a_{ik} = a_{ki}\\), the first and second terms in the above expression can be merged, hence the above expression becomes \\begin{equation} \\sum _{\\substack {j=1 \\\\ j\\neq k}}^n a_{kj}x_j + a_{kk}x_k = \\sum _{j=1}^n a_{kj}x_j. \\end{equation} This is just the \\(k\\)-th component of the vector \\(Ax\\). The partial derivative of the second term in \\(\\varphi (x)\\) with respect to \\(x_k\\) is simply \\(-b_k\\). Therefore, we have \\(\\nabla \\varphi (x) = Ax-b\\). Comment 1 The gradient of \\((Ax,x)\\) with respect to \\(x\\) is \\(2Ax\\) and the gradient of \\((b,x)\\) is \\(b\\). This is a generalization of the derivation rule for scalar values, i.e. \\(\\frac {\\diff (ax^2)}{\\diff x} = 2ax\\) and \\(\\frac {\\diff (bx)}{\\diff x} = b\\). 3 Wirtinger derivatives for complex functions Before we examine the gradient of the functional \\(\\varphi (x)\\) in the complex valued case, we need to clarify the derivatives of a function with respect to a complex variable \\(x\\) and its complex conjugate \\(\\overline {x}\\), both of which will appear in the functional \\(\\varphi (x)\\). The basic idea is that even though \\(x\\) is related to \\(\\overline {x}\\) via complex conjugation, these two variables are actually independent. This determines how we compute the gradient of \\(\\varphi (x)\\). The reason for \\(x\\) and \\(\\overline {x}\\) are independent is given below. Let \\(f\\) be a complex valued function dependent on a complex variable \\(z\\). If \\(f\\) is differentiable at \\(z_0\\), the variation of the function value at \\(z_0 + \\Delta z\\) can be written as \\begin{equation} \\Delta f(z_0) = f(z_0 + \\Delta z) - f(z_0) = \\frac {\\partial f}{\\partial x} \\Delta x + \\frac {\\partial f}{\\partial y} \\Delta y + o(\\Delta z), \\end{equation} where \\(o(\\Delta z)/\\lvert \\Delta z \\rvert \\rightarrow 0\\) when \\(\\Delta z \\rightarrow 0\\). We should bear in mind that a complex function is just a complex valued function defined on a 2D plane. Therefore, its variation around the point \\(z_0\\) can be represented as a combination of the variations in the \\(x\\) direction and \\(y\\) direction. If we define \\(\\Delta z = \\Delta x + \\rmi \\Delta y\\) and \\(\\Delta \\overline {z} = \\Delta x - \\rmi \\Delta y\\), we can represent \\((\\Delta x, \\Delta y)\\) with \\((\\Delta z, \\Delta \\overline {z})\\) as \\begin{equation} \\Delta x = \\frac {1}{2} (\\Delta z + \\Delta \\overline {z}), \\Delta y = \\frac {1}{2\\rmi } ( \\Delta z - \\Delta \\overline {z} ). \\end{equation} Then \\begin{equation} \\Delta f(z_0) = \\frac {1}{2} \\left ( \\frac {\\partial f}{\\partial x} - \\rmi \\frac {\\partial f}{\\partial y} \\right ) \\Delta z + \\frac {1}{2} \\left ( \\frac {\\partial f}{\\partial x} + \\rmi \\frac {\\partial f}{\\partial y}\\right ) \\Delta \\overline {z} + o(\\Delta z) \\end{equation} and divide it by \\(\\Delta z\\), we have \\begin{equation} \\begin{aligned} \\frac {\\mathrm {d} f}{\\mathrm {d} z} \\Big \\vert _{z_0} &amp;= \\lim _{\\substack {\\Delta z \\rightarrow 0 \\\\ \\Delta z \\in \\mathbb {C}}} \\left [ \\frac {1}{2} \\left ( \\frac {\\partial f}{\\partial x} - \\rmi \\frac {\\partial f}{\\partial y} \\right ) + \\frac {1}{2} \\left (\\frac {\\partial f}{\\partial x} + \\rmi \\frac {\\partial f}{\\partial y} \\right ) \\frac {\\Delta \\overline {z}}{\\Delta z} + \\frac {o(\\Delta z)}{\\Delta z} \\right ]. \\end{aligned} \\end{equation} Because \\(\\frac {\\Delta \\overline {z}}{\\Delta z}\\) depends on the path of \\(\\Delta z\\) approaching \\(0\\), it does not have a limiting value. To make \\(\\frac {\\diff f}{\\diff z}\\) meaningful, we should enforce \\(\\frac {\\partial f}{\\partial x} + \\rmi \\frac {\\partial f}{\\partial y} = 0\\), which is just equivalent to the Cauchy-Riemann equations. Therefore, with the differentiability of \\(f\\) with respect to real variables \\(x\\) and \\(y\\) as well as the Cauchy-Riemann equations, we have \\begin{equation} \\frac {\\diff f}{\\diff z} \\Big \\vert _{z_0} = \\frac {1}{2} \\left ( \\frac {\\partial f}{\\partial x} - \\rmi \\frac {\\partial f}{\\partial y} \\right ). \\end{equation} Similarly, the derivative of \\(f\\) with respect to \\(\\overline {z}\\) can be written as \\begin{equation} \\begin{aligned} \\frac {\\mathrm {d} f}{\\mathrm {d} \\overline {z}} \\Big \\vert _{z_0} &amp;= \\lim _{\\substack {\\Delta z \\rightarrow 0 \\\\ \\Delta z \\in \\mathbb {C}}} \\left [ \\frac {1}{2} \\left ( \\frac {\\partial f}{\\partial x} - \\rmi \\frac {\\partial f}{\\partial y} \\right ) \\frac {\\Delta z}{\\Delta \\overline {z}} + \\frac {1}{2} \\left (\\frac {\\partial f}{\\partial x} + \\rmi \\frac {\\partial f}{\\partial y} \\right ) + \\frac {o(\\Delta z)}{\\Delta z} \\right ]. \\end{aligned} \\end{equation} Enforcing \\(\\frac {\\partial f}{\\partial x} - \\rmi \\frac {\\partial f}{\\partial y} = 0\\), which is the counterpart of the Cauchy-Riemann equations when \\(f\\) is a function of \\(\\overline {z}\\) instead of \\(z\\), we have \\begin{equation} \\frac {\\diff f}{\\diff \\overline {z}} \\Big \\vert _{z_0} = \\frac {1}{2} \\left ( \\frac {\\partial f}{\\partial x} + \\rmi \\frac {\\partial f}{\\partial y} \\right ). \\end{equation} The above \\(\\frac {\\diff f}{\\diff z}\\) and \\(\\frac {\\diff f}{\\diff \\overline {z}}\\) are called Wirtinger derivatives. Comment 2 Because a complex function \\(f\\) can be considered as a complex valued function on the \\(xy\\) plane, its variation in the neighborhood about a point \\(z_0\\) can be decomposed into real partial derivatives along two orthogonal directions \\(x\\) and \\(y\\). On the other hand, the function variation can be represented with the complex derivative with respect to \\(z\\), which is along the direction angle \\(\\mathrm {atan}(x,y)\\) or with respect to \\(\\overline {z}\\), which is along the direction angle \\(\\mathrm {atan}(x,-y)\\). These two directions are linearly independent. Therefore, even though the two variables \\(z\\) and \\(\\overline {z}\\) are linked via complex conjugation, they are two independent variables. 4 Gradient of the functional \\(\\varphi (x)\\) for \\(Ax=b\\) (complex valued) When the matrix \\(A\\) and vectors \\(x\\) and \\(b\\) are complex valued 1, the definition of the inner product in \\(\\mathbb {C}^n\\) involves complex conjugation. For any \\(x\\) and \\(y\\) in \\(\\mathbb {C}^n\\), the following conditions should be satisfied: \\((x,y) = \\overline {(y,x)}\\); \\(( \\alpha x,y ) = \\alpha (x,y)\\); \\((x,\\alpha y) = \\overline {\\alpha } (x,y)\\). Meanwhile, the SPD condition of \\(A\\) now becomes Hermite symmetric (\\(A = A^{\\mathrm {H}}\\)) and positive definite. In the complex valued case, the range of the functional \\(\\varphi (x)\\) should still be \\(\\mathbb {R}\\), since it is related to energy. Therefore, the term \\(( b,x )\\) in the original \\(\\varphi (x)\\), which is not necessarily real valued, should be changed to \\(\\real ( b,x )\\), while \\(\\frac {1}{2} ( Ax,x )\\) must be real valued because \\(A\\) is SPD. Then the functional is (Sauter and Schwab, page 354) \\begin{equation} \\begin{aligned} \\varphi (x) &amp;= \\frac {1}{2} ( Ax,x )-\\real (b,x) = \\frac {1}{2} x^{\\mathrm {H}}Ax - \\real (x^{\\mathrm {H}}b) \\\\ &amp;= \\frac {1}{2} \\sum _{i=1}^n \\sum _{j=1}^n a_{ij}\\overline {x}_ix_j - \\real \\left ( \\sum _{i=1}^n b_i \\overline {x}_i \\right ). \\end{aligned} \\end{equation} First, let’s see the gradient of \\(\\varphi (x)\\) with respect to \\(x\\). Before proceeding, we notice that the second term above does not contain \\(x\\) at all. If we directly evaluate \\(\\nabla _x \\varphi (x)\\) using this expression, the result will not depend on \\(b\\), which is obviously incorrect. Therefore, the functional should be written as below, which is still the same as before \\begin{equation} \\begin{aligned} \\varphi (x) &amp;= \\frac {1}{2} ( Ax,x )-\\real (x,b) = \\frac {1}{2} x^{\\mathrm {H}}Ax - \\real (b^{\\mathrm {H}}x) \\\\ &amp;= \\frac {1}{2} \\sum _{i=1}^n \\sum _{j=1}^n a_{ij}\\overline {x}_ix_j - \\real \\left ( \\sum _{i=1}^n \\overline {b}_i x_i \\right ). \\end{aligned} \\end{equation} The complex partial derivative of the first term in the above expression with respect to \\(x_k\\) is \\begin{equation} \\frac {\\partial }{\\partial x_k} \\left ( \\frac {1}{2} \\sum _{i=1}^n \\sum _{j=1}^n a_{ij} \\overline {x}_ix_j \\right ) = \\frac {1}{2} \\sum _{i=1}^{n} a_{ik} \\overline {x}_i. \\end{equation} For the second term, only when \\(i=k\\), the term in the sum contributes to the complex partial derivative of \\(\\varphi (x)\\). Let \\(b_k = b_{k1} + \\rmi b_{k2}\\) and \\(x_k = x_{k1} + \\rmi x_{k2}\\), \\begin{equation} \\real (\\overline {b}_k x_k) = \\real (b_{k1}x_{k1} + b_{k2}x_{k2} + \\rmi (b_{k1}x_{k2} - b_{k2}x_{k1})) = b_{k1}x_{k1} + b_{k2}x_{k2}. \\end{equation} Using the Wirtinger derivative, the complex partial derivative of the second term with respect to \\(x_k\\) is \\begin{equation} \\begin{aligned} \\frac {\\partial }{\\partial x_k} \\real (\\overline {b}_kx_k) &amp;= \\frac {\\partial }{\\partial x_k} (b_{k1}x_{k1} + b_{k2}x_{k2}) \\\\ &amp;= \\frac {1}{2} \\left ( \\frac {\\partial }{\\partial x_{k1}} - \\rmi \\frac {\\partial }{\\partial x_{k2}} \\right ) (b_{k1}x_{k1} + b_{k2}x_{k2}) \\\\ &amp;= \\frac {1}{2} ( b_{k1} - \\rmi b_{k2} ) = \\frac {1}{2} \\overline {b}_k. \\end{aligned} \\end{equation} Then the gradient \\(\\nabla _x \\varphi (x)\\) is \\begin{equation} \\nabla _x\\varphi (x) = \\frac {1}{2} \\overline {A^{\\mathrm {H}}x} - \\frac {1}{2} \\overline {b}. \\end{equation} When \\(\\nabla _x\\varphi (x) = 0\\), we have \\begin{equation} \\overline {A^{\\mathrm {H}}x} = \\overline {b} \\Leftrightarrow A^{\\mathrm {H}}x = b. \\end{equation} Because \\(A\\) is Hermite symmetric, this is further equivalent to \\(Ax = b\\). Second, we check the gradient of \\(\\varphi (x)\\) with respect to \\(\\overline {x}\\). Using the same procedure, we still obtain \\(Ax=b\\), when \\(\\nabla _{\\overline {x}}\\varphi (x)=0\\). 5 Gradient of the residual norm in linear least square problems For a linear system \\(Ax=b\\), where \\(A\\in \\mathbb {R}^{m\\times n}\\) with \\(m\\geq n\\), if \\(x\\) minimizes the 2-norm of the residual vector \\(\\lVert b - Ax \\rVert _2\\), it is called the linear least square solution. This is equivalent to solve the normal equation \\(A^{\\mathrm {T}}Ax=b\\), or \\(A^{\\mathrm {H}}Ax=b\\) in the complex valued case. Then we will show this equivalence. First, we consider the real valued case. The 2-norm of the residual vector can also be considered as a functional \\begin{equation} \\varphi (x) = \\lVert b - Ax \\rVert _2 = ( Ax-b,Ax-b ) = ( Ax,Ax ) + ( b,b ) - ( Ax,b ) - ( b,Ax ). \\end{equation} The gradient of the first term in this functional is \\begin{equation} \\nabla _x ( Ax,Ax ) = \\nabla _x ( A^{\\mathrm {T}}Ax,x ) = 2A^{\\mathrm {T}}Ax. \\end{equation} The gradient of the third term in \\(\\varphi (x)\\) is \\begin{equation} \\nabla _x ( Ax,b ) = \\nabla _x b^{\\mathrm {T}}Ax = b^{\\mathrm {T}}A. \\end{equation} Write it as a column vector \\begin{equation} \\nabla _x ( Ax,b ) = A^{\\mathrm {T}}b. \\end{equation} The gradient of the fourth term in \\(\\varphi (x)\\) is \\begin{equation} \\nabla _x ( b,Ax ) = \\nabla _x x^{\\mathrm {T}}A^{\\mathrm {T}}b = A^{\\mathrm {T}}b. \\end{equation} Therefore, \\begin{equation} \\nabla _x \\varphi (x) = 2A^{\\mathrm {T}}Ax - 2A^{\\mathrm {T}}b. \\end{equation} When it is 0, we have \\(A^{\\mathrm {T}}Ax = A^{\\mathrm {T}}b\\), which is the normal equation. Second, we consider the complex valued case. The functional \\(\\varphi (x)\\) still takes its original form, since \\(( Ax,Ax )\\) and \\(( b,b )\\) are real valued for sure, while \\(( Ax,b ) + ( b,Ax )\\) is also real valued. For the gradient with respect to \\(x\\), we have \\begin{equation} \\nabla _x ( Ax,Ax ) = \\nabla _x x^{\\mathrm {H}}A^{\\mathrm {H}}Ax = x^{\\mathrm {H}}A^{\\mathrm {H}}A, \\end{equation} \\begin{equation} \\nabla _x ( Ax,b ) = b^{\\mathrm {H}}A \\end{equation} and \\begin{equation} \\nabla _x ( b,Ax ) = 0. \\end{equation} Hence \\begin{equation} \\nabla _x \\varphi (x) = x^{\\mathrm {H}}A^{\\mathrm {H}}A - b^{\\mathrm {H}}A. \\end{equation} When the gradient with respect to \\(x\\) is 0, we obtain the normal equation \\begin{equation} x^{\\mathrm {H}}A^{\\mathrm {H}}A - b^{\\mathrm {H}}A = 0 \\Leftrightarrow A^{\\mathrm {H}}Ax = A^{\\mathrm {H}}b. \\end{equation} For the gradient with respect to \\(\\overline {x}\\), we have \\begin{equation} \\nabla _{\\overline {x}} ( Ax,Ax ) = \\nabla _{\\overline {x}} x^{\\mathrm {H}}A^{\\mathrm {H}}Ax = A^{\\mathrm {H}}Ax, \\end{equation} \\begin{equation} \\nabla _{\\overline {x}} (Ax,b ) = 0 \\end{equation} and \\begin{equation} \\nabla _{\\overline {x}} ( b,Ax ) = \\nabla _{\\overline {x}} x^{\\mathrm {H}}A^{\\mathrm {H}}b = A^{\\mathrm {H}}b. \\end{equation} Hence \\begin{equation} \\nabla _{\\overline {x}} \\varphi (x) = A^{\\mathrm {H}}Ax - A^{\\mathrm {H}}b. \\end{equation} When this gradient is 0, we also obtain the normal equation \\(A^{\\mathrm {H}}Ax = A^{\\mathrm {H}}b\\). 6 Summary &lt;ul class=&#39;itemize1&#39;&gt; &lt;li class=&#39;itemize&#39;&gt; &lt;!-- l. 280 --&gt;&lt;p class=&#39;noindent&#39;&gt;The solution of a linear system \\(Ax=b\\) is equivalent to finding the minimum point of a functional, if the linear operator \\(A\\) satisfies some properties. When \\(A\\) is a partial differential operator, there are infinite number of DoFs to be solved. When \\(A\\) is a large matrix, as in FEM or BEM which is usually the discrete version of a corresponding partial differential operator, there are still a large number of unknowns to be solved. If the said functional is found, the solution of &lt;span class=&#39;p1xb-x-x-109&#39;&gt;many &lt;/span&gt;unknowns is converted to the minimization of a &lt;span class=&#39;p1xb-x-x-109&#39;&gt;single &lt;/span&gt;objective function, which can be reckoned as a kind of simplification or reduction. &lt;/p&gt;&lt;/li&gt; &lt;li class=&#39;itemize&#39;&gt; &lt;!-- l. 281 --&gt;&lt;p class=&#39;noindent&#39;&gt;The critical point of the functional \\(\\varphi (x) = \\frac {1}{2}( Ax,x ) - ( b,x )\\) in the real valued case, or the critical point of the functional \\(\\varphi (x) = \\frac {1}{2} (Ax,x) - \\real ( b,x )\\) in the complex valued case, is the solution of \\(Ax=b\\). &lt;/p&gt;&lt;/li&gt; &lt;li class=&#39;itemize&#39;&gt; &lt;!-- l. 282 --&gt;&lt;p class=&#39;noindent&#39;&gt;The critical point of the residual norm \\(\\lVert b - Ax \\rVert _2\\) in both real and complex valued cases is the solution of the normal equation \\(A^{\\mathrm {T}}Ax = A^{\\mathrm {T}}b\\) in the real valued case or \\(A^{\\mathrm {H}}Ax = A^{\\mathrm {H}}b\\) in the complex valued case, which is also the linear least square solution of \\(Ax=b\\).&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt; &lt;!-- l. 1 --&gt;&lt;p class=&#39;noindent&#39;&gt; &lt;/p&gt; References    Yousef Saad. Iterative Methods for Sparse Linear Systems. Other Titles in Applied Mathematics. Society for Industrial and Applied Mathematics. ISBN 978-0-89871-534-7. doi: 10.1137/1.9780898718003.    Stefan Sauter and Christoph Schwab. Boundary Element Methods. Springer Science &amp; Business Media. ISBN 978-3-540-68093-2.    Olaf Steinbach. Numerical Approximation Methods for Elliptic Boundary Value Problems: Finite and Boundary Elements. Springer Science &amp; Business Media. ISBN 978-0-387-31312-2. 1Complex valued problems will be met, when we solve harmonic acoutics or electromagnetic equations, i.e Helmholtz equations.","headline":"Minimization of a functional for solving linear problems","mainEntityOfPage":{"@type":"WebPage","@id":"https://jihuan-tian.github.io/math/2025/06/07/minimization-of-a-functional-for-solving-linear-problems.html"},"url":"https://jihuan-tian.github.io/math/2025/06/07/minimization-of-a-functional-for-solving-linear-problems.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/font.css">
  <link rel="stylesheet" href="/assets/main.css">
  <link rel="stylesheet" href="/assets/css/htmlize-syntax-highlight.css">
  
    <link rel="stylesheet" href="/assets/css/make4ht.css">
  
  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon.ico?">
  <link href="https://fonts.googlefonts.cn/css?family=EB+Garamond" rel="stylesheet"><link type="application/atom+xml" rel="alternate" href="https://jihuan-tian.github.io/feed.xml" title="止于至善" />
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/SVG"],
    extensions: ["tex2jax.js", "TeX/AMSmath.js", "TeX/AMSsymbols.js", "TeX/noUndefined.js", "TeX/AMScd.js"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      skipTags: ["script","noscript","style","textarea","pre","code"],
      processEscapes: true,
      processEnvironments: true,
      preview: "TeX"
    },
    TeX: {
      Macros: {
        intd: "\\,{\\rm d}",
        diff: "{\\rm d}",
        Diff: "{\\rm D}",
        pdiff: "\\partial",
        DD: ["\\frac{\\diff}{\\diff #2}\\left( #1 \\right)", 2],
        Dd: ["\\frac{\\diff #1}{\\diff #2}", 2],
        PD: ["\\frac{\\pdiff}{\\pdiff #2}\\left( #1 \\right)", 2],
        Pd: ["\\frac{\\pdiff #1}{\\pdiff #2}", 2],
        rme: "{\\rm e}",
        rmi: "{\\rm i}",
        rmj: "{\\rm j}",
        vect: ["\\boldsymbol{#1}", 1],
        dform: ["\\overset{\\rightharpoonup}{\\boldsymbol{#1}}", 1],
        cochain: ["\\overset{\\rightharpoonup}{#1}", 1],
        bigabs: ["\\bigg\\lvert#1\\bigg\\rvert", 1],
        Abs: ["\\big\\lvert#1\\big\\rvert", 1],
        abs: ["\\lvert#1\\rvert", 1],
        bignorm: ["\\bigg\\lVert#1\\bigg\\rVert", 1],
        Norm: ["\\big\\lVert#1\\big\\rVert", 1],
        norm: ["\\lVert#1\\rVert", 1],
        normvect: "\\vect{n}",
        ouset: ["\\overset{#3}{\\underset{#2}{#1}}", 3],
        cscript: ["\\;\\; #1", 1],
        suchthat: "\\textit{S.T. }",
        prefstar: "\\ast",
        restrict: "\\big\\vert",
        sgn: "{\\rm sgn}",
        erf: "{\\rm erf}",
        Bd: "{\\rm Bd}",
        Int: "{\\rm Int}",
        dim: "{\\rm dim}",
        rank: "{\\rm rank}",
        range: "{\\rm range}",
        divergence: "{\\rm div}",
        curl: "{\\rm curl}",
        grad: "{\\rm grad}",
        tr: "{\\rm tr}",
        lhs: "{\\rm LHS}",
        rhs: "{\\rm RHS}",
        span: "{\\rm span}",
        diag: "{\\rm diag}",
        argmin: "{\\rm argmin}",
        argmax: "{\\rm argmax}",
        esssup: "{\\rm esssup}",
        essinf: "{\\rm essinf}",
        kernel: "{\\rm ker}",
        image: "{\\rm Im}",
        diam: "{\\rm diam}",
        dist: "{\\rm dist}",
        const: "{\\rm const}",
        real: "{\\rm Re}",
        imag: "{\\rm Imag}"
      },
      equationNumbers: { autoNumber: "AMS" }
    }
  });
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_SVG"></script>

  
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">止于至善</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
          <!-- Enforce a fixed order for my categories. -->
          <a class="page-link" href="/math/">Math</a>
          <a class="page-link" href="/computer/">Computer</a>
          <a class="page-link" href="/thoughts/">Thoughts</a>
          <a class="page-link" href="/tags/">Tags</a>
          <a class="page-link" href="/about/">About</a>
        </div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Minimization of a functional for solving linear problems</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2025-06-07T00:00:00+08:00" itemprop="datePublished">Jun 7, 2025 &nbsp;

        
          Categories:
          
            <a href="/math">Math</a>
          
         &nbsp;
        
          Tags:
          
            <a href="/tags/linear-algebra">linear-algebra</a>
          
            <a href="/tags/PDE">PDE</a>
          
        
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h3 class='likesectionHead'><a id='x1-1000'></a>Contents</h3>
   <div class='tableofcontents'>
    <span class='sectionToc'>1 <a href='#x1-20001' id='QQ2-1-2'>General idea</a></span>
<br />    <span class='sectionToc'>2 <a href='#x1-30002' id='QQ2-1-3'>Gradient of the functional \(\varphi (x)\) for \(Ax=b\) (real valued)</a></span>
<br />    <span class='sectionToc'>3 <a href='#x1-40003' id='QQ2-1-4'>Wirtinger derivatives for complex functions</a></span>
<br />    <span class='sectionToc'>4 <a href='#x1-50004' id='QQ2-1-5'>Gradient of the functional \(\varphi (x)\) for \(Ax=b\) (complex valued)</a></span>
<br />    <span class='sectionToc'>5 <a href='#x1-60005' id='QQ2-1-6'>Gradient of the residual norm in linear least square problems</a></span>
<br />    <span class='sectionToc'>6 <a href='#x1-70006' id='QQ2-1-7'>Summary</a></span>
   </div>
<!-- l. 23 --><p class='indent'>   <span class='p1xb-x-x-109'>Abstract </span>Minimization of a functional can be considered as a reduction or simplification method for solving a
linear system with many or infinite number of degree of freedoms (DoFs). The minimum point can be
found by computing the critical point of the functional, which is achieved when the gradient of the
functional is zero. In the complex valued case, the evaluation of the gradient requires Wirtinger
derivatives, where the complex variable \(z\) and its complex conjugation \(\overline {z}\) are independent from each
other.
</p>
   <h3 class='sectionHead'><span class='titlemark'>1    </span> <a id='x1-20001'></a>General idea</h3>
<!-- l. 26 --><p class='noindent'>According to <a href='/math/2024/10/26/variational-problems.html'>Variational problems</a>, a linear operator equation \(Au=f\) (as a strong form problem) is equivalent to a
variational equation \(\langle Au,v \rangle = \langle f,v \rangle \) (as a weak form problem) due to the boundedness of the operator \(A\). Furthermore, the
variational equation is equivalent to the minimization of a functional \(\mathcal {L}(v)=\frac {1}{2}\langle Av,v \rangle - \langle f,v \rangle \), if the linear operator \(A\) is also self-adjoint
(in the sense of normed space, which is also called self-dual, see <a href='/math/2024/11/10/adjoint-operators-in-functional-analysis.html'>Adjoint operators in functional analysis</a>) and
elliptic. The variational equation is based on the notion of measurement by projection, while the functional
minimization problem is based on the notion of minimizing energy, such as the <a href='http://www.scholarpedia.org/article/Principle_of_least_action'>principle of least action</a> in
physics.
</p><!-- l. 28 --><p class='indent'>   Even though the above theory is usually introduced in a book about PDE, such as (<a href='#XSteinbachNumerical2007'>Steinbach</a>), it is actually a
general theory, which naturally holds for linear algebra where finite dimensional spaces are involved. For
                                                                                               
                                                                                               
example, to solve the linear system \(Ax=b\), where \(A\in \mathbb {R}^{n\times n}\) is symmetric positive definite (SPD), \(x\in \mathbb {R}^n\) and \(b\in \mathbb {R}^n\), we can search the critical
point \(x\) which minimizes a functional \(\varphi (x) = \frac {1}{2} ( Ax,x ) - ( b,x )\), where \(( \cdot ,\cdot )\) is the inner product in \(\mathbb {R}^{n}\). According to <a href='/math/2025/05/28/understanding-about-the-lagrange-multiplier-method.html'>Understanding about the
Lagrange multiplier method and its application in PDEs</a>, the critical point of \(\varphi (x)\) must be a minimum
point, if the operator \(A\) is positive semi-definite. Of course, if \(A\) is a SPD matrix in \(\mathbb {R}^{n\times n}\), this condition is
satisfied.
</p><!-- l. 30 --><p class='indent'>   If we use an iterative algorithm, such as the steepest descent method, we need to construct a sequence \(\{ x^{(k)} \}_{k\geq 1}\),
which minimizes the functional \(\varphi (x)\) incrementally. In each iteration step, with the previous vector \(x^{(k-1)}\), we search the
next vector \(x^{(k)}\) along the negative gradient direction \(-\nabla \varphi (x) \Big \vert _{x^{(k-1)}}\), the concept of which is the same as the Newton’s method in
1D search. The step size in this search direction is an unknown factor \(\alpha ^{(k)}\), which can be obtained by solving the
critical point of this functional \begin{equation}  \tilde {\varphi }(\alpha ^{(k)}) = \varphi \left (x^{(k)} + \alpha ^{(k)}r^{(k)} \right ),  \end{equation}<a id='x1-2001r1'></a> where \(r^{(k)}\) is the unit vector of \(-\nabla \varphi (x)\Big \vert _{x^{(k-1)}}\). Now, let’s see what on earth the gradient of the
functional is.
</p>
   <h3 class='sectionHead'><span class='titlemark'>2    </span> <a id='x1-30002'></a>Gradient of the functional \(\varphi (x)\) for \(Ax=b\) (real valued)</h3>
   <div class='theorem'><div class='newtheorem'>
<!-- l. 37 --><p class='noindent'><span class='head'>
<span class='p1xb-x-x-109'>Theorem 1</span> </span><a id='x1-3002'></a><span class='p1xi-x-x-109'>The gradient of </span>\(\varphi (x)\) <span class='p1xi-x-x-109'>is </span>\(Ax-b\)<span class='p1xi-x-x-109'>.</span>
</p>
   </div>
<!-- l. 39 --><p class='indent'>   </p></div>
   <div class='proof'><div class='newtheorem'>
<!-- l. 41 --><p class='noindent'><span class='head'>
<span class='p1xsc-x-x-109'>P<span class='small-caps'>roof</span></span> </span><a id='x1-3004'></a>\begin{equation}  \begin{aligned} \varphi (x) &amp;= \frac {1}{2} ( Ax,x ) - ( b,x ) = \frac {1}{2} x^{\mathrm {T}}Ax - x^{\mathrm {T}}b \\ &amp;= \frac {1}{2} \sum _{i=1}^n \sum _{j=1}^n a_{ij}x_ix_j - \sum _{i=1}^n b_ix_i. \end{aligned}  \end{equation}<a id='x1-3005r2'></a> We need to enforce the conditions \begin{equation}  \frac {\diff \varphi (x)}{\diff x_k} = 0 \quad k=1,\cdots ,n.  \end{equation}<a id='x1-3006r3'></a> For the first term \(\frac {1}{2} \sum _{i=1}^n \sum _{j=1}^n a_{ij}x_ix_j\) in \(\varphi (x)\), we consider the following cases about
\(k\):
</p><!-- l. 55 --><p class='indent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-3008x1'>
     <!-- l. 56 --><p class='noindent'>When \(k\neq i\) and \(k\neq j\), \begin{equation}  \frac {\partial a_{ij}x_ix_j}{\partial x_{k}} = 0.  \end{equation}<a id='x1-3009r4'></a>
     </p></li>
<li class='enumerate' id='x1-3011x2'>
     <!-- l. 60 --><p class='noindent'>When \(k=i\) and \(k\neq j\), \begin{equation}  \frac {\partial a_{ij}x_ix_j}{\partial x_{k}} = \frac {\partial a_{kj}x_kx_j}{\partial x_k} = a_{kj}x_j.  \end{equation}<a id='x1-3012r5'></a>
     </p></li>
<li class='enumerate' id='x1-3014x3'>
     <!-- l. 65 --><p class='noindent'>When \(k\neq i\) and \(k=j\), \begin{equation}  \frac {\partial a_{ij}x_ix_j}{\partial x_{k}} = \frac {\partial a_{ik}x_ix_k}{\partial x_k} = a_{ik}x_i.  \end{equation}<a id='x1-3015r6'></a>
                                                                                               
                                                                                               
     </p></li>
<li class='enumerate' id='x1-3017x4'>
     <!-- l. 69 --><p class='noindent'>When \(k=i=j\), \begin{equation}  \frac {\partial a_{ij}x_ix_j}{\partial x_{k}} = \frac {\partial a_{kk}x_k^2}{\partial x_k} = 2a_{kk}x_k.  \end{equation}<a id='x1-3018r7'></a></p></li></ol>
<!-- l. 76 --><p class='indent'>   Then the partial derivative of the first term with respect to \(x_k\) is \begin{equation}  \frac {1}{2} \sum _{\substack {j=1 \\ j\neq k}}^n a_{kj}x_j + \frac {1}{2} \sum _{\substack {i=1 \\ i\neq k}}^n a_{ik}x_i + a_{kk}x_k.  \end{equation}<a id='x1-3019r8'></a> Because \(A\) is SPD, \(a_{ik} = a_{ki}\), the first and second terms in
the above expression can be merged, hence the above expression becomes \begin{equation}  \sum _{\substack {j=1 \\ j\neq k}}^n a_{kj}x_j + a_{kk}x_k = \sum _{j=1}^n a_{kj}x_j.  \end{equation}<a id='x1-3020r9'></a> This is just the \(k\)-th component of the
vector \(Ax\).
</p><!-- l. 87 --><p class='indent'>   The partial derivative of the second term in \(\varphi (x)\) with respect to \(x_k\) is simply \(-b_k\). Therefore, we have
\(\nabla \varphi (x) = Ax-b\).
</p>
   </div>
<!-- l. 88 --><p class='indent'>   </p></div>
   <div class='newtheorem'>
<!-- l. 90 --><p class='noindent'><span class='head'>
<span class='p1xb-x-x-109'>Comment 1</span> </span><a id='x1-3022'></a><span class='p1xi-x-x-109'>The gradient of </span>\((Ax,x)\) <span class='p1xi-x-x-109'>with respect to </span>\(x\) <span class='p1xi-x-x-109'>is </span>\(2Ax\) <span class='p1xi-x-x-109'>and the gradient of </span>\((b,x)\) <span class='p1xi-x-x-109'>is </span>\(b\)<span class='p1xi-x-x-109'>. This is a generalization of the derivation
</span><span class='p1xi-x-x-109'>rule for scalar values, i.e. </span>\(\frac {\diff (ax^2)}{\diff x} = 2ax\) <span class='p1xi-x-x-109'>and </span>\(\frac {\diff (bx)}{\diff x} = b\)<span class='p1xi-x-x-109'>.</span>
</p>
   </div>
<!-- l. 92 --><p class='indent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>3    </span> <a id='x1-40003'></a>Wirtinger derivatives for complex functions</h3>
<!-- l. 95 --><p class='noindent'>Before we examine the gradient of the functional \(\varphi (x)\) in the complex valued case, we need to clarify the derivatives
of a function with respect to a complex variable \(x\) and its complex conjugate \(\overline {x}\), both of which will appear in the
functional \(\varphi (x)\). The basic idea is that even though \(x\) is related to \(\overline {x}\) via complex conjugation, these two variables are
actually independent. This determines how we compute the gradient of \(\varphi (x)\). The reason for \(x\) and \(\overline {x}\) are independent is
given below.
</p><!-- l. 97 --><p class='indent'>   Let \(f\) be a complex valued function dependent on a complex variable \(z\). If \(f\) is differentiable at \(z_0\), the variation of
the function value at \(z_0 + \Delta z\) can be written as \begin{equation}  \Delta f(z_0) = f(z_0 + \Delta z) - f(z_0) = \frac {\partial f}{\partial x} \Delta x + \frac {\partial f}{\partial y} \Delta y + o(\Delta z),  \end{equation}<a id='x1-4001r10'></a> where \(o(\Delta z)/\lvert \Delta z \rvert \rightarrow 0\) when \(\Delta z \rightarrow 0\). We should bear in mind that a complex
function is just a complex valued function defined on a 2D plane. Therefore, its variation around the
point \(z_0\) can be represented as a combination of the variations in the \(x\) direction and \(y\) direction. If we
define \(\Delta z = \Delta x + \rmi \Delta y\) and \(\Delta \overline {z} = \Delta x - \rmi \Delta y\), we can represent \((\Delta x, \Delta y)\) with \((\Delta z, \Delta \overline {z})\) as \begin{equation}  \Delta x = \frac {1}{2} (\Delta z + \Delta \overline {z}), \Delta y = \frac {1}{2\rmi } ( \Delta z - \Delta \overline {z} ).  \end{equation}<a id='x1-4002r11'></a> Then \begin{equation}  \Delta f(z_0) = \frac {1}{2} \left ( \frac {\partial f}{\partial x} - \rmi \frac {\partial f}{\partial y} \right ) \Delta z + \frac {1}{2} \left ( \frac {\partial f}{\partial x} + \rmi \frac {\partial f}{\partial y}\right ) \Delta \overline {z} + o(\Delta z)  \end{equation}<a id='x1-4003r12'></a> and divide it by \(\Delta z\), we have \begin{equation}  \begin{aligned} \frac {\mathrm {d} f}{\mathrm {d} z} \Big \vert _{z_0} &amp;= \lim _{\substack {\Delta z \rightarrow 0 \\ \Delta z \in \mathbb {C}}} \left [ \frac {1}{2} \left ( \frac {\partial f}{\partial x} - \rmi \frac {\partial f}{\partial y} \right ) + \frac {1}{2} \left (\frac {\partial f}{\partial x} + \rmi \frac {\partial f}{\partial y} \right ) \frac {\Delta \overline {z}}{\Delta z} + \frac {o(\Delta z)}{\Delta z} \right ]. \end{aligned}  \end{equation}<a id='x1-4004r13'></a> Because \(\frac {\Delta \overline {z}}{\Delta z}\) depends on the
path of \(\Delta z\) approaching \(0\), it does not have a limiting value. To make \(\frac {\diff f}{\diff z}\) meaningful, we should enforce \(\frac {\partial f}{\partial x} + \rmi \frac {\partial f}{\partial y} = 0\),
which is just equivalent to the Cauchy-Riemann equations. Therefore, with the differentiability of \(f\)
with respect to real variables \(x\) and \(y\) as well as the Cauchy-Riemann equations, we have \begin{equation}  \frac {\diff f}{\diff z} \Big \vert _{z_0} = \frac {1}{2} \left ( \frac {\partial f}{\partial x} - \rmi \frac {\partial f}{\partial y} \right ).  \end{equation}<a id='x1-4005r14'></a> Similarly,
the derivative of \(f\) with respect to \(\overline {z}\) can be written as \begin{equation}  \begin{aligned} \frac {\mathrm {d} f}{\mathrm {d} \overline {z}} \Big \vert _{z_0} &amp;= \lim _{\substack {\Delta z \rightarrow 0 \\ \Delta z \in \mathbb {C}}} \left [ \frac {1}{2} \left ( \frac {\partial f}{\partial x} - \rmi \frac {\partial f}{\partial y} \right ) \frac {\Delta z}{\Delta \overline {z}} + \frac {1}{2} \left (\frac {\partial f}{\partial x} + \rmi \frac {\partial f}{\partial y} \right ) + \frac {o(\Delta z)}{\Delta z} \right ]. \end{aligned}  \end{equation}<a id='x1-4006r15'></a> Enforcing \(\frac {\partial f}{\partial x} - \rmi \frac {\partial f}{\partial y} = 0\), which is the counterpart of the
Cauchy-Riemann equations when \(f\) is a function of \(\overline {z}\) instead of \(z\), we have \begin{equation}  \frac {\diff f}{\diff \overline {z}} \Big \vert _{z_0} = \frac {1}{2} \left ( \frac {\partial f}{\partial x} + \rmi \frac {\partial f}{\partial y} \right ).  \end{equation}<a id='x1-4007r16'></a> The above \(\frac {\diff f}{\diff z}\) and \(\frac {\diff f}{\diff \overline {z}}\) are called Wirtinger
derivatives.
</p>
   <div class='newtheorem'>
<!-- l. 145 --><p class='noindent'><span class='head'>
                                                                                               
                                                                                               
<span class='p1xb-x-x-109'>Comment 2</span> </span><a id='x1-4009'></a><span class='p1xi-x-x-109'>Because  a  complex  function  </span>\(f\)  <span class='p1xi-x-x-109'>can  be  considered  as  a  complex  valued  function  on  the  </span>\(xy\)  <span class='p1xi-x-x-109'>plane,  its
</span><span class='p1xi-x-x-109'>variation in the neighborhood about a point </span>\(z_0\) <span class='p1xi-x-x-109'>can be decomposed into </span><span class='p1xbi-x-x-109'>real </span><span class='p1xi-x-x-109'>partial derivatives along two orthogonal
</span><span class='p1xi-x-x-109'>directions </span>\(x\) <span class='p1xi-x-x-109'>and </span>\(y\)<span class='p1xi-x-x-109'>. On the other hand, the function variation can be represented with the </span><span class='p1xbi-x-x-109'>complex </span><span class='p1xi-x-x-109'>derivative with
</span><span class='p1xi-x-x-109'>respect  to  </span>\(z\)<span class='p1xi-x-x-109'>,  which  is  along  the  direction  angle  </span>\(\mathrm {atan}(x,y)\)  <span class='p1xi-x-x-109'>or  with  respect  to  </span>\(\overline {z}\)<span class='p1xi-x-x-109'>,  which  is  along  the  direction  angle  </span>\(\mathrm {atan}(x,-y)\)<span class='p1xi-x-x-109'>.  These
</span><span class='p1xi-x-x-109'>two directions are linearly independent. Therefore, even though the two variables </span>\(z\) <span class='p1xi-x-x-109'>and </span>\(\overline {z}\) <span class='p1xi-x-x-109'>are linked via complex
</span><span class='p1xi-x-x-109'>conjugation, they are two independent variables.</span>
</p>
   </div>
<!-- l. 147 --><p class='indent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>4    </span> <a id='x1-50004'></a>Gradient of the functional \(\varphi (x)\) for \(Ax=b\) (complex valued)</h3>
<!-- l. 150 --><p class='noindent'>When  the  matrix  \(A\)  and  vectors  \(x\)  and  \(b\)  are  complex  valued
<span class='footnote-mark'><a href='#fn1x0' id='fn1x0-bk'><sup class='textsuperscript'>1</sup></a></span><a id='x1-5001f1'></a>, the
definition of the inner product in \(\mathbb {C}^n\) involves complex conjugation. For any \(x\) and \(y\) in \(\mathbb {C}^n\), the following conditions
should be satisfied:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-5004x1'>
     <!-- l. 152 --><p class='noindent'>\((x,y) = \overline {(y,x)}\);
     </p></li>
<li class='enumerate' id='x1-5006x2'>
     <!-- l. 153 --><p class='noindent'>\(( \alpha x,y ) = \alpha (x,y)\);
     </p></li>
<li class='enumerate' id='x1-5008x3'>
     <!-- l. 154 --><p class='noindent'>\((x,\alpha y) = \overline {\alpha } (x,y)\).</p></li></ol>
<!-- l. 156 --><p class='noindent'>Meanwhile, the SPD condition of \(A\) now becomes Hermite symmetric (\(A = A^{\mathrm {H}}\)) and positive definite.
</p><!-- l. 158 --><p class='indent'>   In the complex valued case, the range of the functional \(\varphi (x)\) should still be \(\mathbb {R}\), since it is related to energy. Therefore,
the term \(( b,x )\) in the original \(\varphi (x)\), which is not necessarily real valued, should be changed to \(\real ( b,x )\), while \(\frac {1}{2} ( Ax,x )\) must be real valued
because \(A\) is SPD. Then the functional is (<a href='#XSauterBoundary2010'>Sauter and Schwab</a>, page 354) \begin{equation}  \begin{aligned} \varphi (x) &amp;= \frac {1}{2} ( Ax,x )-\real (b,x) = \frac {1}{2} x^{\mathrm {H}}Ax - \real (x^{\mathrm {H}}b) \\ &amp;= \frac {1}{2} \sum _{i=1}^n \sum _{j=1}^n a_{ij}\overline {x}_ix_j - \real \left ( \sum _{i=1}^n b_i \overline {x}_i \right ). \end{aligned}  \end{equation}<a id='x1-5009r17'></a> First, let’s see the gradient of \(\varphi (x)\)
with respect to \(x\). Before proceeding, we notice that the second term above does not contain \(x\) at all.
If we directly evaluate \(\nabla _x \varphi (x)\) using this expression, the result will not depend on \(b\), which is obviously
incorrect. Therefore, the functional should be written as below, which is still the same as before \begin{equation}  \begin{aligned} \varphi (x) &amp;= \frac {1}{2} ( Ax,x )-\real (x,b) = \frac {1}{2} x^{\mathrm {H}}Ax - \real (b^{\mathrm {H}}x) \\ &amp;= \frac {1}{2} \sum _{i=1}^n \sum _{j=1}^n a_{ij}\overline {x}_ix_j - \real \left ( \sum _{i=1}^n \overline {b}_i x_i \right ). \end{aligned}  \end{equation}<a id='x1-5010r18'></a> The
complex partial derivative of the first term in the above expression with respect to \(x_k\) is \begin{equation}  \frac {\partial }{\partial x_k} \left ( \frac {1}{2} \sum _{i=1}^n \sum _{j=1}^n a_{ij} \overline {x}_ix_j \right ) = \frac {1}{2} \sum _{i=1}^{n} a_{ik} \overline {x}_i.  \end{equation}<a id='x1-5011r19'></a> For the second
term, only when \(i=k\), the term in the sum contributes to the complex partial derivative of \(\varphi (x)\). Let \(b_k = b_{k1} + \rmi b_{k2}\) and \(x_k = x_{k1} + \rmi x_{k2}\), \begin{equation}  \real (\overline {b}_k x_k) = \real (b_{k1}x_{k1} + b_{k2}x_{k2} + \rmi (b_{k1}x_{k2} - b_{k2}x_{k1})) = b_{k1}x_{k1} + b_{k2}x_{k2}.  \end{equation}<a id='x1-5012r20'></a>
Using the Wirtinger derivative, the complex partial derivative of the second term with respect to \(x_k\) is \begin{equation}  \begin{aligned} \frac {\partial }{\partial x_k} \real (\overline {b}_kx_k) &amp;= \frac {\partial }{\partial x_k} (b_{k1}x_{k1} + b_{k2}x_{k2}) \\ &amp;= \frac {1}{2} \left ( \frac {\partial }{\partial x_{k1}} - \rmi \frac {\partial }{\partial x_{k2}} \right ) (b_{k1}x_{k1} + b_{k2}x_{k2}) \\ &amp;= \frac {1}{2} ( b_{k1} - \rmi b_{k2} ) = \frac {1}{2} \overline {b}_k. \end{aligned}  \end{equation}<a id='x1-5013r21'></a>
Then the gradient \(\nabla _x \varphi (x)\) is \begin{equation}  \nabla _x\varphi (x) = \frac {1}{2} \overline {A^{\mathrm {H}}x} - \frac {1}{2} \overline {b}.  \end{equation}<a id='x1-5014r22'></a> When \(\nabla _x\varphi (x) = 0\), we have \begin{equation}  \overline {A^{\mathrm {H}}x} = \overline {b} \Leftrightarrow A^{\mathrm {H}}x = b.  \end{equation}<a id='x1-5015r23'></a> Because \(A\) is Hermite symmetric, this is further equivalent to
\(Ax = b\).
                                                                                               
                                                                                               
</p><!-- l. 205 --><p class='indent'>   Second, we check the gradient of \(\varphi (x)\) with respect to \(\overline {x}\). Using the same procedure, we still obtain \(Ax=b\), when
\(\nabla _{\overline {x}}\varphi (x)=0\).
</p>
   <h3 class='sectionHead'><span class='titlemark'>5    </span> <a id='x1-60005'></a>Gradient of the residual norm in linear least square problems</h3>
<!-- l. 208 --><p class='noindent'>For a linear system \(Ax=b\), where \(A\in \mathbb {R}^{m\times n}\) with \(m\geq n\), if \(x\) minimizes the 2-norm of the residual vector \(\lVert b - Ax \rVert _2\), it is called the linear least
square solution. This is equivalent to solve the normal equation \(A^{\mathrm {T}}Ax=b\), or \(A^{\mathrm {H}}Ax=b\) in the complex valued case. Then we will
show this equivalence.
</p><!-- l. 210 --><p class='indent'>   First, we consider the real valued case. The 2-norm of the residual vector can also be considered as a
functional \begin{equation}  \varphi (x) = \lVert b - Ax \rVert _2 = ( Ax-b,Ax-b ) = ( Ax,Ax ) + ( b,b ) - ( Ax,b ) - ( b,Ax ).  \end{equation}<a id='x1-6001r24'></a> The gradient of the first term in this functional is \begin{equation}  \nabla _x ( Ax,Ax ) = \nabla _x ( A^{\mathrm {T}}Ax,x ) = 2A^{\mathrm {T}}Ax.  \end{equation}<a id='x1-6002r25'></a>
</p><!-- l. 219 --><p class='indent'>   The gradient of the third term in \(\varphi (x)\) is \begin{equation}  \nabla _x ( Ax,b ) = \nabla _x b^{\mathrm {T}}Ax = b^{\mathrm {T}}A.  \end{equation}<a id='x1-6003r26'></a> Write it as a column vector \begin{equation}  \nabla _x ( Ax,b ) = A^{\mathrm {T}}b.  \end{equation}<a id='x1-6004r27'></a>
</p><!-- l. 228 --><p class='indent'>   The gradient of the fourth term in \(\varphi (x)\) is \begin{equation}  \nabla _x ( b,Ax ) = \nabla _x x^{\mathrm {T}}A^{\mathrm {T}}b = A^{\mathrm {T}}b.  \end{equation}<a id='x1-6005r28'></a>
</p><!-- l. 233 --><p class='indent'>   Therefore, \begin{equation}  \nabla _x \varphi (x) = 2A^{\mathrm {T}}Ax - 2A^{\mathrm {T}}b.  \end{equation}<a id='x1-6006r29'></a> When it is 0, we have \(A^{\mathrm {T}}Ax = A^{\mathrm {T}}b\), which is the normal equation.
</p><!-- l. 239 --><p class='indent'>   Second, we consider the complex valued case. The functional \(\varphi (x)\) still takes its original form, since \(( Ax,Ax )\) and \(( b,b )\) are real
valued for sure, while \(( Ax,b ) + ( b,Ax )\) is also real valued.
</p><!-- l. 241 --><p class='indent'>   For the gradient with respect to \(x\), we have \begin{equation}  \nabla _x ( Ax,Ax ) = \nabla _x x^{\mathrm {H}}A^{\mathrm {H}}Ax = x^{\mathrm {H}}A^{\mathrm {H}}A,  \end{equation}<a id='x1-6007r30'></a> \begin{equation}  \nabla _x ( Ax,b ) = b^{\mathrm {H}}A  \end{equation}<a id='x1-6008r31'></a> and \begin{equation}  \nabla _x ( b,Ax ) = 0.  \end{equation}<a id='x1-6009r32'></a> Hence \begin{equation}  \nabla _x \varphi (x) = x^{\mathrm {H}}A^{\mathrm {H}}A - b^{\mathrm {H}}A.  \end{equation}<a id='x1-6010r33'></a> When the gradient with respect to \(x\) is 0, we obtain the
normal equation \begin{equation}  x^{\mathrm {H}}A^{\mathrm {H}}A - b^{\mathrm {H}}A = 0 \Leftrightarrow A^{\mathrm {H}}Ax = A^{\mathrm {H}}b.  \end{equation}<a id='x1-6011r34'></a>
</p><!-- l. 261 --><p class='indent'>   For the gradient with respect to \(\overline {x}\), we have \begin{equation}  \nabla _{\overline {x}} ( Ax,Ax ) = \nabla _{\overline {x}} x^{\mathrm {H}}A^{\mathrm {H}}Ax = A^{\mathrm {H}}Ax,  \end{equation}<a id='x1-6012r35'></a> \begin{equation}  \nabla _{\overline {x}} (Ax,b ) = 0  \end{equation}<a id='x1-6013r36'></a> and \begin{equation}  \nabla _{\overline {x}} ( b,Ax ) = \nabla _{\overline {x}} x^{\mathrm {H}}A^{\mathrm {H}}b = A^{\mathrm {H}}b.  \end{equation}<a id='x1-6014r37'></a> Hence \begin{equation}  \nabla _{\overline {x}} \varphi (x) = A^{\mathrm {H}}Ax - A^{\mathrm {H}}b.  \end{equation}<a id='x1-6015r38'></a> When this gradient is 0, we also obtain the normal
equation \(A^{\mathrm {H}}Ax = A^{\mathrm {H}}b\).
</p>
   <h3 class='sectionHead'><span class='titlemark'>6    </span> <a id='x1-70006'></a>Summary</h3>
     <ul class='itemize1'>
     <li class='itemize'>
     <!-- l. 280 --><p class='noindent'>The solution of a linear system \(Ax=b\) is equivalent to finding the minimum point of a functional, if the
     linear operator \(A\) satisfies some properties. When \(A\) is a partial differential operator, there are infinite
     number of DoFs to be solved. When \(A\) is a large matrix, as in FEM or BEM which is usually the
     discrete version of a corresponding partial differential operator, there are still a large number of
     unknowns to be solved. If the said functional is found, the solution of <span class='p1xb-x-x-109'>many </span>unknowns is converted
     to the minimization of a <span class='p1xb-x-x-109'>single </span>objective function, which can be reckoned as a kind of simplification
     or reduction.
     </p></li>
     <li class='itemize'>
     <!-- l. 281 --><p class='noindent'>The critical point of the functional \(\varphi (x) = \frac {1}{2}( Ax,x ) - ( b,x )\) in the real valued case, or the critical point of the functional \(\varphi (x) = \frac {1}{2} (Ax,x) - \real ( b,x )\) in
     the complex valued case, is the solution of \(Ax=b\).
     </p></li>
     <li class='itemize'>
                                                                                               
                                                                                               
     <!-- l. 282 --><p class='noindent'>The critical point of the residual norm \(\lVert b - Ax \rVert _2\) in both real and complex valued cases is the solution of the
     normal equation \(A^{\mathrm {T}}Ax = A^{\mathrm {T}}b\) in the real valued case or \(A^{\mathrm {H}}Ax = A^{\mathrm {H}}b\) in the complex valued case, which is also the linear
     least square solution of \(Ax=b\).</p></li></ul>
<!-- l. 1 --><p class='noindent'>
</p>
   <h3 class='likesectionHead'><a id='x1-8000'></a>References</h3>
<!-- l. 1 --><p class='noindent'>
  </p><div class='thebibliography'>
  <p class='bibitem'><span class='biblabel'>
<a id='XSaadIterative2003'></a><span class='bibsp'>   </span></span>Yousef Saad. <span class='p1xi-x-x-109'>Iterative Methods for Sparse Linear Systems</span>. Other Titles in Applied Mathematics. Society
  for Industrial and Applied Mathematics. ISBN 978-0-89871-534-7. doi: 10.1137/1.9780898718003.
  </p>
  <p class='bibitem'><span class='biblabel'>
<a id='XSauterBoundary2010'></a><span class='bibsp'>   </span></span>Stefan Sauter and Christoph Schwab. <span class='p1xi-x-x-109'>Boundary Element Methods</span>. Springer Science &amp; Business Media.
  ISBN 978-3-540-68093-2.
  </p>
  <p class='bibitem'><span class='biblabel'>
<a id='XSteinbachNumerical2007'></a><span class='bibsp'>   </span></span>Olaf  Steinbach.    <span class='p1xi-x-x-109'>Numerical  Approximation  Methods  for  Elliptic  Boundary  Value  Problems:  Finite  and
  </span><span class='p1xi-x-x-109'>Boundary Elements</span>. Springer Science &amp; Business Media. ISBN 978-0-387-31312-2.
</p>
  </div>
   <div class='footnotes'><a id='x1-5002x4'></a>
<!-- l. 150 --><p class='indent'>      <span class='footnote-mark'><a href='#fn1x0-bk' id='fn1x0'><sup class='textsuperscript'>1</sup></a></span><span class='p1xr-x-x-90'>Complex valued problems will be met, when we solve harmonic acoutics or electromagnetic equations, i.e Helmholtz
</span><span class='p1xr-x-x-90'>equations.</span></p>                                                                                                                                                                                                            </div>
<p></p>

  </div><a class="u-url" href="/math/2025/06/07/minimization-of-a-functional-for-solving-linear-problems.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">止于至善</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Jihuan Tian</li><li><a class="u-email" href="mailto:jihuan_tian@hotmail.com">jihuan_tian@hotmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="/feed.xml"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#rss"></use></svg> <span>RSS</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>As regards numerical analysis, mathematical electromagnetism, Linux techniques and personal thoughts.</p>
      </div>
      <div class="footer-col">
        <p>The articles are under a <a href='http://creativecommons.org/licenses/by-nc-sa/4.0/'>Creative Commons Attribution License</a>. Copyright &copy; 2025 <a href="mailto:jihuan_tian@hotmail.com">Jihuan Tian</a>.</p>
      </div>
    </div>
  </div>

</footer>
</body>

</html>
