<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://jihuan-tian.github.io/feed/math.xml" rel="self" type="application/atom+xml" /><link href="https://jihuan-tian.github.io/" rel="alternate" type="text/html" /><updated>2025-02-06T16:41:12+08:00</updated><id>https://jihuan-tian.github.io/feed/math.xml</id><title type="html">止于至善 | Math</title><subtitle>As regards numerical analysis, mathematical electromagnetism, Linux techniques and personal thoughts.</subtitle><author><name>Jihuan Tian</name></author><entry><title type="html">General theory about the construction of preconditioning bilinear form in BEM</title><link href="https://jihuan-tian.github.io/math/2024/11/16/general-theory-about-the-construction-of-preconditioning-bilinear-form-in-bem.html" rel="alternate" type="text/html" title="General theory about the construction of preconditioning bilinear form in BEM" /><published>2024-11-16T00:00:00+08:00</published><updated>2024-11-16T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/11/16/general-theory-about-the-construction-of-preconditioning-bilinear-form-in-bem</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/11/16/general-theory-about-the-construction-of-preconditioning-bilinear-form-in-bem.html"><![CDATA[<h3 class='likesectionHead'><a id='x1-1000'></a>Contents</h3>
   <div class='tableofcontents'>
    <span class='sectionToc'>1 <a href='#x1-20001' id='QQ2-1-2'>Inverse operator of the preconditioner</a></span>
<br />    <span class='sectionToc'>2 <a href='#x1-30002' id='QQ2-1-3'>Orthogonal decomposition of the domain \(V^s(\Gamma ,A)\) of \(A\)</a></span>
<br />    <span class='sectionToc'>3 <a href='#x1-40003' id='QQ2-1-4'>Define the preconditioning bilinear form</a></span>
<br />    <span class='sectionToc'>4 <a href='#x1-50004' id='QQ2-1-5'>Discretization of the preconditioning bilinear form</a></span>
<br />    <span class='sectionToc'>5 <a href='#x1-60005' id='QQ2-1-6'>Preconditioners for Laplace problem</a></span>
   </div>
<!-- l. 24 --><p class='indent'>   In BEM, the preconditioning technique is indispensable for solving a large scale PDE, when an iterative
solver, such as CG, GMRES, MinRES, BiCGStab, is adopted. Without a preconditioner, the number of iterations
will significantly increase with the deterioration of the system matrix’s condition number. A large condition
number can be caused by a large number of DoFs, distinct mesh element size, sharp corners in the geometry,
etc.
</p><!-- l. 26 --><p class='indent'>   The procedure for constructing the Galerkin matrix for a preconditioner is summarized into four
steps.
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-1002x1'>
     <!-- l. 28 --><p class='noindent'>Find a preconditioning operator having an opposite order with respect to the key operator in the
     original PDE in the theoretical framework of pseudodifferential operators (see <a href='/math/2024/11/11/basic-ideas-of-operator-preconditioning.html#x1-20001'>Pseudodifferential
     operator</a>, <a href='/math/2024/11/11/basic-ideas-of-operator-preconditioning.html#x1-30002'>Operator preconditioning based on pseudodifferential operator of opposite orders</a>). In
     BEM, this is a trivial task thanks to the properties of boundary integral operators (see <a href='/math/2024/11/11/basic-ideas-of-operator-preconditioning.html#x1-40003'>Boundary
     integral operators considered as pseudodifferential operators</a>).
     </p></li>
<li class='enumerate' id='x1-1004x2'>
                                                                                               
                                                                                               
     <!-- l. 29 --><p class='noindent'>Construct  an  inverse  operator  for  the  preconditioner  obtained  from  the  previous  step.  If  the
     preconditioner is not elliptic on its whole domain, we should construct a Moore-Penrose pseudo
     inverse  or  generalized  inverse  operator  instead.  Alternatively,  we  can  build  an  approximate
     operator for the preconditioner, which is elliptic on the whole domain. Then its inverse operator is
     available.
     </p></li>
<li class='enumerate' id='x1-1006x3'>
     <!-- l. 30 --><p class='noindent'>Construct  a  bilinear  form  for  the  above  inverse  operator,  which  is  spectrally  equivalent  to  the
     bilinear form in the variational form of the PDE. The lower bound and upper bound constants
     in  this  spectral  equivalence  condition  do  not  depend  on  the  geometric  discretization  and  finite
     element (function space) discretization.
     </p></li>
<li class='enumerate' id='x1-1008x4'>
     <!-- l. 31 --><p class='noindent'>Discretize the above bilinear form, whose inverse matrix is to be multiplied to the discretized linear
     system for the variational form of the PDE. Since the inverse operator associated with this bilinear
     form cannot be directly evaluated, we will compute an approximate version which is based on the
     inverse matrix of the Galerkin matrix for the preconditioning operator. A general matrix spectral
     equivalence theorem guarantees the equivalence between this approximate version and the exact
     version.</p></li></ol>
<!-- l. 34 --><p class='indent'>   In the following, we’ll introduce the basic ideas and theoretical details. Finally, we present the
preconditioners used in the Laplace problem with either Dirichlet or Neumann boundary condition.
</p>
   <h3 class='sectionHead'><span class='titlemark'>1    </span> <a id='x1-20001'></a>Inverse operator of the preconditioner</h3>
<!-- l. 37 --><p class='noindent'>The operator \(A: V^s(\Gamma ,A):= H^s(\Gamma )_{/\mathrm {ker}(A)} \rightarrow H^{s-2\alpha }(\Gamma )\) in the original PDE is self-adjoint (in the sense of adjointness in Banach spaces), bounded and
\(V^s(\Gamma ,A)\)-elliptic. As a pseudodifferential operator, its order is \(2\alpha \).
</p><!-- l. 39 --><p class='indent'>   The preconditioning operator \(B: H^{s-2\alpha }(\Gamma ) \rightarrow H^s(\Gamma )\) is self-adjoint, bounded and \(V^{s-2\alpha ,0}(\Gamma ,B) := H^{s-2\alpha }(\Gamma )_{/\mathrm {ker}(B)}\)-elliptic. Its order is \(-2\alpha \).
</p><!-- l. 41 --><p class='indent'>   Define the inverse operator \(B^{-1}: H^s(\Gamma ) \rightarrow H^{s-2\alpha }(\Gamma )\) of \(B\) which should be spectrally equivalent to \(A\). Assume \(B\) has a closed range and
also note \(B\) is self-adjoint, then \begin{equation}  \mathrm {Im}(B) = (\mathrm {ker}(B'))^{\circ } = (\mathrm {ker}(B))^{\circ }.  \end{equation}<a id='x1-2001r1'></a> If the operator \(B\) has a non-trivial kernel and let \(H^s(\Gamma )\) be decomposed as \(H^s(\Gamma ) = (\mathrm {ker}(B))^{\circ } \oplus [(\mathrm {ker}(B))^{\circ }]^{\mathrm {c}}\), a generalized
inverse \(B^{+}: H^s(\Gamma ) \rightarrow H^{s-2\alpha }(\Gamma )\) can be defined \begin{equation}  B^{+} = \begin {cases} \dot {B}^{-1}(y) &amp; y\in (\mathrm {ker}(B))^{\circ } \\ 0 &amp; y\in [(\mathrm {ker}(B))^{\circ }]^{\mathrm {c}} \end {cases},  \end{equation}<a id='x1-2002r2'></a> where \((\mathrm {ker}(B))^{\mathrm {c}}\) is the complement of \(\mathrm {ker}(B)\) within \(H^{s-2\alpha }(\Gamma )\) and \(\dot {B}^{-1}: (\mathrm {ker}(B))^{\circ } \rightarrow (\mathrm {ker}(B))^{\mathrm {c}}\) is a bijective map. Even though the above
adjointness and generalized inverse are discussed in the context of Banach spaces, the Sobolev space \(H^{s-2\alpha }(\Gamma )\) is actually
a Hilbert space. Therefore, the complement subspace \((\mathrm {ker}(B))^{\mathrm {c}}\) is also the orthogonal complement subspace
\((\mathrm {ker}(B))^{\perp }\).
</p><!-- l. 54 --><p class='indent'>   The domain and range spaces of \(\dot {B}^{-1}\) are defined as (<a href='#XSteinbachConstruction1998'>Steinbach and Wendland</a>) \begin{equation}  \begin{aligned} V^{s,0}(\Gamma ,B) &amp;:= (\mathrm {ker}(B))^{\circ } = \left \{ v\in H^s(\Gamma ): \langle v,v_p \rangle _{H^{s-\alpha }(\Gamma )} = 0 \right \} \\ V^{s-2\alpha ,0}(\Gamma ,B) &amp;:= (\mathrm {ker}(B))^{\perp } = \left \{ v\in H^{s-2\alpha }(\Gamma ): \langle v,v_p \rangle _{H^{s-2\alpha }(\Gamma )}=0 \right \} \end{aligned} \quad \forall v_p\in \mathrm {ker}(B).  \end{equation}<a id='x1-2003r3'></a>
</p><!-- l. 65 --><p class='indent'>   To simultaneously ensure \(V^s(\Gamma ,A)\)-ellipticity of \(A\) and \(V^{s,0}(\Gamma ,B)\)-ellipticity of \(\dot {B}^{-1}\) which are required by the spectral equivalence
condition, the effective domain for \(A\) and \(\dot {B}^{-1}\) should be the intersection \(V^s(\Gamma ,A) \cap V^{s,0}(\Gamma ,B)\).
                                                                                               
                                                                                               
</p><!-- l. 67 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>2    </span> <a id='x1-30002'></a>Orthogonal decomposition of the domain \(V^s(\Gamma ,A)\) of \(A\)</h3>
<!-- l. 70 --><p class='noindent'>\(V^s(\Gamma ,A)\) is a subspace of \(H^s(\Gamma )\). \(H^s(\Gamma )\) is a Hilbert space and has orthogonal decomposition with respect to its subspace \(V^{s,0}(\Gamma ,B)\): \begin{equation}  H^s(\Gamma )=V^{s,0}(\Gamma ,B) \oplus [ V^{s,0}(\Gamma ,B) ]^{\perp }.  \end{equation}<a id='x1-3001r4'></a> As
defined above, \(V^{s,0}(\Gamma ,B)\) is the annihilator of \(\mathrm {ker}(B)\) in the sense of duality pairing, which can be considered as
a generalization of the concept of orthogonality (based on inner product) in a Hilbert space. Its
orthogonal complement subspace \([ V^{s,0}(\Gamma ,B) ]^{\perp }\) can then be considered as the counterpart of \(\mathrm {ker}(B) \subset H^{s-2\alpha }(\Gamma )\) but in the dual space
\(H^s(\Gamma )\).
</p><!-- l. 76 --><p class='indent'>   For any \(u(x)\in V^s(\Gamma ,A)\), it can be uniquely decomposed as \begin{equation}  u(x) = u_0(x) + u_1(x),  \end{equation}<a id='x1-3002r5'></a> where \(u_0(x)\in V^{s,0}(\Gamma ,B)\) and \(u_1(x)\in [ V^{s,0}(\Gamma ,B) ]^{\perp }\). Obviously, \(u_0(x)\in V^s(\Gamma ,A) \cap V^{s,0}(\Gamma ,B)\). Because \([ V^{s,0}(\Gamma ,B) ]^{\perp }\) is the counterpart of \(\mathrm {ker}(B)\) in \(H^s(\Gamma )\), if we
let \(\{ v_p \}_{p=1}^m\) be the orthonormal basis of \(\mathrm {ker}(B)\), \(u_1(x)\) can be constructed by “projecting” \(u(x)\) to the basis \(\{ v_p \}_{p=1}^m\) in the sense of duality pairing
as below. \begin{equation}  u_1(x)=\sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} (\mathcal {J}^{-2\alpha }v_p)(x).  \end{equation}<a id='x1-3003r6'></a> In this representation, \(\langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}\) is the expansion coefficient and \(\mathcal {J}^{-2\alpha }v_p\) is the basis element for \([ V^{s,0}(\Gamma ,B) ]^{\perp }\) in \(H^s(\Gamma )\), which is the
counterpart of the basis element \(v_p\) for \(\mathrm {ker}(B)\) in \(H^{s-2\alpha }(\Gamma )\). \(\mathcal {J}: \mathcal {S}(\mathbb {R}^d) \rightarrow \mathcal {S}(\mathbb {R}^d)\) is the Bessel potential operator on rapidly decreasing functions. \(\mathcal {J}\) with
an order \(s\) is defined as (<a href='#XSteinbachNumerical2007'>Steinbach</a>, page 32) \begin{equation}  \label {eq:bessel-potential-op} \mathcal {J}^su(x) := ( 2\pi )^{-d/2} \int _{\mathbb {R}^d} ( 1+\lvert \xi \rvert ^2 )^{s/2} \hat {u}(\xi ) \mathrm {e}^{\mathrm {i}\langle x,\xi \rangle } \mathrm {d}\xi ,  \end{equation}<a id='x1-3004r7'></a> where \(\hat {u}(\xi )\) is the Fourier transform of \(u(x)\). Obviously, the effect of \(\mathcal {J}^s\) on \(u(x)\) is
modifying its frequency spectrum by multiplying a polynomial about \(\xi \) with an order \(s\). According to the
pseudodifferential operator theory, \(\mathcal {J}^s\) is equivalent to a partial differential operator with an order \(s\). If
we restrict the domain of \(\mathcal {J}^s\) to a smaller space instead of \(\mathcal {S}(\mathbb {R}^d)\), such as the Sobolev space \(H^t(\Gamma )\), \(\mathcal {J}^s\) maps from \(H^t(\Gamma )\)
to \(H^{t-s}(\Gamma )\), which lowers the Sobolev space order by \(s\). In the above representation for \(u_1(x)\), \(\mathcal {J}^{-2\alpha }\) maps from \(H^{s-2\alpha }(\Gamma )\) to
\(H^s(\Gamma )\).
</p><!-- l. 92 --><p class='indent'>   According to (<a href='#XMcLeanStrongly2000'>McLean</a>, page 75), for any \(s,t\in \mathbb {R}\) and \(u,v\in \mathcal {S}(\mathbb {R}^d)\), Bessel polynomial operators have these properties:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-3006x1'>
     <!-- l. 94 --><p class='noindent'>\(\mathcal {J}^{s+t} = \mathcal {J}^s \mathcal {J}^t\)
     </p></li>
<li class='enumerate' id='x1-3008x2'>
     <!-- l. 95 --><p class='noindent'>\(( \mathcal {J}^s )^{-1} = \mathcal {J}^{-s}\)
     </p></li>
<li class='enumerate' id='x1-3010x3'>
     <!-- l. 96 --><p class='noindent'>\(\mathcal {J}^0 = \text {identity operator}\)
     </p></li>
<li class='enumerate' id='x1-3012x4'>
     <!-- l. 97 --><p class='noindent'>\(\langle \mathcal {J}^s u, v \rangle =\langle u,\mathcal {J}^s v \rangle \)
     </p><!-- l. 99 --><p class='noindent'>\(\langle \cdot ,\cdot \rangle \) can  be  either  considered  as  an  \(L_2\)  inner  product  of  two  functions  in  \(\mathcal {S}(\mathbb {R}^d)\)  or  as  applying  a  tempered
     distribution  in  \(\mathcal {S}'(\mathbb {R}^d)\)  to  a  rapidly  decreasing  function  in  \(\mathcal {S}(\mathbb {R}^d)\).  For  example,  we  can  treat  \(u\)  as  a  tempered
     distribution whose kernel function is \(u\). Then \(\mathcal {J}^s\) in \(\langle \mathcal {J}^s u,v \rangle \) is the Bessel potential operator on \(\mathcal {S}'(\mathbb {R}^d)\) and \(\mathcal {J}^s\) in \(\langle u,\mathcal {J}^s v \rangle \) is the
     Bessel potential operator on \(\mathcal {S}(\mathbb {R})^d\).</p></li></ol>
<!-- l. 102 --><p class='indent'>   The Sobolev space \(H^s(\mathbb {R}^d)\) is defined as (<a href='#XSteinbachNumerical2007'>Steinbach</a>, page 73) \begin{equation}  H^s(\mathbb {R}^d) := \left \{ v\in \mathcal {S}'(\mathbb {R}^d): \mathcal {J}^sv \in L_2(\mathbb {R}^d) \right \}.  \end{equation}<a id='x1-3013r8'></a> The inner product is for any \(u,v\in H^s(\mathbb {R}^d)\), \begin{equation}  \langle u,v \rangle _{H^s(\mathbb {R}^d)} := \langle \mathcal {J}^su, \mathcal {J}^sv \rangle _{L_2(\mathbb {R}^d)}.  \end{equation}<a id='x1-3014r9'></a> The norm definition is
\begin{equation}  \lVert u \rVert _{H^s(\mathbb {R}^d)} := \lVert \mathcal {J}^su \rVert _{L_2(\mathbb {R}^d)}.  \end{equation}<a id='x1-3015r10'></a>
                                                                                               
                                                                                               
</p><!-- l. 117 --><p class='indent'>   From the above, we also have \begin{equation}  \langle u,v \rangle _{H^s(\mathbb {R}^d)} = \langle \mathcal {J}^su, \mathcal {J}^sv \rangle _{L_2(\mathbb {R}^d)} = \langle \mathcal {J}^s \mathcal {J}^s u,v \rangle = \langle \mathcal {J}^{2s}u,v \rangle .  \end{equation}<a id='x1-3016r11'></a>
</p><!-- l. 124 --><p class='indent'>   The above \(\mathcal {J}^{-2\alpha }\) is actually the Riesz map from \(H^{s-2\alpha }(\Gamma )\) to \(H^s(\Gamma )\), which satisfies \begin{equation}  \langle \mathcal {J}^{-2\alpha }u,v \rangle _{H^{s-\alpha }(\Gamma )} = \langle u,v \rangle _{H^{s-2\alpha }(\Gamma )} \quad \forall u,v\in H^{s-2\alpha }(\Gamma ).  \end{equation}<a id='x1-3017r12'></a> This can be proved by using the properties of \(\mathcal {J}\).
The left hand side of the above equation is \begin{equation}  \langle \mathcal {J}^{-2\alpha }u,v \rangle _{H^{s-\alpha }(\Gamma )} = \langle \mathcal {J}^{-\alpha }u, \mathcal {J}^{-\alpha }v \rangle _{H^{s-\alpha }(\Gamma )},  \end{equation}<a id='x1-3018r13'></a> where \(\langle \cdot ,\cdot \rangle _{H^{s-\alpha }(\Gamma )}\) on the left hand side is the duality pairing between \(H^s(\Gamma )\) and \(H^{s-2\alpha }(\Gamma )\),
while on the right hand side it is the inner product in \(H^{s-\alpha }(\Gamma )\). Because both \(\mathcal {J}^{-\alpha }u\) and \(\mathcal {J}^{-\alpha }v\) belong to \(H^{s-\alpha }(\Gamma )\), according to the norm
definition, \begin{equation}  \langle \mathcal {J}^{-\alpha }u, \mathcal {J}^{-\alpha }v \rangle = \langle \mathcal {J}^{s-\alpha }\mathcal {J}^{-\alpha }u, \mathcal {J}^{s-\alpha }\mathcal {J}^{-\alpha }v \rangle _{L_2(\Gamma )} = \langle \mathcal {J}^{s-2\alpha }u,\mathcal {J}^{s-2\alpha }v \rangle _{L_2(\Gamma )}.  \end{equation}<a id='x1-3019r14'></a> This is just the same as \(\langle u,v \rangle _{H^{s-2\alpha }(\Gamma )}\).
</p><!-- l. 142 --><p class='indent'>   We can also show that \(u_0(x)\) is orthogonal to \(u_1(x)\). \begin{equation}  \begin{aligned} \langle u_0,u_1 \rangle _{H^s(\Gamma )} &amp;= \langle u_0, \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \mathcal {J}^{-2\alpha }v_p \rangle _{H^s(\Gamma )} \\ &amp;= \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle u_0,\mathcal {J}^{-2\alpha }v_p \rangle _{H^s(\Gamma )} \\ &amp;= \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle \mathcal {J}^su_0,\mathcal {J}^s \mathcal {J}^{-2\alpha }v_p \rangle _{L_2(\Gamma )} \\ &amp;= \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle \mathcal {J}^su_0,\mathcal {J}^{s-2\alpha }v_p \rangle _{L_2(\Gamma )}, \end{aligned}  \end{equation}<a id='x1-3020r15'></a> where \begin{equation}  \langle \mathcal {J}^su_0,\mathcal {J}^{s-2\alpha }v_p \rangle _{L_2(\Gamma )} = \langle u_0,v_p \rangle _{H^{s-\alpha }(\Gamma )}.  \end{equation}<a id='x1-3021r16'></a> Because \(u_0\in V^{s,0}(\Gamma ,B)\), which is the annihilator of \(\mathrm {ker}(B)\), the above expression is
zero.
</p><!-- l. 162 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>3    </span> <a id='x1-40003'></a>Define the preconditioning bilinear form</h3>
<!-- l. 166 --><p class='noindent'>Now we define the preconditioning bilinear form \(c(u,w)\), which is spectrally equivalent to \(a(u,w)=\langle Au,w \rangle \)
</p>
     <ul class='itemize1'>
     <li class='itemize'>
     <!-- l. 169 --><p class='noindent'>Method 1: Directly construct \(c(u,w)\) based on the generalized inverse \(B^{+}\).
     </p><!-- l. 171 --><p class='noindent'><span class='p1xb-x-x-109'>Basic idea</span>:  use the above orthogonal decomposition and construct the bilinear form \(c(u,u)\),  which is
     spectrally equivalent to \(a(u,u)=\langle Au,u \rangle \).
     </p><!-- l. 173 --><p class='noindent'>Use the boundedness of \(a(\cdot ,\cdot )\) or \(A\), for all \(u\in V^s(\Gamma ,A)\) \begin{equation*}  \langle Au,u \rangle _{H^{s-\alpha }(\Gamma )} \leq c_2^A \lVert u \rVert _{H^s(\Gamma )}^2  \end{equation*} Substitute the orthogonal decomposition \(u=u_0+u_1\) \begin{equation*}  = c_2^A \lVert u_0 + u_1 \rVert _{H^s(\Gamma )}^2  \end{equation*} Because we are dealing with
     Hilbert space, \begin{equation*}  = c_2^A (\langle u_0,u_0 \rangle + \langle u_0,u_1 \rangle + \langle u_1,u_0 \rangle + \langle u_1,u_1 \rangle )  \end{equation*} Because \(u_0\) is orthogonal to \(u_1\), \begin{align*}  &amp;= c_2^A (\lVert u_0 \rVert _{H^s(\Gamma )}^2 + \lVert u_1 \rVert _{H^s(\Gamma )}^2) \\ &amp;= c_2^A \left ( \lVert u_0 \rVert _{H^s(\Gamma )}^2 + \left \langle \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \mathcal {J}^{-2\alpha }v_p, \sum _{q=1}^m \langle u,v_q \rangle _{H^{s-\alpha }(\Gamma )} \mathcal {J}^{-2\alpha }v_q \right \rangle \right ) \\ &amp;= c_2^A \left ( \lVert u_0 \rVert _{H^s(\Gamma )}^2 + \sum _{p=1}^m\sum _{q=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle u,v_q \rangle _{H^{s-\alpha }(\Gamma )} \langle \mathcal {J}^{-2\alpha }v_p, \mathcal {J}^{-2\alpha }v_q \rangle _{H^s(\Gamma )} \right ) \\ &amp;= c_2^A \left ( \lVert u_0 \rVert _{H^s(\Gamma )}^2 + \sum _{p=1}^m\sum _{q=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle u,v_q \rangle _{H^{s-\alpha }(\Gamma )} \langle v_p,v_q \rangle _{H^{s-2\alpha }(\Gamma )} \right )  \end{align*}
     </p><!-- l. 201 --><p class='noindent'>Because \(\{ v_p \}_{p=1}^m\) is an orthonormal basis, \begin{equation*}  = c_2^A \left ( \lVert u_0 \rVert _{H^s(\Gamma )}^2 + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right )  \end{equation*} Use the \(V^{s,0}(\Gamma,B)\)-ellipticity of \(\dot {B}^{-1}\) and the reciprocal relationship
     between its spectrum and that of \(B\), \begin{align*}  &amp;\leq c_2^A \left ( c_2^B \langle \dot {B}^{-1}u_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right ) \\ &amp;\leq c_2^A \max \{ c_2^B,1 \} \left ( \langle \dot {B}^{-1}u_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{k=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right ).  \end{align*}
     </p><!-- l. 214 --><p class='noindent'>If we define the bilinear form \(c(u,w)\) as \begin{equation}  \label {eq:bilinear-form-c} \begin{aligned} c(u,w) &amp;:= \langle B^{\dagger }u,w \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle w,v_p \rangle _{H^{s-\alpha }(\Gamma )} \\ &amp;= \langle \dot {B}^{-1}u_0,w_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle w,v_p \rangle _{H^{s-\alpha }(\Gamma )} \quad \forall u,w\in H^s(\Gamma ) \end{aligned},  \end{equation}<a id='x1-4001r17'></a> it satisfies the upper part of the spectral equivalence condition
     \begin{equation}  \langle Au,u \rangle _{H^{s-\alpha }(\Gamma )} \leq c_2^A \max \{ c_2^{B},1 \} c(u,u).  \end{equation}<a id='x1-4002r18'></a>
     </p><!-- l. 230 --><p class='noindent'>Then we prove the lower part of the spectral equivalence condition. Use the \(V^s(\Gamma,A)\)-ellipticity of \(A\), for all \(u\in V^s(\Gamma ,A)\)
     \begin{equation*}  \langle Au,u \rangle _{H^{s-\alpha }(\Gamma )} \geq c_1^A \lVert u \rVert _{H^s(\Gamma )}^2  \end{equation*} Substitute the orthogonal decomposition \(u=u_0+u_1\) \begin{align*}  &amp;= c_1^A \lVert u_0+u_1 \rVert _{H^s(\Gamma )}^2 \\ &amp;= c_1^A \left ( \lVert u_0 \rVert _{H^s(\Gamma )}^2 + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right )  \end{align*}
     </p><!-- l. 240 --><p class='noindent'>Use the boundedness of \(\dot {B}^{-1}\) and the reciprocal relationship between its spectrum and that of \(B\)
     \begin{align*}  &amp;\geq c_1^A \left ( c_1^B \langle \dot {B}^{-1}u_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right ) \\ &amp;\geq c_1^A \min \{ c_1^B, 1 \} \left ( \langle \dot {B}^{-1}u_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right ).  \end{align*}
     </p><!-- l. 248 --><p class='noindent'>Therefore, we have \begin{equation}  c_1^A \min \{ c_1^B,1 \} c(u,u) \leq \langle Au,u \rangle _{H^{s-\alpha }(\Gamma )}.  \end{equation}<a id='x1-4003r19'></a>
     </p><!-- l. 253 --><p class='noindent'>In these two special cases
          </p><ol class='enumerate1'>
<li class='enumerate' id='x1-4005x1'>
          <!-- l. 255 --><p class='noindent'>when \(V^s(\Gamma ,A) \subseteq V^{s,0}(\Gamma ,B)\), the orthogonal decomposition of \(u\) degenerates to \(u=u_0\),
          </p></li>
<li class='enumerate' id='x1-4007x2'>
          <!-- l. 256 --><p class='noindent'>\(\mathrm {ker}(B) = \{ 0 \}\)</p></li></ol>
                                                                                               
                                                                                               
     <!-- l. 258 --><p class='noindent'>the bilinear form \(c(u,u)\) degenerates to \(\langle \dot {B}^{-1}u,u \rangle \).
     </p></li>
     <li class='itemize'>
     <!-- l. 260 --><p class='noindent'>Method 2: Build an approximate operator \(\tilde {B}\) for \(B\), which is elliptic on the whole domain \(H^{s-2\alpha }(\Gamma )\). Then use its inverse
     to build the bilinear form \(c(u,w) = \langle \tilde {B}^{-1}u,w \rangle _{H^{s-\alpha }(\Gamma )}\).
     </p><!-- l. 262 --><p class='noindent'>For any \(u\in H^{s-2\alpha }(\Gamma )\), it also has an orthogonal decomposition in \(V^{s-2\alpha }(\Gamma ,B) \oplus \mathrm {ker}(B)\) as \begin{equation}  u = u_0 + u_1 = u_0 + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )} v_p(x),  \end{equation}<a id='x1-4008r20'></a> where the inner product \(\langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}\) is the expansion
     coefficient for \(u_1\) and \(v_p\) is the corresponding basis element. Then we try to construct \(\tilde {B}\) which is \(H^{s-2\alpha }(\Gamma )\)-elliptic.
     \begin{align*}  \lVert u \rVert _{H^{s-2\alpha }(\Gamma )}^2 &amp;= \lVert u_0 + u_1 \rVert _{H^{s-2\alpha }(\Gamma )}^2 \\ &amp;= \left ( \lVert u_0 \rVert _{H^{s-2\alpha }(\Gamma )}^2 + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right )  \end{align*}
     </p><!-- l. 273 --><p class='noindent'>Use the \(V^{s-2\alpha}(\Gamma,B)\)-ellipticity of \(B\) \begin{align*}  &amp;\leq \left ( \frac {1}{c_1^B} \langle Bu_0, u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ) \\ &amp;\leq \max \{ \frac {1}{c_1^B},1 \} \left ( \langle Bu_0, u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ).  \end{align*}
     </p><!-- l. 281 --><p class='noindent'>It is equivalent to \begin{equation}  c_1^{\tilde {B}} \lVert u \rVert _{H^{s-2\alpha }(\Gamma )}^2 \leq \left ( \langle Bu_0, u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ),  \end{equation}<a id='x1-4009r21'></a> where \(c_1^{\tilde {B}} = 1/\max \{ \frac {1}{c_1^B},1 \} = \min \{ c_1^B,1 \}\).
     </p><!-- l. 289 --><p class='noindent'>On the other hand, we start from the boundedness of \(B\): \begin{align*}  \lVert u \rVert _{H^{s-2\alpha }(\Gamma )}^2 &amp;= \lVert u_0 + u_1 \rVert _{H^{s-2\alpha }(\Gamma )}^2 \\ &amp;= \left ( \lVert u_0 \rVert _{H^{s-2\alpha }(\Gamma )}^2 + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ) \\ &amp;\geq \left ( \frac {1}{c_2^B} \langle Bu_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_0 \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ) \\ &amp;\geq \min \{ \frac {1}{c_2^B},1 \} \left ( \langle Bu_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_0 \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ).  \end{align*}
     </p><!-- l. 300 --><p class='noindent'>So we have \begin{equation}  \left ( \langle Bu_0, u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ) \leq c_2^{\tilde {B}} \lVert u \rVert _{H^{s-2\alpha }(\Gamma )}^2,  \end{equation}<a id='x1-4010r22'></a> where \(c_2^{\tilde {B}} = \max \{ c_2^B,1 \}\).
     </p><!-- l. 309 --><p class='noindent'>Therefore, we can define the bilinear form related to \(\tilde {B}\) as \begin{equation}  \label {eq:bilinear-form-b} \begin{aligned} \langle \tilde {B}u,w \rangle _{H^{s-\alpha }(\Gamma )} &amp;= \langle Bu,w \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle v,v_p \rangle _{H^{s-2\alpha }(\Gamma )} \langle w,v_p \rangle _{H^{s-2\alpha }(\Gamma )} \\ &amp;= \langle Bu_0,w_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle v,v_p \rangle _{H^{s-2\alpha }(\Gamma )} \langle w,v_p \rangle _{H^{s-2\alpha }(\Gamma )} \quad \forall u,w\in H^{s-2\alpha }(\Gamma ). \end{aligned}  \end{equation}<a id='x1-4011r23'></a> It is both bounded and \(H^{s-2\alpha }(\Gamma )\)-elliptic.
     </p><!-- l. 323 --><p class='noindent'>The differences between the bilinear forms in Equation (<a href='#x1-4001r17'>17<!-- tex4ht:ref: eq:bilinear-form-c  --></a>) and (<a href='#x1-4011r23'>23<!-- tex4ht:ref: eq:bilinear-form-b  --></a>) are:
          </p><ol class='enumerate1'>
<li class='enumerate' id='x1-4013x1'>
          <!-- l. 325 --><p class='noindent'>In Equation (<a href='#x1-4011r23'>23<!-- tex4ht:ref: eq:bilinear-form-b  --></a>), \(u\) and \(w\) are projected to \(v_p\) via inner product, which is a true projection. In Equation
          (<a href='#x1-4001r17'>17<!-- tex4ht:ref: eq:bilinear-form-c  --></a>), \(u\) and \(w\) are in a different space from \(v_p\). They are “projected” to \(v_p\) via duality pairing, which
          can be considered as a generalization of projection.
          </p></li>
<li class='enumerate' id='x1-4015x2'>
          <!-- l. 326 --><p class='noindent'>In Equation (<a href='#x1-4011r23'>23<!-- tex4ht:ref: eq:bilinear-form-b  --></a>), the basis used for function expansion is \(\{ v_p \}_{p=1}^m\). In Equation (<a href='#x1-4001r17'>17<!-- tex4ht:ref: eq:bilinear-form-c  --></a>), the basis is \(\{ \mathcal {J}v_p \}_{p=1}^m\).</p></li></ol>
     </li></ul>
<!-- l. 330 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>4    </span> <a id='x1-50004'></a>Discretization of the preconditioning bilinear form</h3>
<!-- l. 333 --><p class='noindent'>According to (<a href='#XSteinbachConstruction1998'>Steinbach and Wendland</a>, section 4), Method 1 in Section <a href='#x1-40003'>3<!-- tex4ht:ref: sec:precond-bilinear-form  --></a> requires coordinate transformation
between the finite element basis and the orthonormal basis adopted for the orthogonal decomposition \(V_h^s(\Gamma ,A) = V_h^0 \oplus \{ \mathcal {J}v_p \}_{p=1}^m\), which is
more complicated than Method 2. Meanwhile, (<a href='#XSteinbachNumerical2007'>Steinbach</a>, section 13.2 page 299) only introduces Method 2. So
we stick to this method.
</p><!-- l. 335 --><p class='indent'>   Usually, the inverse operator \(\tilde {B}^{-1}\) cannot be directly evaluated or discretized. We need to compute the discretized
matrix of \(\tilde {B}\) first, then use its inverse matrix to compute an approximation to the discretized matrix for the inverse
operator \(\tilde {B}^{-1}\). This can be visualized in the following diagram. </p>
<div class='center'>
                                                                                               
                                                                                               
<!-- l. 336 --><p class='noindent'>
</p><!-- l. 337 --><p class='noindent'><img alt='PIC'  src='/figures/2024-11-16_11-54-40-approximation-of-inverse-operator.png'  /></p></div>
<!-- l. 340 --><p class='indent'>   Therefore, we have \begin{equation}  \underline {\tilde {B}^{-1}} \approx \mathcal {M}_{\tilde {B}}^{\mathrm {T}} \tilde {\mathcal {B}}^{-1} \mathcal {M}_{\tilde {B}}.  \end{equation}<a id='x1-5001r24'></a> The matrix to be multiplied to both sides of the equation is \begin{equation}  \mathcal {M}_{\tilde {B}}^{-1} \tilde {\mathcal {B}} \mathcal {M}_{\tilde {B}}^{-\mathrm {T}}.  \end{equation}<a id='x1-5002r25'></a>
</p><!-- l. 350 --><p class='indent'>   The spectral equivalence between the above two matrices \(\underline {\tilde {B}^{-1}}\) and \(\mathcal {M}_{\tilde {B}}^{\mathrm {T}} \tilde {\mathcal {B}}^{-1} \mathcal {M}_{\tilde {B}}\) can be proved using the theory given in
(<a href='#XSteinbachConstruction1998'>Steinbach and Wendland</a>, section 3). This theory is described in a general form as below.
</p><!-- l. 352 --><p class='indent'>   \(V\) and \(W\) are two Hilbert spaces. \(A: V \rightarrow V'\) is a \(V\)-elliptic, self-adjoint and bounded operator, while \(B: W \rightarrow V'\) is any bounded operator.
Define an operator \(T: W \rightarrow W'\) as \(T = B' A^{-1} B\). In principle, it has a Galerkin matrix \(\mathcal {T}\). In reality, this matrix cannot be directly computed.
The Galerkin matrices for \(A\) and \(B\) are \(\mathcal {A}\) and \(\mathcal {B}\) respectively. Then we have another matrix \begin{equation}  \tilde {\mathcal {T}} = \mathcal {B}^{\mathrm {T}} \mathcal {A}^{-1} \mathcal {B}.  \end{equation}<a id='x1-5003r26'></a> It can be proved that for any
\(w_h\in W_h\), \begin{equation}  ( \tilde {\mathcal {T}} \underline {w}, \underline {w} ) \leq ( \mathcal {T} \underline {w}, \underline {w} ).  \end{equation}<a id='x1-5004r27'></a> If the stability condition or \(\inf \sup \) condition is satisfied \begin{equation}  \inf _{0\neq w_h\in W_h} \sup _{0\neq v_h\in V_h} \frac {\lvert \langle Bw_h,v_h \rangle _{V} \rvert }{\lVert w_h \rVert _W \lVert v_h \rVert _V} \geq c,  \end{equation}<a id='x1-5005r28'></a> we also have \begin{equation}  \gamma ( \mathcal {T} \underline {w}, \underline {w} ) \leq ( \tilde {\mathcal {T}} \underline {w}, \underline {w} ).  \end{equation}<a id='x1-5006r29'></a> Therefore, \(\mathcal {T}\) and \(\tilde {\mathcal {T}}\) are spectrally
equivalent.
</p><!-- l. 372 --><p class='indent'>   The relationship between the spaces, operators and matrices are shown below. </p>
<div class='center'>
<!-- l. 373 --><p class='noindent'>
</p><!-- l. 374 --><p class='noindent'><img alt='PIC' src='/figures/IMG_1857.JPG' /></p></div>
                                                                                               
                                                                                               
<!-- l. 377 --><p class='indent'>   Coming back to the spectral equivalence between the two matrices \(\underline {\tilde {B}^{-1}}\) and \(\mathcal {M}_{\tilde {B}}^{\mathrm {T}} \tilde {\mathcal {B}}^{-1} \mathcal {M}_{\tilde {B}}\), we can see that it
is the special case when \(W\) is selected to be \(V'\) and \(B\) is the identity operator on \(V'\). The mass matrix \(M_A\) for
the duality pairing from \(V'\) to \(V\) is just the Galerkin matrix of \(B\). This is shown in the following figure.
</p>
<div class='center'>
<!-- l. 378 --><p class='noindent'>
</p><!-- l. 379 --><p class='noindent'><img alt='PIC' src='/figures/IMG_1858.JPG' /></p></div>
<!-- l. 382 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>5    </span> <a id='x1-60005'></a>Preconditioners for Laplace problem</h3>
<!-- l. 384 --><p class='noindent'>The operator equation for the Laplace problem with Dirichlet boundary condition is \begin{equation}  \langle Vt, \tau \rangle _{\Gamma } = \langle (\frac {1}{2}I + K)g, \tau \rangle _{\Gamma } \quad \forall \tau \in H^{-1/2}(\Gamma ).  \end{equation}<a id='x1-6001r30'></a> The hypersingular operator
\(D\) has an opposite order with respect to \(V\), which is naturally the preconditioner for this equation.
                                                                                               
                                                                                               
However, according to <a href='/math/2024/11/11/ellipticity-of-boundary-integral-operators.html#x1-50004'>here</a>, \(D\) is not \(H^{1/2}(\Gamma )\)-elliptic, but only \(H_{\ast }^{1/2}(\Gamma )\)-elliptic or \(H_{\ast \ast }^{1/2}(\Gamma )\)-elliptic depending on which kind
of inner product is desired. To avoid the complex of computing the natural density \(w_{\mathrm {eq}}\), we choose
\(H_{\ast \ast }^{1/2}(\Gamma )\)-ellipticity. Then the bilinear form associated with the regularized or gauged operator \(\tilde {D}\) is \begin{equation}  \langle \tilde {D}u,w \rangle = \langle Du,w \rangle + \langle u,1 \rangle _{\Gamma } \langle w,1 \rangle _{\Gamma } \quad \forall u,w\in H^{1/2}(\Gamma ).  \end{equation}<a id='x1-6002r31'></a> Then the
inverse of the preconditioning matrix is \begin{equation}  \mathcal {M}_{\tilde {D}}^{-1} \tilde {\mathcal {D}} \mathcal {M}_{\tilde {D}}^{-\mathrm {T}},  \end{equation}<a id='x1-6003r32'></a> where \(\mathcal {M}_{\tilde {D}}\) is the mass matrix for the duality pairing from \(H^{-1/2}(\Gamma )\) to
\(H^{1/2}(\Gamma )\).
</p><!-- l. 398 --><p class='indent'>   The operator equation for the Laplace problem with Neumann boundary condition is \begin{equation}  \langle Du, v \rangle _{\Gamma } + \alpha \langle u,1 \rangle _{\Gamma } \langle v,1 \rangle _{\Gamma } = \langle (\frac {1}{2}I - K')g,v \rangle _{\Gamma } \quad \forall v\in H^{1/2}(\Gamma ).  \end{equation}<a id='x1-6004r33'></a> The single layer
potential operator \(V\) has an opposite order with respect to \(D\), which is the preconditioner. When \(d=3\), \(V\) is \(H^{-1/2}(\Gamma )\)-elliptic, so we
directly obtain the inverse of the preconditioning matrix \begin{equation}  \mathcal {M}_V^{-1} \mathcal {V} \mathcal {M}_V^{-\mathrm {T}},  \end{equation}<a id='x1-6005r34'></a> where \(\mathcal {M}_V\) is the mass matrix for the duality pairing from \(H^{1/2}(\Gamma )\)
to \(H^{-1/2}(\Gamma )\).
</p><!-- l. 1 --><p class='noindent'>
</p>
   <h3 class='likesectionHead'><a id='x1-7000'></a>References</h3>
<!-- l. 1 --><p class='noindent'>
  </p><div class='thebibliography'>
  <p class='bibitem'><span class='biblabel'>
<a id='XMcLeanStrongly2000'></a><span class='bibsp'>   </span></span>William Charles Hector McLean. <span class='p1xi-x-x-109'>Strongly Elliptic Systems and Boundary Integral Equations</span>. Cambridge
  University Press. ISBN 978-0-521-66375-5.
  </p>
  <p class='bibitem'><span class='biblabel'>
<a id='XSteinbachNumerical2007'></a><span class='bibsp'>   </span></span>Olaf  Steinbach.    <span class='p1xi-x-x-109'>Numerical  Approximation  Methods  for  Elliptic  Boundary  Value  Problems:  Finite  and
  </span><span class='p1xi-x-x-109'>Boundary Elements</span>. Springer Science &amp; Business Media. ISBN 978-0-387-31312-2.
  </p>
  <p class='bibitem'><span class='biblabel'>
<a id='XSteinbachConstruction1998'></a><span class='bibsp'>   </span></span>Olaf               Steinbach               and               Wolfgang L.               Wendland.                                      The
  construction of some efficient preconditioners in the boundary element method.  9(1-2):191–216.  URL
  <a class='url' href='http://link.springer.com/article/10.1023/A:1018937506719'><span class='t1xtt-x-x-109'>http://link.springer.com/article/10.1023/A:1018937506719</span></a>.
</p>
  </div>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="PDE" /><category term="BEM" /><summary type="html"><![CDATA[Contents  1 Inverse operator of the preconditioner  2 Orthogonal decomposition of the domain \(V^s(\Gamma ,A)\) of \(A\)  3 Define the preconditioning bilinear form  4 Discretization of the preconditioning bilinear form  5 Preconditioners for Laplace problem In BEM, the preconditioning technique is indispensable for solving a large scale PDE, when an iterative solver, such as CG, GMRES, MinRES, BiCGStab, is adopted. Without a preconditioner, the number of iterations will significantly increase with the deterioration of the system matrix’s condition number. A large condition number can be caused by a large number of DoFs, distinct mesh element size, sharp corners in the geometry, etc. The procedure for constructing the Galerkin matrix for a preconditioner is summarized into four steps. Find a preconditioning operator having an opposite order with respect to the key operator in the original PDE in the theoretical framework of pseudodifferential operators (see Pseudodifferential operator, Operator preconditioning based on pseudodifferential operator of opposite orders). In BEM, this is a trivial task thanks to the properties of boundary integral operators (see Boundary integral operators considered as pseudodifferential operators). Construct an inverse operator for the preconditioner obtained from the previous step. If the preconditioner is not elliptic on its whole domain, we should construct a Moore-Penrose pseudo inverse or generalized inverse operator instead. Alternatively, we can build an approximate operator for the preconditioner, which is elliptic on the whole domain. Then its inverse operator is available. Construct a bilinear form for the above inverse operator, which is spectrally equivalent to the bilinear form in the variational form of the PDE. The lower bound and upper bound constants in this spectral equivalence condition do not depend on the geometric discretization and finite element (function space) discretization. Discretize the above bilinear form, whose inverse matrix is to be multiplied to the discretized linear system for the variational form of the PDE. Since the inverse operator associated with this bilinear form cannot be directly evaluated, we will compute an approximate version which is based on the inverse matrix of the Galerkin matrix for the preconditioning operator. A general matrix spectral equivalence theorem guarantees the equivalence between this approximate version and the exact version. In the following, we’ll introduce the basic ideas and theoretical details. Finally, we present the preconditioners used in the Laplace problem with either Dirichlet or Neumann boundary condition. 1 Inverse operator of the preconditioner The operator \(A: V^s(\Gamma ,A):= H^s(\Gamma )_{/\mathrm {ker}(A)} \rightarrow H^{s-2\alpha }(\Gamma )\) in the original PDE is self-adjoint (in the sense of adjointness in Banach spaces), bounded and \(V^s(\Gamma ,A)\)-elliptic. As a pseudodifferential operator, its order is \(2\alpha \). The preconditioning operator \(B: H^{s-2\alpha }(\Gamma ) \rightarrow H^s(\Gamma )\) is self-adjoint, bounded and \(V^{s-2\alpha ,0}(\Gamma ,B) := H^{s-2\alpha }(\Gamma )_{/\mathrm {ker}(B)}\)-elliptic. Its order is \(-2\alpha \). Define the inverse operator \(B^{-1}: H^s(\Gamma ) \rightarrow H^{s-2\alpha }(\Gamma )\) of \(B\) which should be spectrally equivalent to \(A\). Assume \(B\) has a closed range and also note \(B\) is self-adjoint, then \begin{equation} \mathrm {Im}(B) = (\mathrm {ker}(B'))^{\circ } = (\mathrm {ker}(B))^{\circ }. \end{equation} If the operator \(B\) has a non-trivial kernel and let \(H^s(\Gamma )\) be decomposed as \(H^s(\Gamma ) = (\mathrm {ker}(B))^{\circ } \oplus [(\mathrm {ker}(B))^{\circ }]^{\mathrm {c}}\), a generalized inverse \(B^{+}: H^s(\Gamma ) \rightarrow H^{s-2\alpha }(\Gamma )\) can be defined \begin{equation} B^{+} = \begin {cases} \dot {B}^{-1}(y) &amp; y\in (\mathrm {ker}(B))^{\circ } \\ 0 &amp; y\in [(\mathrm {ker}(B))^{\circ }]^{\mathrm {c}} \end {cases}, \end{equation} where \((\mathrm {ker}(B))^{\mathrm {c}}\) is the complement of \(\mathrm {ker}(B)\) within \(H^{s-2\alpha }(\Gamma )\) and \(\dot {B}^{-1}: (\mathrm {ker}(B))^{\circ } \rightarrow (\mathrm {ker}(B))^{\mathrm {c}}\) is a bijective map. Even though the above adjointness and generalized inverse are discussed in the context of Banach spaces, the Sobolev space \(H^{s-2\alpha }(\Gamma )\) is actually a Hilbert space. Therefore, the complement subspace \((\mathrm {ker}(B))^{\mathrm {c}}\) is also the orthogonal complement subspace \((\mathrm {ker}(B))^{\perp }\). The domain and range spaces of \(\dot {B}^{-1}\) are defined as (Steinbach and Wendland) \begin{equation} \begin{aligned} V^{s,0}(\Gamma ,B) &amp;:= (\mathrm {ker}(B))^{\circ } = \left \{ v\in H^s(\Gamma ): \langle v,v_p \rangle _{H^{s-\alpha }(\Gamma )} = 0 \right \} \\ V^{s-2\alpha ,0}(\Gamma ,B) &amp;:= (\mathrm {ker}(B))^{\perp } = \left \{ v\in H^{s-2\alpha }(\Gamma ): \langle v,v_p \rangle _{H^{s-2\alpha }(\Gamma )}=0 \right \} \end{aligned} \quad \forall v_p\in \mathrm {ker}(B). \end{equation} To simultaneously ensure \(V^s(\Gamma ,A)\)-ellipticity of \(A\) and \(V^{s,0}(\Gamma ,B)\)-ellipticity of \(\dot {B}^{-1}\) which are required by the spectral equivalence condition, the effective domain for \(A\) and \(\dot {B}^{-1}\) should be the intersection \(V^s(\Gamma ,A) \cap V^{s,0}(\Gamma ,B)\). 2 Orthogonal decomposition of the domain \(V^s(\Gamma ,A)\) of \(A\) \(V^s(\Gamma ,A)\) is a subspace of \(H^s(\Gamma )\). \(H^s(\Gamma )\) is a Hilbert space and has orthogonal decomposition with respect to its subspace \(V^{s,0}(\Gamma ,B)\): \begin{equation} H^s(\Gamma )=V^{s,0}(\Gamma ,B) \oplus [ V^{s,0}(\Gamma ,B) ]^{\perp }. \end{equation} As defined above, \(V^{s,0}(\Gamma ,B)\) is the annihilator of \(\mathrm {ker}(B)\) in the sense of duality pairing, which can be considered as a generalization of the concept of orthogonality (based on inner product) in a Hilbert space. Its orthogonal complement subspace \([ V^{s,0}(\Gamma ,B) ]^{\perp }\) can then be considered as the counterpart of \(\mathrm {ker}(B) \subset H^{s-2\alpha }(\Gamma )\) but in the dual space \(H^s(\Gamma )\). For any \(u(x)\in V^s(\Gamma ,A)\), it can be uniquely decomposed as \begin{equation} u(x) = u_0(x) + u_1(x), \end{equation} where \(u_0(x)\in V^{s,0}(\Gamma ,B)\) and \(u_1(x)\in [ V^{s,0}(\Gamma ,B) ]^{\perp }\). Obviously, \(u_0(x)\in V^s(\Gamma ,A) \cap V^{s,0}(\Gamma ,B)\). Because \([ V^{s,0}(\Gamma ,B) ]^{\perp }\) is the counterpart of \(\mathrm {ker}(B)\) in \(H^s(\Gamma )\), if we let \(\{ v_p \}_{p=1}^m\) be the orthonormal basis of \(\mathrm {ker}(B)\), \(u_1(x)\) can be constructed by “projecting” \(u(x)\) to the basis \(\{ v_p \}_{p=1}^m\) in the sense of duality pairing as below. \begin{equation} u_1(x)=\sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} (\mathcal {J}^{-2\alpha }v_p)(x). \end{equation} In this representation, \(\langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}\) is the expansion coefficient and \(\mathcal {J}^{-2\alpha }v_p\) is the basis element for \([ V^{s,0}(\Gamma ,B) ]^{\perp }\) in \(H^s(\Gamma )\), which is the counterpart of the basis element \(v_p\) for \(\mathrm {ker}(B)\) in \(H^{s-2\alpha }(\Gamma )\). \(\mathcal {J}: \mathcal {S}(\mathbb {R}^d) \rightarrow \mathcal {S}(\mathbb {R}^d)\) is the Bessel potential operator on rapidly decreasing functions. \(\mathcal {J}\) with an order \(s\) is defined as (Steinbach, page 32) \begin{equation} \label {eq:bessel-potential-op} \mathcal {J}^su(x) := ( 2\pi )^{-d/2} \int _{\mathbb {R}^d} ( 1+\lvert \xi \rvert ^2 )^{s/2} \hat {u}(\xi ) \mathrm {e}^{\mathrm {i}\langle x,\xi \rangle } \mathrm {d}\xi , \end{equation} where \(\hat {u}(\xi )\) is the Fourier transform of \(u(x)\). Obviously, the effect of \(\mathcal {J}^s\) on \(u(x)\) is modifying its frequency spectrum by multiplying a polynomial about \(\xi \) with an order \(s\). According to the pseudodifferential operator theory, \(\mathcal {J}^s\) is equivalent to a partial differential operator with an order \(s\). If we restrict the domain of \(\mathcal {J}^s\) to a smaller space instead of \(\mathcal {S}(\mathbb {R}^d)\), such as the Sobolev space \(H^t(\Gamma )\), \(\mathcal {J}^s\) maps from \(H^t(\Gamma )\) to \(H^{t-s}(\Gamma )\), which lowers the Sobolev space order by \(s\). In the above representation for \(u_1(x)\), \(\mathcal {J}^{-2\alpha }\) maps from \(H^{s-2\alpha }(\Gamma )\) to \(H^s(\Gamma )\). According to (McLean, page 75), for any \(s,t\in \mathbb {R}\) and \(u,v\in \mathcal {S}(\mathbb {R}^d)\), Bessel polynomial operators have these properties: \(\mathcal {J}^{s+t} = \mathcal {J}^s \mathcal {J}^t\) \(( \mathcal {J}^s )^{-1} = \mathcal {J}^{-s}\) \(\mathcal {J}^0 = \text {identity operator}\) \(\langle \mathcal {J}^s u, v \rangle =\langle u,\mathcal {J}^s v \rangle \) \(\langle \cdot ,\cdot \rangle \) can be either considered as an \(L_2\) inner product of two functions in \(\mathcal {S}(\mathbb {R}^d)\) or as applying a tempered distribution in \(\mathcal {S}'(\mathbb {R}^d)\) to a rapidly decreasing function in \(\mathcal {S}(\mathbb {R}^d)\). For example, we can treat \(u\) as a tempered distribution whose kernel function is \(u\). Then \(\mathcal {J}^s\) in \(\langle \mathcal {J}^s u,v \rangle \) is the Bessel potential operator on \(\mathcal {S}'(\mathbb {R}^d)\) and \(\mathcal {J}^s\) in \(\langle u,\mathcal {J}^s v \rangle \) is the Bessel potential operator on \(\mathcal {S}(\mathbb {R})^d\). The Sobolev space \(H^s(\mathbb {R}^d)\) is defined as (Steinbach, page 73) \begin{equation} H^s(\mathbb {R}^d) := \left \{ v\in \mathcal {S}'(\mathbb {R}^d): \mathcal {J}^sv \in L_2(\mathbb {R}^d) \right \}. \end{equation} The inner product is for any \(u,v\in H^s(\mathbb {R}^d)\), \begin{equation} \langle u,v \rangle _{H^s(\mathbb {R}^d)} := \langle \mathcal {J}^su, \mathcal {J}^sv \rangle _{L_2(\mathbb {R}^d)}. \end{equation} The norm definition is \begin{equation} \lVert u \rVert _{H^s(\mathbb {R}^d)} := \lVert \mathcal {J}^su \rVert _{L_2(\mathbb {R}^d)}. \end{equation} From the above, we also have \begin{equation} \langle u,v \rangle _{H^s(\mathbb {R}^d)} = \langle \mathcal {J}^su, \mathcal {J}^sv \rangle _{L_2(\mathbb {R}^d)} = \langle \mathcal {J}^s \mathcal {J}^s u,v \rangle = \langle \mathcal {J}^{2s}u,v \rangle . \end{equation} The above \(\mathcal {J}^{-2\alpha }\) is actually the Riesz map from \(H^{s-2\alpha }(\Gamma )\) to \(H^s(\Gamma )\), which satisfies \begin{equation} \langle \mathcal {J}^{-2\alpha }u,v \rangle _{H^{s-\alpha }(\Gamma )} = \langle u,v \rangle _{H^{s-2\alpha }(\Gamma )} \quad \forall u,v\in H^{s-2\alpha }(\Gamma ). \end{equation} This can be proved by using the properties of \(\mathcal {J}\). The left hand side of the above equation is \begin{equation} \langle \mathcal {J}^{-2\alpha }u,v \rangle _{H^{s-\alpha }(\Gamma )} = \langle \mathcal {J}^{-\alpha }u, \mathcal {J}^{-\alpha }v \rangle _{H^{s-\alpha }(\Gamma )}, \end{equation} where \(\langle \cdot ,\cdot \rangle _{H^{s-\alpha }(\Gamma )}\) on the left hand side is the duality pairing between \(H^s(\Gamma )\) and \(H^{s-2\alpha }(\Gamma )\), while on the right hand side it is the inner product in \(H^{s-\alpha }(\Gamma )\). Because both \(\mathcal {J}^{-\alpha }u\) and \(\mathcal {J}^{-\alpha }v\) belong to \(H^{s-\alpha }(\Gamma )\), according to the norm definition, \begin{equation} \langle \mathcal {J}^{-\alpha }u, \mathcal {J}^{-\alpha }v \rangle = \langle \mathcal {J}^{s-\alpha }\mathcal {J}^{-\alpha }u, \mathcal {J}^{s-\alpha }\mathcal {J}^{-\alpha }v \rangle _{L_2(\Gamma )} = \langle \mathcal {J}^{s-2\alpha }u,\mathcal {J}^{s-2\alpha }v \rangle _{L_2(\Gamma )}. \end{equation} This is just the same as \(\langle u,v \rangle _{H^{s-2\alpha }(\Gamma )}\). We can also show that \(u_0(x)\) is orthogonal to \(u_1(x)\). \begin{equation} \begin{aligned} \langle u_0,u_1 \rangle _{H^s(\Gamma )} &amp;= \langle u_0, \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \mathcal {J}^{-2\alpha }v_p \rangle _{H^s(\Gamma )} \\ &amp;= \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle u_0,\mathcal {J}^{-2\alpha }v_p \rangle _{H^s(\Gamma )} \\ &amp;= \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle \mathcal {J}^su_0,\mathcal {J}^s \mathcal {J}^{-2\alpha }v_p \rangle _{L_2(\Gamma )} \\ &amp;= \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle \mathcal {J}^su_0,\mathcal {J}^{s-2\alpha }v_p \rangle _{L_2(\Gamma )}, \end{aligned} \end{equation} where \begin{equation} \langle \mathcal {J}^su_0,\mathcal {J}^{s-2\alpha }v_p \rangle _{L_2(\Gamma )} = \langle u_0,v_p \rangle _{H^{s-\alpha }(\Gamma )}. \end{equation} Because \(u_0\in V^{s,0}(\Gamma ,B)\), which is the annihilator of \(\mathrm {ker}(B)\), the above expression is zero. 3 Define the preconditioning bilinear form Now we define the preconditioning bilinear form \(c(u,w)\), which is spectrally equivalent to \(a(u,w)=\langle Au,w \rangle \) Method 1: Directly construct \(c(u,w)\) based on the generalized inverse \(B^{+}\). Basic idea: use the above orthogonal decomposition and construct the bilinear form \(c(u,u)\), which is spectrally equivalent to \(a(u,u)=\langle Au,u \rangle \). Use the boundedness of \(a(\cdot ,\cdot )\) or \(A\), for all \(u\in V^s(\Gamma ,A)\) \begin{equation*} \langle Au,u \rangle _{H^{s-\alpha }(\Gamma )} \leq c_2^A \lVert u \rVert _{H^s(\Gamma )}^2 \end{equation*} Substitute the orthogonal decomposition \(u=u_0+u_1\) \begin{equation*} = c_2^A \lVert u_0 + u_1 \rVert _{H^s(\Gamma )}^2 \end{equation*} Because we are dealing with Hilbert space, \begin{equation*} = c_2^A (\langle u_0,u_0 \rangle + \langle u_0,u_1 \rangle + \langle u_1,u_0 \rangle + \langle u_1,u_1 \rangle ) \end{equation*} Because \(u_0\) is orthogonal to \(u_1\), \begin{align*} &amp;= c_2^A (\lVert u_0 \rVert _{H^s(\Gamma )}^2 + \lVert u_1 \rVert _{H^s(\Gamma )}^2) \\ &amp;= c_2^A \left ( \lVert u_0 \rVert _{H^s(\Gamma )}^2 + \left \langle \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \mathcal {J}^{-2\alpha }v_p, \sum _{q=1}^m \langle u,v_q \rangle _{H^{s-\alpha }(\Gamma )} \mathcal {J}^{-2\alpha }v_q \right \rangle \right ) \\ &amp;= c_2^A \left ( \lVert u_0 \rVert _{H^s(\Gamma )}^2 + \sum _{p=1}^m\sum _{q=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle u,v_q \rangle _{H^{s-\alpha }(\Gamma )} \langle \mathcal {J}^{-2\alpha }v_p, \mathcal {J}^{-2\alpha }v_q \rangle _{H^s(\Gamma )} \right ) \\ &amp;= c_2^A \left ( \lVert u_0 \rVert _{H^s(\Gamma )}^2 + \sum _{p=1}^m\sum _{q=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle u,v_q \rangle _{H^{s-\alpha }(\Gamma )} \langle v_p,v_q \rangle _{H^{s-2\alpha }(\Gamma )} \right ) \end{align*} Because \(\{ v_p \}_{p=1}^m\) is an orthonormal basis, \begin{equation*} = c_2^A \left ( \lVert u_0 \rVert _{H^s(\Gamma )}^2 + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right ) \end{equation*} Use the \(V^{s,0}(\Gamma,B)\)-ellipticity of \(\dot {B}^{-1}\) and the reciprocal relationship between its spectrum and that of \(B\), \begin{align*} &amp;\leq c_2^A \left ( c_2^B \langle \dot {B}^{-1}u_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right ) \\ &amp;\leq c_2^A \max \{ c_2^B,1 \} \left ( \langle \dot {B}^{-1}u_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{k=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right ). \end{align*} If we define the bilinear form \(c(u,w)\) as \begin{equation} \label {eq:bilinear-form-c} \begin{aligned} c(u,w) &amp;:= \langle B^{\dagger }u,w \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle w,v_p \rangle _{H^{s-\alpha }(\Gamma )} \\ &amp;= \langle \dot {B}^{-1}u_0,w_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )} \langle w,v_p \rangle _{H^{s-\alpha }(\Gamma )} \quad \forall u,w\in H^s(\Gamma ) \end{aligned}, \end{equation} it satisfies the upper part of the spectral equivalence condition \begin{equation} \langle Au,u \rangle _{H^{s-\alpha }(\Gamma )} \leq c_2^A \max \{ c_2^{B},1 \} c(u,u). \end{equation} Then we prove the lower part of the spectral equivalence condition. Use the \(V^s(\Gamma,A)\)-ellipticity of \(A\), for all \(u\in V^s(\Gamma ,A)\) \begin{equation*} \langle Au,u \rangle _{H^{s-\alpha }(\Gamma )} \geq c_1^A \lVert u \rVert _{H^s(\Gamma )}^2 \end{equation*} Substitute the orthogonal decomposition \(u=u_0+u_1\) \begin{align*} &amp;= c_1^A \lVert u_0+u_1 \rVert _{H^s(\Gamma )}^2 \\ &amp;= c_1^A \left ( \lVert u_0 \rVert _{H^s(\Gamma )}^2 + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right ) \end{align*} Use the boundedness of \(\dot {B}^{-1}\) and the reciprocal relationship between its spectrum and that of \(B\) \begin{align*} &amp;\geq c_1^A \left ( c_1^B \langle \dot {B}^{-1}u_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right ) \\ &amp;\geq c_1^A \min \{ c_1^B, 1 \} \left ( \langle \dot {B}^{-1}u_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-\alpha }(\Gamma )}^2 \right ). \end{align*} Therefore, we have \begin{equation} c_1^A \min \{ c_1^B,1 \} c(u,u) \leq \langle Au,u \rangle _{H^{s-\alpha }(\Gamma )}. \end{equation} In these two special cases when \(V^s(\Gamma ,A) \subseteq V^{s,0}(\Gamma ,B)\), the orthogonal decomposition of \(u\) degenerates to \(u=u_0\), \(\mathrm {ker}(B) = \{ 0 \}\) the bilinear form \(c(u,u)\) degenerates to \(\langle \dot {B}^{-1}u,u \rangle \). Method 2: Build an approximate operator \(\tilde {B}\) for \(B\), which is elliptic on the whole domain \(H^{s-2\alpha }(\Gamma )\). Then use its inverse to build the bilinear form \(c(u,w) = \langle \tilde {B}^{-1}u,w \rangle _{H^{s-\alpha }(\Gamma )}\). For any \(u\in H^{s-2\alpha }(\Gamma )\), it also has an orthogonal decomposition in \(V^{s-2\alpha }(\Gamma ,B) \oplus \mathrm {ker}(B)\) as \begin{equation} u = u_0 + u_1 = u_0 + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )} v_p(x), \end{equation} where the inner product \(\langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}\) is the expansion coefficient for \(u_1\) and \(v_p\) is the corresponding basis element. Then we try to construct \(\tilde {B}\) which is \(H^{s-2\alpha }(\Gamma )\)-elliptic. \begin{align*} \lVert u \rVert _{H^{s-2\alpha }(\Gamma )}^2 &amp;= \lVert u_0 + u_1 \rVert _{H^{s-2\alpha }(\Gamma )}^2 \\ &amp;= \left ( \lVert u_0 \rVert _{H^{s-2\alpha }(\Gamma )}^2 + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ) \end{align*} Use the \(V^{s-2\alpha}(\Gamma,B)\)-ellipticity of \(B\) \begin{align*} &amp;\leq \left ( \frac {1}{c_1^B} \langle Bu_0, u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ) \\ &amp;\leq \max \{ \frac {1}{c_1^B},1 \} \left ( \langle Bu_0, u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ). \end{align*} It is equivalent to \begin{equation} c_1^{\tilde {B}} \lVert u \rVert _{H^{s-2\alpha }(\Gamma )}^2 \leq \left ( \langle Bu_0, u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ), \end{equation} where \(c_1^{\tilde {B}} = 1/\max \{ \frac {1}{c_1^B},1 \} = \min \{ c_1^B,1 \}\). On the other hand, we start from the boundedness of \(B\): \begin{align*} \lVert u \rVert _{H^{s-2\alpha }(\Gamma )}^2 &amp;= \lVert u_0 + u_1 \rVert _{H^{s-2\alpha }(\Gamma )}^2 \\ &amp;= \left ( \lVert u_0 \rVert _{H^{s-2\alpha }(\Gamma )}^2 + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ) \\ &amp;\geq \left ( \frac {1}{c_2^B} \langle Bu_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_0 \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ) \\ &amp;\geq \min \{ \frac {1}{c_2^B},1 \} \left ( \langle Bu_0,u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_0 \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ). \end{align*} So we have \begin{equation} \left ( \langle Bu_0, u_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle u,v_p \rangle _{H^{s-2\alpha }(\Gamma )}^2 \right ) \leq c_2^{\tilde {B}} \lVert u \rVert _{H^{s-2\alpha }(\Gamma )}^2, \end{equation} where \(c_2^{\tilde {B}} = \max \{ c_2^B,1 \}\). Therefore, we can define the bilinear form related to \(\tilde {B}\) as \begin{equation} \label {eq:bilinear-form-b} \begin{aligned} \langle \tilde {B}u,w \rangle _{H^{s-\alpha }(\Gamma )} &amp;= \langle Bu,w \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle v,v_p \rangle _{H^{s-2\alpha }(\Gamma )} \langle w,v_p \rangle _{H^{s-2\alpha }(\Gamma )} \\ &amp;= \langle Bu_0,w_0 \rangle _{H^{s-\alpha }(\Gamma )} + \sum _{p=1}^m \langle v,v_p \rangle _{H^{s-2\alpha }(\Gamma )} \langle w,v_p \rangle _{H^{s-2\alpha }(\Gamma )} \quad \forall u,w\in H^{s-2\alpha }(\Gamma ). \end{aligned} \end{equation} It is both bounded and \(H^{s-2\alpha }(\Gamma )\)-elliptic. The differences between the bilinear forms in Equation (17) and (23) are: In Equation (23), \(u\) and \(w\) are projected to \(v_p\) via inner product, which is a true projection. In Equation (17), \(u\) and \(w\) are in a different space from \(v_p\). They are “projected” to \(v_p\) via duality pairing, which can be considered as a generalization of projection. In Equation (23), the basis used for function expansion is \(\{ v_p \}_{p=1}^m\). In Equation (17), the basis is \(\{ \mathcal {J}v_p \}_{p=1}^m\). 4 Discretization of the preconditioning bilinear form According to (Steinbach and Wendland, section 4), Method 1 in Section 3 requires coordinate transformation between the finite element basis and the orthonormal basis adopted for the orthogonal decomposition \(V_h^s(\Gamma ,A) = V_h^0 \oplus \{ \mathcal {J}v_p \}_{p=1}^m\), which is more complicated than Method 2. Meanwhile, (Steinbach, section 13.2 page 299) only introduces Method 2. So we stick to this method. Usually, the inverse operator \(\tilde {B}^{-1}\) cannot be directly evaluated or discretized. We need to compute the discretized matrix of \(\tilde {B}\) first, then use its inverse matrix to compute an approximation to the discretized matrix for the inverse operator \(\tilde {B}^{-1}\). This can be visualized in the following diagram. Therefore, we have \begin{equation} \underline {\tilde {B}^{-1}} \approx \mathcal {M}_{\tilde {B}}^{\mathrm {T}} \tilde {\mathcal {B}}^{-1} \mathcal {M}_{\tilde {B}}. \end{equation} The matrix to be multiplied to both sides of the equation is \begin{equation} \mathcal {M}_{\tilde {B}}^{-1} \tilde {\mathcal {B}} \mathcal {M}_{\tilde {B}}^{-\mathrm {T}}. \end{equation} The spectral equivalence between the above two matrices \(\underline {\tilde {B}^{-1}}\) and \(\mathcal {M}_{\tilde {B}}^{\mathrm {T}} \tilde {\mathcal {B}}^{-1} \mathcal {M}_{\tilde {B}}\) can be proved using the theory given in (Steinbach and Wendland, section 3). This theory is described in a general form as below. \(V\) and \(W\) are two Hilbert spaces. \(A: V \rightarrow V'\) is a \(V\)-elliptic, self-adjoint and bounded operator, while \(B: W \rightarrow V'\) is any bounded operator. Define an operator \(T: W \rightarrow W'\) as \(T = B' A^{-1} B\). In principle, it has a Galerkin matrix \(\mathcal {T}\). In reality, this matrix cannot be directly computed. The Galerkin matrices for \(A\) and \(B\) are \(\mathcal {A}\) and \(\mathcal {B}\) respectively. Then we have another matrix \begin{equation} \tilde {\mathcal {T}} = \mathcal {B}^{\mathrm {T}} \mathcal {A}^{-1} \mathcal {B}. \end{equation} It can be proved that for any \(w_h\in W_h\), \begin{equation} ( \tilde {\mathcal {T}} \underline {w}, \underline {w} ) \leq ( \mathcal {T} \underline {w}, \underline {w} ). \end{equation} If the stability condition or \(\inf \sup \) condition is satisfied \begin{equation} \inf _{0\neq w_h\in W_h} \sup _{0\neq v_h\in V_h} \frac {\lvert \langle Bw_h,v_h \rangle _{V} \rvert }{\lVert w_h \rVert _W \lVert v_h \rVert _V} \geq c, \end{equation} we also have \begin{equation} \gamma ( \mathcal {T} \underline {w}, \underline {w} ) \leq ( \tilde {\mathcal {T}} \underline {w}, \underline {w} ). \end{equation} Therefore, \(\mathcal {T}\) and \(\tilde {\mathcal {T}}\) are spectrally equivalent. The relationship between the spaces, operators and matrices are shown below. Coming back to the spectral equivalence between the two matrices \(\underline {\tilde {B}^{-1}}\) and \(\mathcal {M}_{\tilde {B}}^{\mathrm {T}} \tilde {\mathcal {B}}^{-1} \mathcal {M}_{\tilde {B}}\), we can see that it is the special case when \(W\) is selected to be \(V'\) and \(B\) is the identity operator on \(V'\). The mass matrix \(M_A\) for the duality pairing from \(V'\) to \(V\) is just the Galerkin matrix of \(B\). This is shown in the following figure. 5 Preconditioners for Laplace problem The operator equation for the Laplace problem with Dirichlet boundary condition is \begin{equation} \langle Vt, \tau \rangle _{\Gamma } = \langle (\frac {1}{2}I + K)g, \tau \rangle _{\Gamma } \quad \forall \tau \in H^{-1/2}(\Gamma ). \end{equation} The hypersingular operator \(D\) has an opposite order with respect to \(V\), which is naturally the preconditioner for this equation. However, according to here, \(D\) is not \(H^{1/2}(\Gamma )\)-elliptic, but only \(H_{\ast }^{1/2}(\Gamma )\)-elliptic or \(H_{\ast \ast }^{1/2}(\Gamma )\)-elliptic depending on which kind of inner product is desired. To avoid the complex of computing the natural density \(w_{\mathrm {eq}}\), we choose \(H_{\ast \ast }^{1/2}(\Gamma )\)-ellipticity. Then the bilinear form associated with the regularized or gauged operator \(\tilde {D}\) is \begin{equation} \langle \tilde {D}u,w \rangle = \langle Du,w \rangle + \langle u,1 \rangle _{\Gamma } \langle w,1 \rangle _{\Gamma } \quad \forall u,w\in H^{1/2}(\Gamma ). \end{equation} Then the inverse of the preconditioning matrix is \begin{equation} \mathcal {M}_{\tilde {D}}^{-1} \tilde {\mathcal {D}} \mathcal {M}_{\tilde {D}}^{-\mathrm {T}}, \end{equation} where \(\mathcal {M}_{\tilde {D}}\) is the mass matrix for the duality pairing from \(H^{-1/2}(\Gamma )\) to \(H^{1/2}(\Gamma )\). The operator equation for the Laplace problem with Neumann boundary condition is \begin{equation} \langle Du, v \rangle _{\Gamma } + \alpha \langle u,1 \rangle _{\Gamma } \langle v,1 \rangle _{\Gamma } = \langle (\frac {1}{2}I - K')g,v \rangle _{\Gamma } \quad \forall v\in H^{1/2}(\Gamma ). \end{equation} The single layer potential operator \(V\) has an opposite order with respect to \(D\), which is the preconditioner. When \(d=3\), \(V\) is \(H^{-1/2}(\Gamma )\)-elliptic, so we directly obtain the inverse of the preconditioning matrix \begin{equation} \mathcal {M}_V^{-1} \mathcal {V} \mathcal {M}_V^{-\mathrm {T}}, \end{equation} where \(\mathcal {M}_V\) is the mass matrix for the duality pairing from \(H^{1/2}(\Gamma )\) to \(H^{-1/2}(\Gamma )\). References    William Charles Hector McLean. Strongly Elliptic Systems and Boundary Integral Equations. Cambridge University Press. ISBN 978-0-521-66375-5.    Olaf Steinbach. Numerical Approximation Methods for Elliptic Boundary Value Problems: Finite and Boundary Elements. Springer Science &amp; Business Media. ISBN 978-0-387-31312-2.    Olaf Steinbach and Wolfgang L. Wendland. The construction of some efficient preconditioners in the boundary element method. 9(1-2):191–216. URL http://link.springer.com/article/10.1023/A:1018937506719.]]></summary></entry><entry><title type="html">Basic ideas of operator preconditioning</title><link href="https://jihuan-tian.github.io/math/2024/11/11/basic-ideas-of-operator-preconditioning.html" rel="alternate" type="text/html" title="Basic ideas of operator preconditioning" /><published>2024-11-11T00:00:00+08:00</published><updated>2024-11-11T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/11/11/basic-ideas-of-operator-preconditioning</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/11/11/basic-ideas-of-operator-preconditioning.html"><![CDATA[<h3 class='likesectionHead'><a id='x1-1000'></a>Contents</h3>
   <div class='tableofcontents'>
    <span class='sectionToc'>1 <a href='#x1-20001' id='QQ2-1-2'>Pseudodifferential operator</a></span>
<br />    <span class='sectionToc'>2 <a href='#x1-30002' id='QQ2-1-3'>Operator preconditioning based on pseudodifferential operator
of opposite orders</a></span>
<br />    <span class='sectionToc'>3 <a href='#x1-40003' id='QQ2-1-4'>Boundary integral operators considered as pseudodifferential operators</a></span>
   </div>
<!-- l. 24 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>1    </span> <a id='x1-20001'></a>Pseudodifferential operator</h3>
<!-- l. 26 --><p class='noindent'>Derivative properties of Fourier transform </p>
     <ul class='itemize1'>
     <li class='itemize'>
     <!-- l. 28 --><p class='noindent'>Fourier transform on a rapidly decreasing function \begin{align}  D^{\alpha }(\mathcal {F}\varphi )(\xi ) &amp;= (-\rmi )^{\abs {\alpha }}\mathcal {F}(x^{\alpha }\varphi )(\xi ) \\ \mathcal {F}(D^{\alpha }\varphi )(\xi )&amp;=\rmi ^{\abs {\alpha }}\xi ^{\alpha }(\mathcal {F}\varphi )(\xi )  \end{align}
     </p></li>
     <li class='itemize'>
     <!-- l. 35 --><p class='noindent'>Fourier transform on a tempered distribution \begin{align}  D^{\alpha }(\mathcal {F}T)&amp;=(-\rmi )^{\abs {\alpha }}\mathcal {F}(\xi ^{\alpha }T)\\ \mathcal {F}(D^{\alpha }T)&amp;=\rmi ^{\abs {\alpha }}x^{\alpha }(\mathcal {F}T)  \end{align}
</p>
     </li></ul>
<!-- l. 42 --><p class='indent'>   We can see that when the partial differential operator \(D^{\alpha }\) is applied to a function \(\varphi \) or a distribution \(T\), it is equivalent to
multiply a factor \(\xi ^{\alpha }\) or \(x^{\alpha }\) along with the complex constant \(\mathrm {i}^{\lvert \alpha \rvert }\), i.e. a monomial with respect to the variable in the reciprocal
space <span class='footnote-mark'><a href='#fn1x0' id='fn1x0-bk'><sup class='textsuperscript'>1</sup></a></span><a id='x1-2001f1'></a>,
to the Fourier transform of the original \(\varphi \) or \(T\).
                                                                                               
                                                                                               
</p><!-- l. 44 --><p class='indent'>   Therefore, for a general linear partial differential operator (<a href='#Xfolland-pde-1995'>Folland</a>, Chapter 8) \begin{equation}  L = \sum _{\lvert \alpha \rvert \leq k} a_{\alpha }(x) D^{\alpha }, a_{\alpha }(x)\in C^{\infty }(\Omega ),  \end{equation}<a id='x1-2003r1'></a> Its influence on a rapidly
decreasing function \(u\) in the frequency domain is multiplying a polynomial with respect to \(\xi \) \(\sigma _L(x,\xi )\) to \(\hat {u}(\xi )\), i.e. \begin{equation}  \mathcal {F}(Lu)(\xi ) = \sigma _L(x,\xi ) \hat {u}(\xi ) = \left ( \sum _{\lvert \alpha \rvert \leq k} a_{\alpha }(x)\mathrm {i}^{\lvert \alpha \rvert }\xi ^{\alpha } \right ) \hat {u}(\xi ).  \end{equation}<a id='x1-2004r2'></a> If we
generalize the polynomial \(\sigma _L(x,\xi )\) to a larger class of functions \(p(x,\xi )\), more types of linear operators, such as integral
operators, can be considered in a same unified framework. This brings about the concept of pseudodifferential
operators.
</p>
   <div class='definition'><div class='newtheorem'>
<!-- l. 56 --><p class='noindent'><span class='head'>
<span class='p1xb-x-x-109'>Definition 1 (Space of symbols)</span> </span><a id='x1-2006'></a>The space of symbols of order \(m\) is a set containing all \(p(x,\xi )\in C^{\infty }(\Omega \times \mathbb {R}^d)\) such that for all
multi-indices \(\alpha \) and \(\beta \) and any compact set \(K\subset \Omega \), there exists \(C_{\alpha ,\beta ,K}\) satisfying \begin{equation}  \sup _{x\in K} \lvert D_x^{\beta }D_{\xi }^{\alpha }p(x,\xi ) \rvert \leq C_{\alpha ,\beta ,K}(1+\lvert \xi \rvert )^{m-\lvert \alpha \rvert }.  \end{equation}<a id='x1-2007r3'></a>
</p>
   </div>
<!-- l. 62 --><p class='indent'>   </p></div>
   <div class='definition'><div class='newtheorem'>
<!-- l. 64 --><p class='noindent'><span class='head'>
<span class='p1xb-x-x-109'>Definition 2 (Pseudodifferential operator)</span> </span><a id='x1-2009'></a>A pseudodifferential operator \(p(x,D)\) of order \(m\) is a linear map from \(\mathcal {D}(\Omega )\) to \(C^{\infty }(\Omega )\)
such that \begin{equation}  \mathcal {F}(p(x,D)u) = p(x,\xi )\hat {u}(\xi ),  \end{equation}<a id='x1-2010r4'></a> where \(p(x,\xi )\) is a symbol of order \(m\).
</p>
   </div>
<!-- l. 71 --><p class='indent'>   </p></div>
<!-- l. 73 --><p class='indent'>   <span class='p1xb-x-x-109'>The ingenious idea here is converting a differential operator to an integral representation in the frequency
</span><span class='p1xb-x-x-109'>domain via Fourier transform. This is not only equivalent to the original form, but also makes it possible to
</span><span class='p1xb-x-x-109'>handle more classes of operators.</span>
</p>
   <h3 class='sectionHead'><span class='titlemark'>2    </span> <a id='x1-30002'></a>Operator preconditioning based on pseudodifferential operator of opposite orders</h3>
<!-- l. 76 --><p class='noindent'>The construction of a preconditioner \(B\) for an operator \(A\) is equivalent to looking for an approximate inverse
operator of \(A\). In the discrete case, the preconditioning matrix \(\underline {B}^{-1}\) should be spectrally equivalent to \(A\) and \(\underline {B}\)
should be an approximate inverse matrix of \(\mathcal {A}\). Therefore, the behavior of the composite operator \(BA\)
should be similar to an identity operator. Being a pseudodifferential operator of order \(m\), \(A\) has the
symbol \(p(x,\xi )\) whose mixed partial derivatives (with respect to both \(x\) and \(\xi \)) are controlled by \(C_{\alpha ,\beta ,K}(1+\lvert \xi \rvert )^{m-\lvert \alpha \rvert }\). For partial
derivatives with respect to only \(\xi \), they are comparable to the polynomial of \(1+\lvert \xi \rvert ^m\) of order \(m\). If the symbol of the
preconditioner \(B\) is \(\frac {1}{1+\lvert \xi \rvert ^m}\) of order \(-m\), the symbol of the composite operator \(BA\) is comparable to \(1\). Therefore,
the operator preconditioning method looks for a pseudodifferential operator having the opposite
order.
</p>
                                                                                               
                                                                                               
   <h3 class='sectionHead'><span class='titlemark'>3    </span> <a id='x1-40003'></a>Boundary integral operators considered as pseudodifferential operators</h3>
<!-- l. 79 --><p class='noindent'>According to the definition in (<a href='#XSteinbachConstruction1998'>Steinbach and Wendland</a>), the pseudodifferential operator \(A: H^s(\Gamma ) \rightarrow H^{s-2\alpha }(\Gamma )\) has an order \(2\alpha \).
Therefore, we have the orders for boundary integral operators which are treated as pseudodifferential operators.
</p>
<div class='center'>
<!-- l. 80 --><p class='noindent'>
</p>
<div class='tabular'> <table class='tabular' id='TBL-2'><colgroup id='TBL-2-1g'><col id='TBL-2-1' /><col id='TBL-2-2' /></colgroup><tr id='TBL-2-1-' style='vertical-align:baseline;'><td class='td11' id='TBL-2-1-1' style='white-space:nowrap; text-align:left;'>Boundary integral operator                                                                                                                                     </td><td class='td11' id='TBL-2-1-2' style='white-space:nowrap; text-align:right;'>Order</td>
</tr><tr class='hline'><td><hr /></td><td><hr /></td></tr><tr id='TBL-02-4-' style='vertical-align:baseline;'><td class='td11' id='TBL-02-4-1' style='white-space:nowrap; text-align:left;'>\(V: H^{-1/2}(\Gamma ) \rightarrow H^{1/2}(\Gamma )\)                                                                                       </td><td class='td11' id='TBL-02-4-2' style='white-space:nowrap; text-align:right;'>   -1</td></tr><tr id='TBL-002-7-' style='vertical-align:baseline;'><td class='td11' id='TBL-002-7-1' style='white-space:nowrap; text-align:left;'>\(K: H^{1/2}(\Gamma ) \rightarrow H^{1/2}(\Gamma )\)</td><td class='td11' id='TBL-002-7-2' style='white-space:nowrap; text-align:right;'>    0</td>
</tr><tr id='TBL-0002-10-' style='vertical-align:baseline;'><td class='td11' id='TBL-0002-10-1' style='white-space:nowrap; text-align:left;'>\(K': H^{-1/2}(\Gamma ) \rightarrow H^{-1/2}(\Gamma )\)                                                                                       </td><td class='td11' id='TBL-0002-10-2' style='white-space:nowrap; text-align:right;'>    0</td>
</tr><tr id='TBL-00002-13-' style='vertical-align:baseline;'><td class='td11' id='TBL-00002-13-1' style='white-space:nowrap; text-align:left;'>\(D: H^{1/2}(\Gamma ) \rightarrow H^{-1/2}(\Gamma )\)                                                                                       </td><td class='td11' id='TBL-00002-13-2' style='white-space:nowrap; text-align:right;'>    1</td>
</tr></table>        </div></div>
<!-- l. 91 --><p class='indent'>   Among these operators, \(V\) and \(D\) are self-adjoint and appear as the main operators in PDEs. They
happen to have opposite orders. In addition, according to (<a href='#XSteinbachNumerical2007'>Steinbach</a>, Corollary 6.19 P138), \(V\) and \(D\) have
the following relations: \begin{equation}  \begin{aligned} VD &amp;= (\sigma I+K)((1-\sigma )I-K) \\ DV &amp;= (\sigma I+K')((1-\sigma )I-K') \end{aligned}.  \end{equation}<a id='x1-4001r5'></a> The terms on the right hand sides of these two equations involve \(K\) and
\(K'\). Both of them are zero order pseudodifferential operators. So are \(VD\) and \(DV\). This is consistent with
the fact that \(V\) and \(D\) have opposite orders. Therefore, \(V\) is inherently the preconditioner of \(D\) and vice
versa.
</p><!-- l. 1 --><p class='noindent'>
</p>
   <h3 class='likesectionHead'><a id='x1-5000'></a>References</h3>
<!-- l. 1 --><p class='noindent'>
  </p><div class='thebibliography'>
  <p class='bibitem'><span class='biblabel'>
<a id='Xfolland-pde-1995'></a><span class='bibsp'>   </span></span>Gerald B. Folland.  <span class='p1xi-x-x-109'>Introduction to Partial Differential Equations. Second Edition</span>.  Princeton University
  Press, second edition edition. ISBN 978-0-691-04361-6.
  </p>
  <p class='bibitem'><span class='biblabel'>
<a id='XSteinbachNumerical2007'></a><span class='bibsp'>   </span></span>Olaf  Steinbach.    <span class='p1xi-x-x-109'>Numerical  Approximation  Methods  for  Elliptic  Boundary  Value  Problems:  Finite  and
  </span><span class='p1xi-x-x-109'>Boundary Elements</span>. Springer Science &amp; Business Media. ISBN 978-0-387-31312-2.
  </p>
  <p class='bibitem'><span class='biblabel'>
<a id='XSteinbachConstruction1998'></a><span class='bibsp'>   </span></span>Olaf               Steinbach               and               Wolfgang L.               Wendland.                                      The
  construction of some efficient preconditioners in the boundary element method.  9(1-2):191–216.  URL
  <a class='url' href='http://link.springer.com/article/10.1023/A:1018937506719'><span class='t1xtt-x-x-109'>http://link.springer.com/article/10.1023/A:1018937506719</span></a>.
                                                                                               
                                                                                               
</p>
  </div>
   <div class='footnotes'><a id='x1-2002x1'></a>
<!-- l. 42 --><p class='indent'>      <span class='footnote-mark'><a href='#fn1x0-bk' id='fn1x0'><sup class='textsuperscript'>1</sup></a></span><span class='p1xr-x-x-90'>As a convention, we use </span>\(x\) <span class='p1xr-x-x-90'>as the variable for </span>\(\varphi \) <span class='p1xr-x-x-90'>in the “time” domain and </span>\(\xi \) <span class='p1xr-x-x-90'>in the reciprocal, i.e. “frequency”, domain. According to
</span><span class='p1xr-x-x-90'>the definition of the Fourier transform of a distribution, the variable adopted for </span>\(T\) <span class='p1xr-x-x-90'>should be </span>\(\xi \) <span class='p1xr-x-x-90'>instead and the variable for its Fourier
</span><span class='p1xr-x-x-90'>transform should be </span>\(x\)<span class='p1xr-x-x-90'>.</span></p>                                                                                                                                                                                         </div>

Backlinks: <a href="/math/2024/11/16/general-theory-about-the-construction-of-preconditioning-bilinear-form-in-bem.html">《General theory about the construction of preconditioning bilinear form in BEM》</a>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="PDE" /><category term="BEM" /><summary type="html"><![CDATA[Contents  1 Pseudodifferential operator  2 Operator preconditioning based on pseudodifferential operator of opposite orders  3 Boundary integral operators considered as pseudodifferential operators 1 Pseudodifferential operator Derivative properties of Fourier transform Fourier transform on a rapidly decreasing function \begin{align} D^{\alpha }(\mathcal {F}\varphi )(\xi ) &amp;= (-\rmi )^{\abs {\alpha }}\mathcal {F}(x^{\alpha }\varphi )(\xi ) \\ \mathcal {F}(D^{\alpha }\varphi )(\xi )&amp;=\rmi ^{\abs {\alpha }}\xi ^{\alpha }(\mathcal {F}\varphi )(\xi ) \end{align} Fourier transform on a tempered distribution \begin{align} D^{\alpha }(\mathcal {F}T)&amp;=(-\rmi )^{\abs {\alpha }}\mathcal {F}(\xi ^{\alpha }T)\\ \mathcal {F}(D^{\alpha }T)&amp;=\rmi ^{\abs {\alpha }}x^{\alpha }(\mathcal {F}T) \end{align} We can see that when the partial differential operator \(D^{\alpha }\) is applied to a function \(\varphi \) or a distribution \(T\), it is equivalent to multiply a factor \(\xi ^{\alpha }\) or \(x^{\alpha }\) along with the complex constant \(\mathrm {i}^{\lvert \alpha \rvert }\), i.e. a monomial with respect to the variable in the reciprocal space 1, to the Fourier transform of the original \(\varphi \) or \(T\). Therefore, for a general linear partial differential operator (Folland, Chapter 8) \begin{equation} L = \sum _{\lvert \alpha \rvert \leq k} a_{\alpha }(x) D^{\alpha }, a_{\alpha }(x)\in C^{\infty }(\Omega ), \end{equation} Its influence on a rapidly decreasing function \(u\) in the frequency domain is multiplying a polynomial with respect to \(\xi \) \(\sigma _L(x,\xi )\) to \(\hat {u}(\xi )\), i.e. \begin{equation} \mathcal {F}(Lu)(\xi ) = \sigma _L(x,\xi ) \hat {u}(\xi ) = \left ( \sum _{\lvert \alpha \rvert \leq k} a_{\alpha }(x)\mathrm {i}^{\lvert \alpha \rvert }\xi ^{\alpha } \right ) \hat {u}(\xi ). \end{equation} If we generalize the polynomial \(\sigma _L(x,\xi )\) to a larger class of functions \(p(x,\xi )\), more types of linear operators, such as integral operators, can be considered in a same unified framework. This brings about the concept of pseudodifferential operators. Definition 1 (Space of symbols) The space of symbols of order \(m\) is a set containing all \(p(x,\xi )\in C^{\infty }(\Omega \times \mathbb {R}^d)\) such that for all multi-indices \(\alpha \) and \(\beta \) and any compact set \(K\subset \Omega \), there exists \(C_{\alpha ,\beta ,K}\) satisfying \begin{equation} \sup _{x\in K} \lvert D_x^{\beta }D_{\xi }^{\alpha }p(x,\xi ) \rvert \leq C_{\alpha ,\beta ,K}(1+\lvert \xi \rvert )^{m-\lvert \alpha \rvert }. \end{equation} Definition 2 (Pseudodifferential operator) A pseudodifferential operator \(p(x,D)\) of order \(m\) is a linear map from \(\mathcal {D}(\Omega )\) to \(C^{\infty }(\Omega )\) such that \begin{equation} \mathcal {F}(p(x,D)u) = p(x,\xi )\hat {u}(\xi ), \end{equation} where \(p(x,\xi )\) is a symbol of order \(m\). The ingenious idea here is converting a differential operator to an integral representation in the frequency domain via Fourier transform. This is not only equivalent to the original form, but also makes it possible to handle more classes of operators. 2 Operator preconditioning based on pseudodifferential operator of opposite orders The construction of a preconditioner \(B\) for an operator \(A\) is equivalent to looking for an approximate inverse operator of \(A\). In the discrete case, the preconditioning matrix \(\underline {B}^{-1}\) should be spectrally equivalent to \(A\) and \(\underline {B}\) should be an approximate inverse matrix of \(\mathcal {A}\). Therefore, the behavior of the composite operator \(BA\) should be similar to an identity operator. Being a pseudodifferential operator of order \(m\), \(A\) has the symbol \(p(x,\xi )\) whose mixed partial derivatives (with respect to both \(x\) and \(\xi \)) are controlled by \(C_{\alpha ,\beta ,K}(1+\lvert \xi \rvert )^{m-\lvert \alpha \rvert }\). For partial derivatives with respect to only \(\xi \), they are comparable to the polynomial of \(1+\lvert \xi \rvert ^m\) of order \(m\). If the symbol of the preconditioner \(B\) is \(\frac {1}{1+\lvert \xi \rvert ^m}\) of order \(-m\), the symbol of the composite operator \(BA\) is comparable to \(1\). Therefore, the operator preconditioning method looks for a pseudodifferential operator having the opposite order. 3 Boundary integral operators considered as pseudodifferential operators According to the definition in (Steinbach and Wendland), the pseudodifferential operator \(A: H^s(\Gamma ) \rightarrow H^{s-2\alpha }(\Gamma )\) has an order \(2\alpha \). Therefore, we have the orders for boundary integral operators which are treated as pseudodifferential operators. Boundary integral operator Order \(V: H^{-1/2}(\Gamma ) \rightarrow H^{1/2}(\Gamma )\) -1\(K: H^{1/2}(\Gamma ) \rightarrow H^{1/2}(\Gamma )\) 0 \(K': H^{-1/2}(\Gamma ) \rightarrow H^{-1/2}(\Gamma )\) 0 \(D: H^{1/2}(\Gamma ) \rightarrow H^{-1/2}(\Gamma )\) 1 Among these operators, \(V\) and \(D\) are self-adjoint and appear as the main operators in PDEs. They happen to have opposite orders. In addition, according to (Steinbach, Corollary 6.19 P138), \(V\) and \(D\) have the following relations: \begin{equation} \begin{aligned} VD &amp;= (\sigma I+K)((1-\sigma )I-K) \\ DV &amp;= (\sigma I+K')((1-\sigma )I-K') \end{aligned}. \end{equation} The terms on the right hand sides of these two equations involve \(K\) and \(K'\). Both of them are zero order pseudodifferential operators. So are \(VD\) and \(DV\). This is consistent with the fact that \(V\) and \(D\) have opposite orders. Therefore, \(V\) is inherently the preconditioner of \(D\) and vice versa. References    Gerald B. Folland. Introduction to Partial Differential Equations. Second Edition. Princeton University Press, second edition edition. ISBN 978-0-691-04361-6.    Olaf Steinbach. Numerical Approximation Methods for Elliptic Boundary Value Problems: Finite and Boundary Elements. Springer Science &amp; Business Media. ISBN 978-0-387-31312-2.    Olaf Steinbach and Wolfgang L. Wendland. The construction of some efficient preconditioners in the boundary element method. 9(1-2):191–216. URL http://link.springer.com/article/10.1023/A:1018937506719. 1As a convention, we use \(x\) as the variable for \(\varphi \) in the “time” domain and \(\xi \) in the reciprocal, i.e. “frequency”, domain. According to the definition of the Fourier transform of a distribution, the variable adopted for \(T\) should be \(\xi \) instead and the variable for its Fourier transform should be \(x\).]]></summary></entry><entry><title type="html">Ellipticity of boundary integral operators</title><link href="https://jihuan-tian.github.io/math/2024/11/11/ellipticity-of-boundary-integral-operators.html" rel="alternate" type="text/html" title="Ellipticity of boundary integral operators" /><published>2024-11-11T00:00:00+08:00</published><updated>2024-11-11T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/11/11/ellipticity-of-boundary-integral-operators</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/11/11/ellipticity-of-boundary-integral-operators.html"><![CDATA[<h3 class='likesectionHead'><a id='x1-1000'></a>Contents</h3>
   <div class='tableofcontents'>
    <span class='sectionToc'>1 <a href='#x1-20001' id='QQ2-1-2'>Ellipticity and solution uniqueness</a></span>
<br />    <span class='sectionToc'>2 <a href='#x1-30002' id='QQ2-1-3'>Boundary integral operator related to the single layer potential</a></span>
<br />    <span class='sectionToc'>3 <a href='#x1-40003' id='QQ2-1-4'>Natural density and capacity</a></span>
<br />    <span class='sectionToc'>4 <a href='#x1-50004' id='QQ2-1-5'>Hypersingular boundary integral operator</a></span>
<br />    <span class='sectionToc'>5 <a href='#x1-60005' id='QQ2-1-6'>Steklov-Poincaré operator</a></span>
   </div>
<!-- l. 24 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>1    </span> <a id='x1-20001'></a>Ellipticity and solution uniqueness</h3>
<!-- l. 26 --><p class='noindent'><a href='/math/2024/06/19/fundamental-theorems-in-pde-theory.html#org650a387'>Lax-Milgram Lemma</a> is an existence and uniqueness theorem for the solution of an operator equation as well as
its equivalent variational formulation, whose key feature is governed by the operator \(A: X \rightarrow X'\) or the corresponding
bilinear form \(a(\cdot ,\cdot )\).
</p><!-- l. 28 --><p class='indent'>   The operator equation is \begin{equation}  Au = f  \end{equation}<a id='x1-2001r1'></a> and its variational formulation is \begin{equation}  a(u,v) = \left \langle Au,v \right \rangle = \left \langle f,v \right \rangle \quad \forall v\in X.  \end{equation}<a id='x1-2002r2'></a> The boundedness and \(X\)-ellipticity of \(A\) ensures the
existence and uniqueness of the solution \(u\). N.B. Ellipticity of an operator is defined with respect to with its
domain.
</p><!-- l. 38 --><p class='indent'>   <span class='p1xb-x-x-109'>Analogy</span> </p>
     <ul class='itemize1'>
     <li class='itemize'>
     <!-- l. 40 --><p class='noindent'>For a matrix equation \(Ax=b\), the ellipticity condition is equivalent to that the minimum eigenvalue of \(A\) is
     larger than 0 (see <a href='/math/2024/06/07/understanding-about-ellipticity-of-operators.html'>here</a>). Therefore, the matrix is invertible and the solution exists and is unique.
     </p></li>
     <li class='itemize'>
                                                                                               
                                                                                               
     <!-- l. 41 --><p class='noindent'>For the simple division in elementary arithmetic, the ellipticity is equivalent to that the denominator
     is not zero.</p></li></ul>
<!-- l. 43 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>2    </span> <a id='x1-30002'></a>Boundary integral operator related to the single layer potential</h3>
<!-- l. 45 --><p class='noindent'>When the spatial dimension \(d=3\), \(V: H^{-1/2}(\Gamma ) \rightarrow H^{1/2}(\Gamma )\) is \(H^{-1/2}(\Gamma )\)-elliptic, i.e. it is elliptic on its whole domain. However, when \(d=2\), \(V\) is only \(H_{\ast }^{-1/2}(\Gamma )\)-elliptic,
i.e. it is elliptic on a subspace of its domain, where \begin{equation}  H_{\ast }^{-1/2}(\Gamma ) := \left \{ v\in H^{-1/2}(\Gamma ): \left \langle v,1 \right \rangle _{\Gamma }=0 \right \}.  \end{equation}<a id='x1-3001r3'></a> The functions in this space are orthogonal to constant
functions, or we say the kernel of \(V\) when \(d=2\) is \(\mathrm {span}\{u_0\}\) where \(u_0 \equiv 1\).
</p><!-- l. 51 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>3    </span> <a id='x1-40003'></a>Natural density and capacity</h3>
<!-- l. 53 --><p class='noindent'>By introducing a natural density \(w_{\mathrm {eq}}\) and capacity \(\lambda \), it can be proved that when \(\mathrm {diam}(\Omega )&lt; 1\) and \(d=2\), \(V\) is \(H^{-1/2}(\Gamma )\)-elliptic (<a href='#XSteinbachNumerical2007'>Steinbach</a>, Theorem
6.23 on P143).
</p><!-- l. 55 --><p class='indent'>   The variational equation for \(w_{\mathrm {eq}}\) is \begin{equation}  \label {eq:weq-var-eq} \begin{aligned} \left \langle V w_{\mathrm {eq}},\tau \right \rangle _{\Gamma } - \lambda \left \langle 1,\tau \right \rangle _{\Gamma } &amp;= 0 \\ \left \langle w_{\mathrm {eq}}, 1 \right \rangle _{\Gamma } &amp;= 1 \end{aligned} \quad \forall \tau \in H^{-1/2}(\Gamma ),  \end{equation}<a id='x1-4001r4'></a> where \(\lambda \) is the Lagrange multiplier to restrict the test function \(\tau \) within
the subspace \(H_{\ast }^{-1/2}(\Gamma )\). This variational equation is equivalent to the following operator equation: \begin{equation}  \label {eq:weq-op-eq} \begin{aligned} (V w_{\mathrm {eq}})(x) &amp;= \lambda \\ \left \langle w_{\mathrm {eq}}, 1 \right \rangle _{\Gamma } &amp;= 1 \end{aligned} \quad x\in \Gamma .  \end{equation}<a id='x1-4002r5'></a> From
the variational equation, we have \(\langle Vw_{\mathrm {eq}},w_{\mathrm {eq}} \rangle _{\Gamma }=\lambda \). We can also derive a normalized version of \(w_{\mathrm {eq}}\): \begin{equation}  \tilde {w}_{\mathrm {eq}} := \frac {w_{\mathrm {eq}}}{\lambda },  \end{equation}<a id='x1-4003r6'></a> which satisfies
\begin{equation}  (V\tilde {w}_{\mathrm {eq}})(x) = 1, \; \langle \tilde {w}_{\mathrm {eq}},1 \rangle _{\Gamma }=\frac {1}{\lambda }.  \end{equation}<a id='x1-4004r7'></a>
</p><!-- l. 81 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>4    </span> <a id='x1-50004'></a>Hypersingular boundary integral operator</h3>
<!-- l. 83 --><p class='noindent'>Unlike the single layer potential boundary integral operator \(V\), the hypersingular boundary integral operator \(D: H^{1/2}(\Gamma ) \rightarrow H^{-1/2}(\Gamma )\) has
a non-trivial kernel \(\mathrm {ker}(D)=\mathrm {span} \{ u_0 \}\), so the ellipticity is not available on the whole space \(H^{1/2}(\Gamma )\) but is only valid in some subspace,
which is the orthogonal space of \(\mathrm {ker}(D)\). The concept of orthogonality requires an inner product structure assigned to
the space. Because \(H^{1/2}(\Gamma )\) is a Hilbert space, the inner product is available. Different definitions of the inner product for \(H^{1/2}(\Gamma )\)
lead to different definitions of orthogonality. Hence, different orthogonal subspaces with respect to \(\mathrm {ker}(D)\) will be
obtained. Here we have three choices.
</p><!-- l. 85 --><p class='indent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-5002x1'>
     <!-- l. 86 --><p class='noindent'>The inner product is \begin{equation}  ( u,v )_{H_{\ast }^{1/2}(\Gamma )} := \langle u,w_{\mathrm {eq}} \rangle _{\Gamma } \cdot \langle v,w_{\mathrm {eq}} \rangle _{\Gamma } + \lvert u \rvert _{H^{1/2}(\Gamma )}\cdot \lvert v \rvert _{H^{1/2}(\Gamma )},  \end{equation}<a id='x1-5003r8'></a> where \(\lvert \cdot \rvert _{H^{1/2}(\Gamma )}\) is the Sobolev-Slobodeckii semi-norm.
     </p><!-- l. 94 --><p class='noindent'>The corresponding norm is \begin{equation}  \lVert v \rVert _{H_{\ast }^{1/2}(\Gamma )} := \left \{ \langle v,w_{\mathrm {eq}} \rangle _{\Gamma }^{2} + \lvert v \rvert _{H^{1/2}(\Gamma )}^{2} \right \}^{1/2}.  \end{equation}<a id='x1-5004r9'></a> This norm is equivalent to the Sobolev norm for \(H^{1/2}(\Gamma )\).
     </p><!-- l. 100 --><p class='noindent'>Define the following space \begin{equation}  H_{\ast }^{1/2}(\Gamma ) := \{ v\in H^{1/2}(\Gamma ): \langle v,w_{\mathrm {eq}} \rangle _{\Gamma }=0 \}.  \end{equation}<a id='x1-5005r10'></a> It can be proved that using the above definition of inner product, \(H_{\ast }^{1/2}(\Gamma )\) is orthogonal
     to \(\mathrm {ker}(D)\). Therefore, \(D\) is \(H_{\ast }^{1/2}(\Gamma )\)-elliptic.
     </p></li>
                                                                                               
                                                                                               
<li class='enumerate' id='x1-5007x2'>
     <!-- l. 106 --><p class='noindent'>The inner product is \begin{equation}  ( u,v )_{H_{\ast \ast }^{1/2}(\Gamma )} := \langle u,1 \rangle _{\Gamma }\cdot \langle v,1 \rangle _{\Gamma } + \lvert u \rvert _{H^{1/2}(\Gamma )}\cdot \lvert v \rvert _{H^{1/2}(\Gamma )}.  \end{equation}<a id='x1-5008r11'></a> The corresponding norm is \begin{equation}  \lVert v \rVert _{H_{\ast \ast }^{1/2}(\Gamma )} := \left \{ \langle v,1 \rangle _{\Gamma }^2 + \lvert v \rvert _{H^{1/2}(\Gamma )}^2 \right \}^{1/2}.  \end{equation}<a id='x1-5009r12'></a> This norm is equivalent to the Sobolev norm for
     \(H^{1/2}(\Gamma )\).
     </p><!-- l. 116 --><p class='noindent'>Define the following space \begin{equation}  H_{\ast \ast }^{1/2}(\Gamma ) := \{ v\in H^{1/2}(\Gamma ): \langle v,1 \rangle _{\Gamma }=0 \}.  \end{equation}<a id='x1-5010r13'></a> This space is orthogonal to \(\mathrm {ker}(D)\) and \(D\) is also \(H_{\ast \ast }^{1/2}(\Gamma )\)-elliptic.
     </p></li>
<li class='enumerate' id='x1-5012x3'>
     <!-- l. 122 --><p class='noindent'>Let \(\Gamma _0\) be an open set contained in \(\Gamma \) and we consider functions on \(\Gamma _0\) instead of on the whole boundary, i.e. the
     function space is \(\tilde {H}^{1/2}(\Gamma _0)\).
     </p><!-- l. 124 --><p class='noindent'>The inner product is \begin{equation}  ( u,v )_{H^{1/2}(\Gamma ),\Gamma _0} := \lVert u \rVert _{L_2(\Gamma \backslash \Gamma _0)} \cdot \lVert v \rVert _{L_2(\Gamma \backslash \Gamma _0)} + \lvert u \rvert _{H^{1/2}(\Gamma )}\cdot \lvert v \rvert _{H^{1/2}(\Gamma )}.  \end{equation}<a id='x1-5013r14'></a> Here \(u\) and \(v\) belongs to \(H^{1/2}(\Gamma )\). When they are only defined on \(\Gamma _0\) and belong to \(\tilde {H}^{1/2}(\Gamma _0)\), we
     need to apply the canonical extension to them, i.e. extension by zero while preserving the
     resulted function within the space \(H^{1/2}(\Gamma )\). \begin{equation}  \tilde {v} = \begin {cases} v(x) &amp; x\in \Gamma _0 \\ 0 &amp; x\in \Gamma \backslash \Gamma _0 \end {cases}.  \end{equation}<a id='x1-5014r15'></a> Then we define the inner product of \(u\) and \(v\) to be equal to
     \(( \tilde {u},\tilde {v} )_{H^{1/2}(\Gamma ),\Gamma _0}\).
     </p><!-- l. 141 --><p class='noindent'>The corresponding norm is \begin{equation}  \lVert v \rVert _{H^{1/2}(\Gamma ),\Gamma _0} := \left \{ \lVert v \rVert _{L_2(\Gamma \backslash \Gamma _0)}^2 + \lvert v \rvert _{H^{1/2}(\Gamma )}^2 \right \}^{1/2}.  \end{equation}<a id='x1-5015r16'></a> This norm is equivalent to the Sobolev norm for \(\tilde {H}^{1/2}(\Gamma _0)\).
     </p><!-- l. 147 --><p class='noindent'>The space \(\tilde {H}^{1/2}(\Gamma _0)\) is orthogonal to \(\mathrm {ker}(D)\) and \(D\) is \(\tilde {H}^{1/2}(\Gamma _0)\)-elliptic.</p></li></ol>
<!-- l. 150 --><p class='indent'>   To prove the ellipticity of \(D\) either on the whole space or a subspace, the Sobolev norm should be used. The
above definitions of norm play the role of scaffold during such a proof.
</p><!-- l. 152 --><p class='indent'>   <span class='p1xb-x-x-109'>Orthogonality defined based on different definitions of inner product lead to ellipticity of </span>\(D\) <span class='p1xb-x-x-109'>with respect to
</span><span class='p1xb-x-x-109'>different subspaces which are orthogonal to </span>\(\mathrm {ker}(D)\)
</p><!-- l. 154 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>5    </span> <a id='x1-60005'></a>Steklov-Poincaré operator</h3>
<!-- l. 156 --><p class='noindent'>The symmetric form of the Steklov-Poincaré operator is \begin{equation}  S := D + (\sigma I + K') V^{-1} (\sigma I + K).  \end{equation}<a id='x1-6001r17'></a> \(V^{-1}\) is \(H^{1/2}(\Gamma )\)-elliptic and the ellipticity of \(S\) is the same as
\(D\).
</p><!-- l. 1 --><p class='noindent'>
</p>
   <h3 class='likesectionHead'><a id='x1-7000'></a>References</h3>
<!-- l. 1 --><p class='noindent'>
  </p><div class='thebibliography'>
  <p class='bibitem'><span class='biblabel'>
<a id='XSteinbachNumerical2007'></a><span class='bibsp'>   </span></span>Olaf  Steinbach.    <span class='p1xi-x-x-109'>Numerical  Approximation  Methods  for  Elliptic  Boundary  Value  Problems:  Finite  and
  </span><span class='p1xi-x-x-109'>Boundary Elements</span>. Springer Science &amp; Business Media. ISBN 978-0-387-31312-2.
</p>
  </div>

Backlinks: <a href="/math/2024/11/16/general-theory-about-the-construction-of-preconditioning-bilinear-form-in-bem.html">《General theory about the construction of preconditioning bilinear form in BEM》</a>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="BEM" /><summary type="html"><![CDATA[Contents  1 Ellipticity and solution uniqueness  2 Boundary integral operator related to the single layer potential  3 Natural density and capacity  4 Hypersingular boundary integral operator  5 Steklov-Poincaré operator 1 Ellipticity and solution uniqueness Lax-Milgram Lemma is an existence and uniqueness theorem for the solution of an operator equation as well as its equivalent variational formulation, whose key feature is governed by the operator \(A: X \rightarrow X'\) or the corresponding bilinear form \(a(\cdot ,\cdot )\). The operator equation is \begin{equation} Au = f \end{equation} and its variational formulation is \begin{equation} a(u,v) = \left \langle Au,v \right \rangle = \left \langle f,v \right \rangle \quad \forall v\in X. \end{equation} The boundedness and \(X\)-ellipticity of \(A\) ensures the existence and uniqueness of the solution \(u\). N.B. Ellipticity of an operator is defined with respect to with its domain. Analogy For a matrix equation \(Ax=b\), the ellipticity condition is equivalent to that the minimum eigenvalue of \(A\) is larger than 0 (see here). Therefore, the matrix is invertible and the solution exists and is unique. For the simple division in elementary arithmetic, the ellipticity is equivalent to that the denominator is not zero. 2 Boundary integral operator related to the single layer potential When the spatial dimension \(d=3\), \(V: H^{-1/2}(\Gamma ) \rightarrow H^{1/2}(\Gamma )\) is \(H^{-1/2}(\Gamma )\)-elliptic, i.e. it is elliptic on its whole domain. However, when \(d=2\), \(V\) is only \(H_{\ast }^{-1/2}(\Gamma )\)-elliptic, i.e. it is elliptic on a subspace of its domain, where \begin{equation} H_{\ast }^{-1/2}(\Gamma ) := \left \{ v\in H^{-1/2}(\Gamma ): \left \langle v,1 \right \rangle _{\Gamma }=0 \right \}. \end{equation} The functions in this space are orthogonal to constant functions, or we say the kernel of \(V\) when \(d=2\) is \(\mathrm {span}\{u_0\}\) where \(u_0 \equiv 1\). 3 Natural density and capacity By introducing a natural density \(w_{\mathrm {eq}}\) and capacity \(\lambda \), it can be proved that when \(\mathrm {diam}(\Omega )&lt; 1\) and \(d=2\), \(V\) is \(H^{-1/2}(\Gamma )\)-elliptic (Steinbach, Theorem 6.23 on P143). The variational equation for \(w_{\mathrm {eq}}\) is \begin{equation} \label {eq:weq-var-eq} \begin{aligned} \left \langle V w_{\mathrm {eq}},\tau \right \rangle _{\Gamma } - \lambda \left \langle 1,\tau \right \rangle _{\Gamma } &amp;= 0 \\ \left \langle w_{\mathrm {eq}}, 1 \right \rangle _{\Gamma } &amp;= 1 \end{aligned} \quad \forall \tau \in H^{-1/2}(\Gamma ), \end{equation} where \(\lambda \) is the Lagrange multiplier to restrict the test function \(\tau \) within the subspace \(H_{\ast }^{-1/2}(\Gamma )\). This variational equation is equivalent to the following operator equation: \begin{equation} \label {eq:weq-op-eq} \begin{aligned} (V w_{\mathrm {eq}})(x) &amp;= \lambda \\ \left \langle w_{\mathrm {eq}}, 1 \right \rangle _{\Gamma } &amp;= 1 \end{aligned} \quad x\in \Gamma . \end{equation} From the variational equation, we have \(\langle Vw_{\mathrm {eq}},w_{\mathrm {eq}} \rangle _{\Gamma }=\lambda \). We can also derive a normalized version of \(w_{\mathrm {eq}}\): \begin{equation} \tilde {w}_{\mathrm {eq}} := \frac {w_{\mathrm {eq}}}{\lambda }, \end{equation} which satisfies \begin{equation} (V\tilde {w}_{\mathrm {eq}})(x) = 1, \; \langle \tilde {w}_{\mathrm {eq}},1 \rangle _{\Gamma }=\frac {1}{\lambda }. \end{equation} 4 Hypersingular boundary integral operator Unlike the single layer potential boundary integral operator \(V\), the hypersingular boundary integral operator \(D: H^{1/2}(\Gamma ) \rightarrow H^{-1/2}(\Gamma )\) has a non-trivial kernel \(\mathrm {ker}(D)=\mathrm {span} \{ u_0 \}\), so the ellipticity is not available on the whole space \(H^{1/2}(\Gamma )\) but is only valid in some subspace, which is the orthogonal space of \(\mathrm {ker}(D)\). The concept of orthogonality requires an inner product structure assigned to the space. Because \(H^{1/2}(\Gamma )\) is a Hilbert space, the inner product is available. Different definitions of the inner product for \(H^{1/2}(\Gamma )\) lead to different definitions of orthogonality. Hence, different orthogonal subspaces with respect to \(\mathrm {ker}(D)\) will be obtained. Here we have three choices. The inner product is \begin{equation} ( u,v )_{H_{\ast }^{1/2}(\Gamma )} := \langle u,w_{\mathrm {eq}} \rangle _{\Gamma } \cdot \langle v,w_{\mathrm {eq}} \rangle _{\Gamma } + \lvert u \rvert _{H^{1/2}(\Gamma )}\cdot \lvert v \rvert _{H^{1/2}(\Gamma )}, \end{equation} where \(\lvert \cdot \rvert _{H^{1/2}(\Gamma )}\) is the Sobolev-Slobodeckii semi-norm. The corresponding norm is \begin{equation} \lVert v \rVert _{H_{\ast }^{1/2}(\Gamma )} := \left \{ \langle v,w_{\mathrm {eq}} \rangle _{\Gamma }^{2} + \lvert v \rvert _{H^{1/2}(\Gamma )}^{2} \right \}^{1/2}. \end{equation} This norm is equivalent to the Sobolev norm for \(H^{1/2}(\Gamma )\). Define the following space \begin{equation} H_{\ast }^{1/2}(\Gamma ) := \{ v\in H^{1/2}(\Gamma ): \langle v,w_{\mathrm {eq}} \rangle _{\Gamma }=0 \}. \end{equation} It can be proved that using the above definition of inner product, \(H_{\ast }^{1/2}(\Gamma )\) is orthogonal to \(\mathrm {ker}(D)\). Therefore, \(D\) is \(H_{\ast }^{1/2}(\Gamma )\)-elliptic. The inner product is \begin{equation} ( u,v )_{H_{\ast \ast }^{1/2}(\Gamma )} := \langle u,1 \rangle _{\Gamma }\cdot \langle v,1 \rangle _{\Gamma } + \lvert u \rvert _{H^{1/2}(\Gamma )}\cdot \lvert v \rvert _{H^{1/2}(\Gamma )}. \end{equation} The corresponding norm is \begin{equation} \lVert v \rVert _{H_{\ast \ast }^{1/2}(\Gamma )} := \left \{ \langle v,1 \rangle _{\Gamma }^2 + \lvert v \rvert _{H^{1/2}(\Gamma )}^2 \right \}^{1/2}. \end{equation} This norm is equivalent to the Sobolev norm for \(H^{1/2}(\Gamma )\). Define the following space \begin{equation} H_{\ast \ast }^{1/2}(\Gamma ) := \{ v\in H^{1/2}(\Gamma ): \langle v,1 \rangle _{\Gamma }=0 \}. \end{equation} This space is orthogonal to \(\mathrm {ker}(D)\) and \(D\) is also \(H_{\ast \ast }^{1/2}(\Gamma )\)-elliptic. Let \(\Gamma _0\) be an open set contained in \(\Gamma \) and we consider functions on \(\Gamma _0\) instead of on the whole boundary, i.e. the function space is \(\tilde {H}^{1/2}(\Gamma _0)\). The inner product is \begin{equation} ( u,v )_{H^{1/2}(\Gamma ),\Gamma _0} := \lVert u \rVert _{L_2(\Gamma \backslash \Gamma _0)} \cdot \lVert v \rVert _{L_2(\Gamma \backslash \Gamma _0)} + \lvert u \rvert _{H^{1/2}(\Gamma )}\cdot \lvert v \rvert _{H^{1/2}(\Gamma )}. \end{equation} Here \(u\) and \(v\) belongs to \(H^{1/2}(\Gamma )\). When they are only defined on \(\Gamma _0\) and belong to \(\tilde {H}^{1/2}(\Gamma _0)\), we need to apply the canonical extension to them, i.e. extension by zero while preserving the resulted function within the space \(H^{1/2}(\Gamma )\). \begin{equation} \tilde {v} = \begin {cases} v(x) &amp; x\in \Gamma _0 \\ 0 &amp; x\in \Gamma \backslash \Gamma _0 \end {cases}. \end{equation} Then we define the inner product of \(u\) and \(v\) to be equal to \(( \tilde {u},\tilde {v} )_{H^{1/2}(\Gamma ),\Gamma _0}\). The corresponding norm is \begin{equation} \lVert v \rVert _{H^{1/2}(\Gamma ),\Gamma _0} := \left \{ \lVert v \rVert _{L_2(\Gamma \backslash \Gamma _0)}^2 + \lvert v \rvert _{H^{1/2}(\Gamma )}^2 \right \}^{1/2}. \end{equation} This norm is equivalent to the Sobolev norm for \(\tilde {H}^{1/2}(\Gamma _0)\). The space \(\tilde {H}^{1/2}(\Gamma _0)\) is orthogonal to \(\mathrm {ker}(D)\) and \(D\) is \(\tilde {H}^{1/2}(\Gamma _0)\)-elliptic. To prove the ellipticity of \(D\) either on the whole space or a subspace, the Sobolev norm should be used. The above definitions of norm play the role of scaffold during such a proof. Orthogonality defined based on different definitions of inner product lead to ellipticity of \(D\) with respect to different subspaces which are orthogonal to \(\mathrm {ker}(D)\) 5 Steklov-Poincaré operator The symmetric form of the Steklov-Poincaré operator is \begin{equation} S := D + (\sigma I + K') V^{-1} (\sigma I + K). \end{equation} \(V^{-1}\) is \(H^{1/2}(\Gamma )\)-elliptic and the ellipticity of \(S\) is the same as \(D\). References    Olaf Steinbach. Numerical Approximation Methods for Elliptic Boundary Value Problems: Finite and Boundary Elements. Springer Science &amp; Business Media. ISBN 978-0-387-31312-2.]]></summary></entry><entry><title type="html">Moore-Penrose pseudoinverse and generalized inverse</title><link href="https://jihuan-tian.github.io/math/2024/11/11/moore-penrose-pseudoinverse-and-generalized-inverse.html" rel="alternate" type="text/html" title="Moore-Penrose pseudoinverse and generalized inverse" /><published>2024-11-11T00:00:00+08:00</published><updated>2024-11-11T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/11/11/moore-penrose-pseudoinverse-and-generalized-inverse</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/11/11/moore-penrose-pseudoinverse-and-generalized-inverse.html"><![CDATA[<!-- l. 20 --><p class='noindent'>When a boundary integral operator \(B\) in BEM to be used as a preconditioner is not elliptic on its <span class='p1xb-x-x-109'>whole </span>domain,
such as the hypersingular operator \(D\), generalized inverse operator \(\dot {B}^{-1}\) is needed (at least theoretically), which is
spectrally equivalent to the original operator \(A\). In (<a href='#XSteinbachConstruction1998'>Steinbach and Wendland</a>), the preconditioning operator
<span class='footnote-mark'><a href='#fn1x0' id='fn1x0-bk'><sup class='textsuperscript'>1</sup></a></span><a id='x1-2f1'></a> is \(B: H^{s-2\alpha }(\Gamma ) \rightarrow H^s(\Gamma )\). Its
generalized inverse is \begin{equation}  \dot {B}^{-1}: V^{s,0}(\Gamma ,B) \rightarrow V^{s-2\alpha ,0}(\Gamma ,B).  \end{equation}<a id='x1-4r1'></a>
</p><!-- l. 25 --><p class='indent'>   Because the generalized inverse is an extension of the Moore-Penrose pseudoinverse, we’ll first introduce the
latter concept. We’ve already met pseudoinverse matrices in linear algebra. For a matrix equation \(Ax=b\), when \(A\) has full
column rank, it has a unique Moore-Penrose pseudoinverse matrix \begin{equation}  A^{\dagger } = (A^{\ast }A)^{-1}A^{\ast },  \end{equation}<a id='x1-5r2'></a> where \(A^{\ast }\) is the Hermite transpose of \(A\). \(A^{\dagger }\) satisfies
the four Penrose conditions (<a href='#XWangGeneralized2018'>Wang et al.</a>):
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-7x1'>
     <!-- l. 31 --><p class='noindent'>\(AA^{\dagger }A=A\)
     </p></li>
<li class='enumerate' id='x1-9x2'>
     <!-- l. 32 --><p class='noindent'>\(A^{\dagger }AA^{\dagger }=A^{\dagger }\)
     </p></li>
<li class='enumerate' id='x1-11x3'>
     <!-- l. 33 --><p class='noindent'>\((AA^{\dagger })^{\ast }=AA^{\dagger }\)
     </p></li>
<li class='enumerate' id='x1-13x4'>
     <!-- l. 34 --><p class='noindent'>\((A^{\dagger }A)^{\ast }=A^{\dagger }A\)</p></li></ol>
<!-- l. 36 --><p class='noindent'>From these conditions, we know that \(A^{\dagger }\) is just the left inverse of \(A\). According to our previous knowledge about the
<a href='/math/2024/11/03/kernel-and-range-of-a-matrix-and-its-transpose.html'>kernel and range spaces of a matrix</a>, when the matrix has full column rank, it is an injective map which should
have a left inverse.
</p><!-- l. 38 --><p class='indent'>   The basic idea behind Moore-Penrose pseudoinverse is simple. Assume \(A\) maps from \(V\) to \(W\). Let \(y\) belongs to \(W\) and
we want to find its pre-image \(x\) in \(V\) in the sense of pseudoinverse. When \(\mathrm {ker}(A)\) is not \(\{ 0 \}\), \(A\) is not surjective. So we first apply \(A^{\ast }\)
to \(y\), which maps \(y\) back into \(( \mathrm {ker}(A) )^{\perp }\). In this smaller subspace of \(V\), \(A^{\ast }A\) is bijective and the pre-image of \(A^{\ast }y\) can be found by
applying its inverse.
</p><!-- l. 40 --><p class='indent'>   Before the study on matrix pseudoinverse by Penrose, there had been research on the generalized inverse
of integral or differential operators by Hilbert, Fredholm et al. Let \(A\) be a bounded linear operator
from Hilbert space \(V\) to \(W\). The operator equation is \(Ax=b\), where \(x\in V\) and \(b\in W\). If the range \(\mathrm {Im}(A)\) of \(A\) is closed in \(W\), the
following generalized solutions are equivalent (<a href='#XWangGeneralized2018'>Wang et al.</a>), which are called the least square
solution:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-15x1'>
                                                                                               
                                                                                               
     <!-- l. 42 --><p class='noindent'>\(Ax=Pb\), where \(P\) is the projection operator maps onto \(\mathrm {Im}(A)\).
     </p></li>
<li class='enumerate' id='x1-17x2'>
     <!-- l. 43 --><p class='noindent'>\(\argmin _{x\in V} \lVert Ax-b \rVert _{W}\).
     </p></li>
<li class='enumerate' id='x1-19x3'>
     <!-- l. 44 --><p class='noindent'>\(A^{\ast }Ax=A^{\ast }b\).</p></li></ol>
<!-- l. 47 --><p class='indent'>   If we loosen the condition by assuming \(V\) and \(W\) are Banach spaces instead of Hilbert spaces, according to the
closed range theorem in (<a href='#XSteinbachNumerical2007'>Steinbach</a>, page 48), when \(A\) has a closed range, \(\mathrm {Im}(A)\) is the annihilator of the kernel of the
adjoint operator \(A': W' \rightarrow V'\), i.e. \begin{equation}  \mathrm {Im}(A)=(\mathrm {ker}(A'))^{\circ },  \end{equation}<a id='x1-20r3'></a> and for any \(y\in \mathrm {Im}(A)\) and \(x\in \mathrm {ker}(A')\), the duality pairing \(\langle y,x \rangle \) is zero. Because there are no inner product structures
on \(V\) and \(W\), we do not have the concepts of orthogonal complement space and Hilbert-adjoint anymore. Then the
above Moore-Penrose pseudoinverse cannot be used. But still the domain \(V\) of \(A\) can be decomposed as \begin{equation}  V = \mathrm {ker}(A) \oplus Z,  \end{equation}<a id='x1-21r4'></a> where \(Z\) is a
closed subspace of \(V\) such that \(\mathrm {ker}(A) \cap Z = \{ 0 \}\). If we restrict the domain of \(A\) to \(Z\), the map \(A\big \vert _Z: Z \rightarrow \mathrm {Im}(A)\) is bijective, which of course has an
inverse. If the range space \(W\) of \(A\) is decomposed as \begin{equation}  W = \mathrm {Im}(A) \oplus Y = (\mathrm {ker}(A'))^{\circ } \oplus Y,  \end{equation}<a id='x1-22r5'></a> the generalized inverse \(A^+\) of \(A\) can be defined as \begin{equation}  A^{+}(y) = \begin {cases} A\big \vert _Z^{-1}(y) &amp; y\in \mathrm {Im}(A) = (\mathrm {ker}(A'))^{\circ } \\ 0 &amp; y\in Y \end {cases}.  \end{equation}<a id='x1-23r6'></a> It
is easy to know that such generalized pseudoinverse only satisfies the first two Moore-Penrose
conditions:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-25x1'>
     <!-- l. 69 --><p class='noindent'>\(AA^{+}A=A\)
     </p></li>
<li class='enumerate' id='x1-27x2'>
     <!-- l. 70 --><p class='noindent'>\(A^{+}AA^{+}=A^{+}\)</p></li></ol>
   <h3 class='likesectionHead'><a id='x1-1000'></a>References</h3>
<!-- l. 1 --><p class='noindent'>
  </p><div class='thebibliography'>
  <p class='bibitem'><span class='biblabel'>
<a id='XSteinbachNumerical2007'></a><span class='bibsp'>   </span></span>Olaf  Steinbach.    <span class='p1xi-x-x-109'>Numerical  Approximation  Methods  for  Elliptic  Boundary  Value  Problems:  Finite  and
  </span><span class='p1xi-x-x-109'>Boundary Elements</span>. Springer Science &amp; Business Media. ISBN 978-0-387-31312-2.
  </p>
  <p class='bibitem'><span class='biblabel'>
<a id='XSteinbachConstruction1998'></a><span class='bibsp'>   </span></span>Olaf               Steinbach               and               Wolfgang L.               Wendland.                                      The
  construction of some efficient preconditioners in the boundary element method.  9(1-2):191–216.  URL
  <a class='url' href='http://link.springer.com/article/10.1023/A:1018937506719'><span class='t1xtt-x-x-109'>http://link.springer.com/article/10.1023/A:1018937506719</span></a>.
                                                                                               
                                                                                               
  </p>
  <p class='bibitem'><span class='biblabel'>
<a id='XWangGeneralized2018'></a><span class='bibsp'>   </span></span>Wang,  Yimin  Wei,  and  Sanzheng  Qiao.   <span class='p1xi-x-x-109'>Generalized  Inverses:  Theory  and  Computations</span>,  volume 53
  of  <span class='p1xi-x-x-109'>Developments  in  Mathematics</span>.     Springer.     ISBN  9789811301452  9789811301469.     doi:  10.1007/
  978-981-13-0146-9.
</p>
  </div>
   <div class='footnotes'><a id='x1-3x'></a>
<!-- l. 20 --><p class='indent'>      <span class='footnote-mark'><a href='#fn1x0-bk' id='fn1x0'><sup class='textsuperscript'>1</sup></a></span><span class='p1xr-x-x-90'>Here we explicitly say “preconditioning operator” not simply “preconditioner”, because we want to distinguish it from
</span><span class='p1xr-x-x-90'>“preconditioning matrix”. While a preconditioning operator such as </span>\(B\) <span class='p1xr-x-x-90'>is an approximate inverse of the original operator </span>\(A\)<span class='p1xr-x-x-90'>, a
</span><span class='p1xr-x-x-90'>preconditioning matrix is the discretized Galerkin matrix associated with </span>\(\dot {B}^{-1}\)<span class='p1xr-x-x-90'>, not </span>\(B\)<span class='p1xr-x-x-90'>. To apply a preconditioning matrix to a discretized linear
</span><span class='p1xr-x-x-90'>system, we need to multiply its approximate inverse matrix to both sides of the equation. For simplicity, we will say “preconditioner”
</span><span class='p1xr-x-x-90'>instead of “preconditioning operator” from now on.</span></p>                                                                                                                                  </div>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="BEM" /><summary type="html"><![CDATA[When a boundary integral operator \(B\) in BEM to be used as a preconditioner is not elliptic on its whole domain, such as the hypersingular operator \(D\), generalized inverse operator \(\dot {B}^{-1}\) is needed (at least theoretically), which is spectrally equivalent to the original operator \(A\). In (Steinbach and Wendland), the preconditioning operator 1 is \(B: H^{s-2\alpha }(\Gamma ) \rightarrow H^s(\Gamma )\). Its generalized inverse is \begin{equation} \dot {B}^{-1}: V^{s,0}(\Gamma ,B) \rightarrow V^{s-2\alpha ,0}(\Gamma ,B). \end{equation} Because the generalized inverse is an extension of the Moore-Penrose pseudoinverse, we’ll first introduce the latter concept. We’ve already met pseudoinverse matrices in linear algebra. For a matrix equation \(Ax=b\), when \(A\) has full column rank, it has a unique Moore-Penrose pseudoinverse matrix \begin{equation} A^{\dagger } = (A^{\ast }A)^{-1}A^{\ast }, \end{equation} where \(A^{\ast }\) is the Hermite transpose of \(A\). \(A^{\dagger }\) satisfies the four Penrose conditions (Wang et al.): \(AA^{\dagger }A=A\) \(A^{\dagger }AA^{\dagger }=A^{\dagger }\) \((AA^{\dagger })^{\ast }=AA^{\dagger }\) \((A^{\dagger }A)^{\ast }=A^{\dagger }A\) From these conditions, we know that \(A^{\dagger }\) is just the left inverse of \(A\). According to our previous knowledge about the kernel and range spaces of a matrix, when the matrix has full column rank, it is an injective map which should have a left inverse. The basic idea behind Moore-Penrose pseudoinverse is simple. Assume \(A\) maps from \(V\) to \(W\). Let \(y\) belongs to \(W\) and we want to find its pre-image \(x\) in \(V\) in the sense of pseudoinverse. When \(\mathrm {ker}(A)\) is not \(\{ 0 \}\), \(A\) is not surjective. So we first apply \(A^{\ast }\) to \(y\), which maps \(y\) back into \(( \mathrm {ker}(A) )^{\perp }\). In this smaller subspace of \(V\), \(A^{\ast }A\) is bijective and the pre-image of \(A^{\ast }y\) can be found by applying its inverse. Before the study on matrix pseudoinverse by Penrose, there had been research on the generalized inverse of integral or differential operators by Hilbert, Fredholm et al. Let \(A\) be a bounded linear operator from Hilbert space \(V\) to \(W\). The operator equation is \(Ax=b\), where \(x\in V\) and \(b\in W\). If the range \(\mathrm {Im}(A)\) of \(A\) is closed in \(W\), the following generalized solutions are equivalent (Wang et al.), which are called the least square solution: \(Ax=Pb\), where \(P\) is the projection operator maps onto \(\mathrm {Im}(A)\). \(\argmin _{x\in V} \lVert Ax-b \rVert _{W}\). \(A^{\ast }Ax=A^{\ast }b\). If we loosen the condition by assuming \(V\) and \(W\) are Banach spaces instead of Hilbert spaces, according to the closed range theorem in (Steinbach, page 48), when \(A\) has a closed range, \(\mathrm {Im}(A)\) is the annihilator of the kernel of the adjoint operator \(A': W' \rightarrow V'\), i.e. \begin{equation} \mathrm {Im}(A)=(\mathrm {ker}(A'))^{\circ }, \end{equation} and for any \(y\in \mathrm {Im}(A)\) and \(x\in \mathrm {ker}(A')\), the duality pairing \(\langle y,x \rangle \) is zero. Because there are no inner product structures on \(V\) and \(W\), we do not have the concepts of orthogonal complement space and Hilbert-adjoint anymore. Then the above Moore-Penrose pseudoinverse cannot be used. But still the domain \(V\) of \(A\) can be decomposed as \begin{equation} V = \mathrm {ker}(A) \oplus Z, \end{equation} where \(Z\) is a closed subspace of \(V\) such that \(\mathrm {ker}(A) \cap Z = \{ 0 \}\). If we restrict the domain of \(A\) to \(Z\), the map \(A\big \vert _Z: Z \rightarrow \mathrm {Im}(A)\) is bijective, which of course has an inverse. If the range space \(W\) of \(A\) is decomposed as \begin{equation} W = \mathrm {Im}(A) \oplus Y = (\mathrm {ker}(A'))^{\circ } \oplus Y, \end{equation} the generalized inverse \(A^+\) of \(A\) can be defined as \begin{equation} A^{+}(y) = \begin {cases} A\big \vert _Z^{-1}(y) &amp; y\in \mathrm {Im}(A) = (\mathrm {ker}(A'))^{\circ } \\ 0 &amp; y\in Y \end {cases}. \end{equation} It is easy to know that such generalized pseudoinverse only satisfies the first two Moore-Penrose conditions: \(AA^{+}A=A\) \(A^{+}AA^{+}=A^{+}\) References    Olaf Steinbach. Numerical Approximation Methods for Elliptic Boundary Value Problems: Finite and Boundary Elements. Springer Science &amp; Business Media. ISBN 978-0-387-31312-2.    Olaf Steinbach and Wolfgang L. Wendland. The construction of some efficient preconditioners in the boundary element method. 9(1-2):191–216. URL http://link.springer.com/article/10.1023/A:1018937506719.    Wang, Yimin Wei, and Sanzheng Qiao. Generalized Inverses: Theory and Computations, volume 53 of Developments in Mathematics. Springer. ISBN 9789811301452 9789811301469. doi: 10.1007/ 978-981-13-0146-9. 1Here we explicitly say “preconditioning operator” not simply “preconditioner”, because we want to distinguish it from “preconditioning matrix”. While a preconditioning operator such as \(B\) is an approximate inverse of the original operator \(A\), a preconditioning matrix is the discretized Galerkin matrix associated with \(\dot {B}^{-1}\), not \(B\). To apply a preconditioning matrix to a discretized linear system, we need to multiply its approximate inverse matrix to both sides of the equation. For simplicity, we will say “preconditioner” instead of “preconditioning operator” from now on.]]></summary></entry><entry><title type="html">Adjoint operators in functional analysis</title><link href="https://jihuan-tian.github.io/math/2024/11/10/adjoint-operators-in-functional-analysis.html" rel="alternate" type="text/html" title="Adjoint operators in functional analysis" /><published>2024-11-10T00:00:00+08:00</published><updated>2024-11-10T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/11/10/adjoint-operators-in-functional-analysis</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/11/10/adjoint-operators-in-functional-analysis.html"><![CDATA[<h3 class='likesectionHead'><a id='x1-1000'></a>Contents</h3>
   <div class='tableofcontents'>
    <span class='sectionToc'>1 <a href='#x1-20001' id='QQ2-1-2'>Basic definitions of adjoint operators</a></span>
<br />    <span class='sectionToc'>2 <a href='#x1-30002' id='QQ2-1-3'>Relationship between adjoint and Hilbert-adjoint in Hilbert spaces</a></span>
<br />    <span class='sectionToc'>3 <a href='#x1-40003' id='QQ2-1-5'>Self-adjointness</a></span>
<br />    <span class='sectionToc'>4 <a href='#x1-50004' id='QQ2-1-6'>Adjoint operators in discrete case</a></span>
<br />    <span class='sectionToc'>5 <a href='#x1-60005' id='QQ2-1-7'>Clarification</a></span>
   </div>
<!-- l. 24 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>1    </span> <a id='x1-20001'></a>Basic definitions of adjoint operators</h3>
<!-- l. 26 --><p class='noindent'>There are two types of adjoint operators in functional analysis.
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-2002x1'>
     <!-- l. 28 --><p class='noindent'>Adjoint operator in normed spaces
     </p><!-- l. 30 --><p class='noindent'>Let \(X\) and \(Y\) be two normed spaces. \(X'\) and \(Y'\) are their dual spaces respectively. Let \(A: X \rightarrow Y\) be a bounded linear operator.
     Then for all \(x\in X\) and \(y\in Y'\), the adjoint operator \(A': Y' \rightarrow X'\) of \(A\) should satisfy \begin{equation}  \langle Ax,y \rangle _{Y,Y'} = \langle x,A'y \rangle _{X,X'},  \end{equation}<a id='x1-2003r1'></a> where \(\langle \cdot ,\cdot \rangle _{Y,Y'}\) is the duality pairing between \(Y\)
     and \(Y'\) and \(\langle \cdot ,\cdot \rangle _{X,X'}\) is the duality pairing between \(X\) and \(X'\). Usually, the subscript can be omitted \begin{equation}  \label {eq:dual-adjoint} \langle Ax,y \rangle = \langle x,A'y \rangle  \end{equation}<a id='x1-2004r2'></a> and
     the order of the two operands in \(\langle \cdot ,\cdot \rangle \) does not matter. In (<a href='#XYosidaFunctional1995'>Yosida</a>, page 193), \(A'\) is also called dual
     operator.
     </p><!-- l. 41 --><p class='noindent'>A duality pairing means applying a linear functional to a function and producing a scalar value. Therefore,
     the above equation can be written as \begin{equation}  (Ax)(y) = y(Ax) = x(A'y) = (A'y)(x).  \end{equation}<a id='x1-2005r3'></a> Whether we apply for example \(Ax\) to \(y\) or apply \(y\) to \(Ax\) does not matter. In
     analogy with C/C++ programming, we consider an object in the primal space \(X\) or \(Y\) as the data to be
     manipulated and an object in the dual space \(X'\) or \(Y'\) as a function. The behavior of the duality pairing depends
                                                                                               
                                                                                               
     on how such a function in the dual space is defined. A typical example is the boundary integral operator.
     Let \(V\) be the boundary integral operator related to the single layer potential. We usually consider its input
     function \(u(y)\) to be in the primal space and \(V\) itself in the dual space. The output of \(Vu\) is another
     function: \begin{equation}  ( Vu(y) )(x) = \int _{\Gamma } k(x,y) u(y) ds_y.  \end{equation}<a id='x1-2006r4'></a> Therefore, as a function or operation, \(V(\cdot ) = \int _{\Gamma } k(x,y) (\cdot ) ds_y\), which can be considered as a function object in
     C++. Its behavior is fully determined by the kernel function \(k(x,y)\) and the definite integration
     operation.
     </p></li>
<li class='enumerate' id='x1-2008x2'>
     <!-- l. 51 --><p class='noindent'>Adjoint operator in Hilbert spaces (<a href='#XKreyszigIntroductory1978'>Kreyszig</a>, page 196)
     </p><!-- l. 53 --><p class='noindent'>Let \(X\) and \(Y\) be two Hilbert spaces and \(A: X \rightarrow Y\) be a bounded linear operator. For all \(x\in X\) and \(y\in Y\), the Hilbert-adjoint operator
     \(A^{\ast }: Y \rightarrow X\) of \(A\) should satisfy \begin{equation}  \label {eq:hilbert-adjoint} \langle Ax,y \rangle _Y = \langle x,A^{\ast }y \rangle _X,  \end{equation}<a id='x1-2009r5'></a> where \(\langle \cdot ,\cdot \rangle _X\) is the inner product in \(X\) and \(\langle \cdot ,\cdot \rangle _Y\) is the inner product in \(Y\). Usually, the above
     condition is simply written as \begin{equation}  \langle Ax,y \rangle = \langle x,A^{\ast }y \rangle .  \end{equation}<a id='x1-2010r6'></a> Such Hilbert-adjoint operator \(A^{\ast }\) uniquely exists and satisfies
     \(\lVert A^{\ast } \rVert = \lVert A \rVert \).
     </p><!-- l. 64 --><p class='noindent'>We should also note that even though \(A^{\ast }\) maps from \(Y\) to \(X\), it is not the inverse operator \(A^{-1}: Y \rightarrow X\) of \(A\).</p></li></ol>
<!-- l. 67 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>2    </span> <a id='x1-30002'></a>Relationship between adjoint and Hilbert-adjoint in Hilbert spaces</h3>
<!-- l. 69 --><p class='noindent'>Because a Hilbert space is also a normed space, when \(X\) and \(Y\) are both Hilbert spaces, the operator \(A: X \rightarrow Y\) has both adjoint
operator \(A'\) and Hilbert-adjoint operator \(A^{\ast }\). Their relationship can be visualized in the following diagram.
</p><figure class='figure'> 

                                                                                               
                                                                                               
<a id='x1-3001r1'></a>
                                                                                               
                                                                                               
<!-- l. 75 --><p class='noindent'><img alt='PIC' src='/figures/2024-11-10_21-32-51-adjoint-and-hilbert-adjoint.png'  />
</p>
<figcaption class='caption'><span class='id'>Figure 1: </span><span class='content'>Diagram for adjoint and Hilbert adjoint operators</span></figcaption><!-- tex4ht:label?: x1-3001r1  -->
                                                                                               
                                                                                               
   </figure>
<!-- l. 78 --><p class='indent'>   According to Riesz representation theorem (see <a href='/math/2024/06/19/fundamental-theorems-in-pde-theory.html#org60d4829'>Riesz representation theorem</a> and <a href='/math/2024/07/11/understanding-about-riesz-map.html#x1-20001'>Riesz representation
theorem (Brezis)</a>), \(J_X\) is the Riesz map from \(X'\) and \(X\) and \(J_Y\) is the Riesz map from \(Y'\) to \(Y\) (see
<a href='/math/2024/07/11/understanding-about-riesz-map.html#x1-60003'>Construction of the Riesz map as a space adaptor</a>). Then we have \begin{equation}  A^{\ast } = J_X A' J_Y^{-1}.  \end{equation}<a id='x1-3002r7'></a>
</p>
   <h3 class='sectionHead'><span class='titlemark'>3    </span> <a id='x1-40003'></a>Self-adjointness</h3>
<!-- l. 84 --><p class='noindent'>When in the context of normed spaces, the operator \(A: X \rightarrow Y\) is self-adjoint, if \(A': Y' \rightarrow X'\) is equal to \(A\). This requires \(X = Y'\) and \(Y = X'\), which
indicates \(X\) and \(Y\) are dual to each other.
</p><!-- l. 86 --><p class='indent'>   When in the context of Hilbert spaces, the operator \(A: X \rightarrow Y\) is self-adjoint, if \(A^{\ast }: Y \rightarrow X\) is equal to \(A\). This requires \(X\) and \(Y\) to be a
same Hilbert space.
</p><!-- l. 88 --><p class='indent'>   <span class='p1xb-x-x-109'>Example </span>In (<a href='#XSteinbachNumerical2007'>Steinbach</a>) and (<a href='#XSteinbachConstruction1998'>Steinbach and Wendland</a>), the boundary integral operators \(V: H^{-1/2}(\Gamma ) \rightarrow H^{1/2}(\Gamma )\) and \(D: H^{1/2}(\Gamma ) \rightarrow H^{-1/2}(\Gamma )\) are self-adjoint.
Because the Sobolev spaces \(H^{-1/2}(\Gamma )\) and \(H^{1/2}(\Gamma )\) are dual to each other, such self-adjointness is in the sense of normed spaces.
Meanwhile, even though both \(H^{-1/2}(\Gamma )\) and \(H^{1/2}(\Gamma )\) are Hilbert spaces, they are not a same space. Therefore, the said
self-adjointness is not in the sense of Hilbert spaces.
</p>
   <h3 class='sectionHead'><span class='titlemark'>4    </span> <a id='x1-50004'></a>Adjoint operators in discrete case</h3>
<!-- l. 91 --><p class='noindent'>Let \(X\) be \(\mathbb {K}^{n}\) and \(Y\) be \(\mathbb {K}^m\), where \(\mathbb {K}\) can be \(\mathbb {R}\) or \(\mathcal {C}\). Let \(x\in X\), \(y\in Y\) and \(\tilde {y}\in Y'\). \(\tilde {y}\) is the dual vector associated with \(y\). In the language
of differential geometry, \(y\) is a tangent vector and \(\tilde {y}\) is the corresponding cotangent vector. When \(\mathbb {K}=\mathbb {R}\), \begin{equation}  \tilde {y}_i = g_{ij} y^j,  \end{equation}<a id='x1-5001r8'></a>
where \(g_{ij}\) is the metric tensor. Because \(X\) and \(Y\) are Cartesian spaces with orthonormal bases, \(g_{ij}=\delta _{ij}\). Hence
\(\tilde {y}_i = y^i\).
</p><!-- l. 97 --><p class='indent'>   When \(\mathbb {K}=\mathbb {C}\), there is an additional complex conjugate: \begin{equation}  \tilde {y}_i = \overline {g_{ij} y^j} = \overline {y^i}.  \end{equation}<a id='x1-5002r9'></a> As a convention, in the discrete form, we use a column
vector to represent a tangent vector and a row vector to represent a cotangent vector. Therefore, \begin{equation}  \tilde {y} = y^{\mathrm {H}},  \end{equation}<a id='x1-5003r10'></a> where \((\cdot )^{\mathrm {H}}\) is the
Hermitian transpose.
</p><!-- l. 107 --><p class='indent'>   If we treat \(X\) and \(Y\) as normed spaces, the left hand side of the adjoint condition in Equation <a href='#x1-2004r2'>2<!-- tex4ht:ref: eq:dual-adjoint  --></a> is a duality pairing
between a tangent vector and a cotangent vector. This operation is just taking the sum of the coefficient-wise
product of the two vectors, i.e. \begin{equation}  \langle Ax,\tilde {y} \rangle = \sum _i \left ( \sum _j \overline {g_{ij}y^j} \right ) (Ax)^i = \sum _i \overline {y^i} (Ax)^i = y^{\mathrm {H}} A x.  \end{equation}<a id='x1-5004r11'></a> The right hand side is \begin{equation}  \langle x,A'\tilde {y} \rangle = \sum _i \left ( A' \tilde {y}^{\mathrm {T}} \right ) x^i = \sum _i \left ( A' \overline {y} \right ) x^i = (A' \overline {y})^{\mathrm {T}} x = y^{\mathrm {H}} A'^{\mathrm {T}} x.  \end{equation}<a id='x1-5005r12'></a> Therefore the adjoint operator is the transpose of the
original operator, i.e. \(A' = A^{\mathrm {T}}\).
</p><!-- l. 121 --><p class='indent'>   If we treat \(X\) and \(Y\) as Hilbert spaces, the left hand side of the adjoint condition in Equation <a href='#x1-2009r5'>5<!-- tex4ht:ref: eq:hilbert-adjoint  --></a> is \begin{equation}  \langle Ax,y \rangle = \sum _{i,j} g_{ij} ( Ax )^i \overline {y^j} = y^{\mathrm {H}} Ax.  \end{equation}<a id='x1-5006r13'></a> Here we
remember that in differential geometry, the inner product of two vectors in a same space involves the metric
tensor. When \(\mathbb {K}=\mathbb {C}\), there should also be a complex conjugate on the second operand.
</p><!-- l. 128 --><p class='indent'>   The right hand side of the adjoint condition is \begin{equation}  \langle x,A^{\ast }y \rangle = \sum _{i,j} g_{ij} x^i \overline {( A^{\ast }y )^j} = \sum _i x^i \overline {(A^{\ast }y)^i} = y^{\mathrm {H}} (A^{\ast })^{\mathrm {H}} x.  \end{equation}<a id='x1-5007r14'></a> Therefore, the Hilbert-adjoint operator is the Hermitian
transpose of the original operator, i.e. \(A^{\ast } = A^{\mathrm {H}}\).
</p>
   <h3 class='sectionHead'><span class='titlemark'>5    </span> <a id='x1-60005'></a>Clarification</h3>
<!-- l. 136 --><p class='noindent'>From above, we can see the phenomenon of terminology abuse (e.g. adjoint, self-adjoint) or symbol abuse (e.g. \(\langle \cdot ,\cdot \rangle \))
in mathematics. Therefore, we clarify or recapitulate the following points. </p>
     <ul class='itemize1'>
     <li class='itemize'>
                                                                                               
                                                                                               
     <!-- l. 138 --><p class='noindent'>When the duality pairing between two different spaces are concerned in the context, adjoint operator
     in normed spaces is used.
     </p></li>
     <li class='itemize'>
     <!-- l. 139 --><p class='noindent'>When  we  see  Hilbert  spaces  <span class='p1xb-x-x-109'>and  </span>their  inner  product  definitions  are  involved  in  the  context,
     Hilbert-adjoint operator is used.
     </p></li>
     <li class='itemize'>
     <!-- l. 140 --><p class='noindent'>The actually meaning of \(\langle \cdot ,\cdot \rangle \) should be determined based on the spaces to which its two operands belong.
     </p>
          <ul class='itemize2'>
          <li class='itemize'>
          <!-- l. 142 --><p class='noindent'>If the two operands are in two different spaces, one space must be the dual space of the other
          and \(\langle \cdot ,\cdot \rangle \) is the duality pairing. Then adjoint operator will be discussed in this context.
          </p></li>
          <li class='itemize'>
          <!-- l. 143 --><p class='noindent'>If the two operands are in a same space, \(\langle \cdot ,\cdot \rangle \) is the inner product in this space. Then Hilbert-adjoint
          operator will be discussed in this context.</p></li></ul>
     </li></ul>
<!-- l. 1 --><p class='noindent'>
</p>
   <h3 class='likesectionHead'><a id='x1-7000'></a>References</h3>
<!-- l. 1 --><p class='noindent'>
  </p><div class='thebibliography'>
  <p class='bibitem'><span class='biblabel'>
<a id='XKreyszigIntroductory1978'></a><span class='bibsp'>   </span></span>Erwin Kreyszig. <span class='p1xi-x-x-109'>Introductory Functional Analysis with Applications</span>. Wiley. ISBN 978-0-471-50731-4.
  </p>
  <p class='bibitem'><span class='biblabel'>
<a id='XSteinbachNumerical2007'></a><span class='bibsp'>   </span></span>Olaf  Steinbach.    <span class='p1xi-x-x-109'>Numerical  Approximation  Methods  for  Elliptic  Boundary  Value  Problems:  Finite  and
  </span><span class='p1xi-x-x-109'>Boundary Elements</span>. Springer Science &amp; Business Media. ISBN 978-0-387-31312-2.
  </p>
  <p class='bibitem'><span class='biblabel'>
<a id='XSteinbachConstruction1998'></a><span class='bibsp'>   </span></span>Olaf               Steinbach               and               Wolfgang L.               Wendland.                                      The
  construction of some efficient preconditioners in the boundary element method.  9(1-2):191–216.  URL
  <a class='url' href='http://link.springer.com/article/10.1023/A:1018937506719'><span class='t1xtt-x-x-109'>http://link.springer.com/article/10.1023/A:1018937506719</span></a>.
  </p>
                                                                                               
                                                                                               
  <p class='bibitem'><span class='biblabel'>
<a id='XYosidaFunctional1995'></a><span class='bibsp'>   </span></span>Kôsaku  Yosida.    <span class='p1xi-x-x-109'>Functional  Analysis</span>,  volume  123  of  <span class='p1xi-x-x-109'>Classics  in  Mathematics</span>.    Springer.    ISBN
  978-3-540-58654-8 978-3-642-61859-8. doi: 10.1007/978-3-642-61859-8.
</p>
  </div>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="functional-analysis" /><summary type="html"><![CDATA[Contents  1 Basic definitions of adjoint operators  2 Relationship between adjoint and Hilbert-adjoint in Hilbert spaces  3 Self-adjointness  4 Adjoint operators in discrete case  5 Clarification 1 Basic definitions of adjoint operators There are two types of adjoint operators in functional analysis. Adjoint operator in normed spaces Let \(X\) and \(Y\) be two normed spaces. \(X'\) and \(Y'\) are their dual spaces respectively. Let \(A: X \rightarrow Y\) be a bounded linear operator. Then for all \(x\in X\) and \(y\in Y'\), the adjoint operator \(A': Y' \rightarrow X'\) of \(A\) should satisfy \begin{equation} \langle Ax,y \rangle _{Y,Y'} = \langle x,A'y \rangle _{X,X'}, \end{equation} where \(\langle \cdot ,\cdot \rangle _{Y,Y'}\) is the duality pairing between \(Y\) and \(Y'\) and \(\langle \cdot ,\cdot \rangle _{X,X'}\) is the duality pairing between \(X\) and \(X'\). Usually, the subscript can be omitted \begin{equation} \label {eq:dual-adjoint} \langle Ax,y \rangle = \langle x,A'y \rangle \end{equation} and the order of the two operands in \(\langle \cdot ,\cdot \rangle \) does not matter. In (Yosida, page 193), \(A'\) is also called dual operator. A duality pairing means applying a linear functional to a function and producing a scalar value. Therefore, the above equation can be written as \begin{equation} (Ax)(y) = y(Ax) = x(A'y) = (A'y)(x). \end{equation} Whether we apply for example \(Ax\) to \(y\) or apply \(y\) to \(Ax\) does not matter. In analogy with C/C++ programming, we consider an object in the primal space \(X\) or \(Y\) as the data to be manipulated and an object in the dual space \(X'\) or \(Y'\) as a function. The behavior of the duality pairing depends on how such a function in the dual space is defined. A typical example is the boundary integral operator. Let \(V\) be the boundary integral operator related to the single layer potential. We usually consider its input function \(u(y)\) to be in the primal space and \(V\) itself in the dual space. The output of \(Vu\) is another function: \begin{equation} ( Vu(y) )(x) = \int _{\Gamma } k(x,y) u(y) ds_y. \end{equation} Therefore, as a function or operation, \(V(\cdot ) = \int _{\Gamma } k(x,y) (\cdot ) ds_y\), which can be considered as a function object in C++. Its behavior is fully determined by the kernel function \(k(x,y)\) and the definite integration operation. Adjoint operator in Hilbert spaces (Kreyszig, page 196) Let \(X\) and \(Y\) be two Hilbert spaces and \(A: X \rightarrow Y\) be a bounded linear operator. For all \(x\in X\) and \(y\in Y\), the Hilbert-adjoint operator \(A^{\ast }: Y \rightarrow X\) of \(A\) should satisfy \begin{equation} \label {eq:hilbert-adjoint} \langle Ax,y \rangle _Y = \langle x,A^{\ast }y \rangle _X, \end{equation} where \(\langle \cdot ,\cdot \rangle _X\) is the inner product in \(X\) and \(\langle \cdot ,\cdot \rangle _Y\) is the inner product in \(Y\). Usually, the above condition is simply written as \begin{equation} \langle Ax,y \rangle = \langle x,A^{\ast }y \rangle . \end{equation} Such Hilbert-adjoint operator \(A^{\ast }\) uniquely exists and satisfies \(\lVert A^{\ast } \rVert = \lVert A \rVert \). We should also note that even though \(A^{\ast }\) maps from \(Y\) to \(X\), it is not the inverse operator \(A^{-1}: Y \rightarrow X\) of \(A\). 2 Relationship between adjoint and Hilbert-adjoint in Hilbert spaces Because a Hilbert space is also a normed space, when \(X\) and \(Y\) are both Hilbert spaces, the operator \(A: X \rightarrow Y\) has both adjoint operator \(A'\) and Hilbert-adjoint operator \(A^{\ast }\). Their relationship can be visualized in the following diagram.]]></summary></entry><entry><title type="html">Spectral equivalence</title><link href="https://jihuan-tian.github.io/math/2024/11/06/spectral-equivalence.html" rel="alternate" type="text/html" title="Spectral equivalence" /><published>2024-11-06T00:00:00+08:00</published><updated>2024-11-06T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/11/06/spectral-equivalence</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/11/06/spectral-equivalence.html"><![CDATA[<!-- l. 24 --><p class='indent'>   For a bounded bilinear form \(a ( \cdot ,\cdot ): X\times X \rightarrow \mathbb {R}\) where \(X\) is a Hilbert space, it has an associated bounded linear operator \(A: X \rightarrow X'\) which
satisfies \(a(u,v)=\langle Au,v \rangle \) for all \(u\) and \(v\) in \(X\). In other words, the behavior of the bilinear form \(a(\cdot ,\cdot )\) as a binary function (usually the operand is
also a function) can be fully determined by the linear operator \(A\) as a unary function plus a definition of the duality
pairing <span class='footnote-mark'><a href='#fn1x0' id='fn1x0-bk'><sup class='textsuperscript'>1</sup></a></span><a id='x1-1001f1'></a>.
Because of the boundedness of \(a(\cdot ,\cdot )\), there exists a positive constant \(c_2^A\) such that \begin{equation}  a ( u,u ) = \langle Au,u \rangle \leq c_2^A \lVert u \rVert _X^2 \quad \forall u\in X.  \end{equation}<a id='x1-1003r1'></a> Meanwhile, if \(A\) is \(X\)-elliptic, there exists a
positive constant \(c_1^A\) such that \begin{equation}  \langle Au,u \rangle \geq c_1^A \lVert u \rVert _X^2 \quad \forall u\in X.  \end{equation}<a id='x1-1004r2'></a> We can consider \(a(u,u)\) or \(\langle Au,u \rangle \) as an energy norm for \(u\) (<a href='#XBrennerMathematical2009'>Brenner</a>, page 5) (see also
<a href='/math/2024/06/21/understanding-about-energy-norm-in-galerkin-method.html'>Understanding about energy norm used in Galerkin method</a>), while \(a(u,u)/\lVert u \rVert _X^2\) or \(\langle Au,u \rangle /\lVert u \rVert _X^2\) as a normalized energy norm. Therefore,
\([c_1^A,c_2^A]\) is the range of this normalized energy norm, which is also the range of the spectrum of the operator
\(A\).
</p><!-- l. 34 --><p class='indent'>   If we consider a finite dimensional subspace \(V_h\) of \(V\), the corresponding function for \(u\) becomes \(u_h\).
Let \(\{ \varphi _k \}_{k=1}^N\) be the basis of \(V_h\), then the function \(u_h\) has this expansion \begin{equation}  u_h=\sum _{k=1}^N u_k\varphi _k.  \end{equation}<a id='x1-1005r3'></a> and can be discretized into a vector
\(\underline {u}\in \mathbb {R}^N\). The bilinear form associated with \(A\) can be discretized into a matrix \(\mathcal {A}\), whose coefficient is \begin{equation}  \mathcal {A}_{ij}=\langle A\varphi _j,\varphi _i \rangle .  \end{equation}<a id='x1-1006r4'></a> \(\mathcal {A}\) has
similar boundedness and ellipticity properties as the operator \(A\): \begin{equation}  \tilde {c}_1^A \lVert \underline {u} \rVert ^{2} \leq \underline {u}^{\mathrm {T}}\mathcal {A}\underline {u} \leq \tilde {c}_2^A \lVert \underline {u} \rVert ^{2} \quad \forall \underline {u}\in \mathbb {R}^N.  \end{equation}<a id='x1-1007r5'></a> According to <a href='/math/2024/06/07/understanding-about-ellipticity-of-operators.html'>Understanding about
ellipticity of operators</a>, \(\tilde {c}_1^A\) and \(\tilde {c}_2^A\) correspond to the minimum and maximum eigenvalues of \(\mathcal {A}\) and the range \([\tilde {c}_1^A,\tilde {c}_2^A]\)
is the spectrum. Because \(\mathcal {A}\) is a finite dimensional approximation to \(A\), it lies in a smaller space and
we must have \(c_1^A\leq \tilde {c}_1^A\) and \(\tilde {c}_2^A\leq c_2^A\). Therefore, the matrix spectrum is contained within the operator spectrum,
\([\tilde {c}_1^A,\tilde {c}_2^A] \subset [c_1^A,c_2^A]\).
</p><!-- l. 50 --><p class='indent'>   If \(A^{-1}: X' \rightarrow X\) is the inverse operator of \(A\), its operator spectrum is the reciprocal of the spectrum for \(A\), i.e. \([\frac {1}{c_2^A},\frac {1}{c_1^A}]\). We
should have \begin{equation}  \frac {1}{c_2^A}\lVert v \rVert _{X'}^{2} \leq \langle A^{-1}v,v \rangle \leq \frac {1}{c_1^A} \lVert v \rVert _{X'}^2 \quad \forall v\in X'.  \end{equation}<a id='x1-1008r6'></a> The discrete version is \begin{equation}  \tilde {c}_1^{A^{-1}} \lVert \underline {v} \rVert ^2 \leq \underline {v}^{\mathrm {T}} \underline {A}^{-1} \underline {v} \leq \tilde {c}_2^{A^{-1}} \lVert \underline {v} \rVert ^2 \quad \forall \underline {v}\in \mathbb {R}^M.  \end{equation}<a id='x1-1009r7'></a> <span class='p1xb-x-x-109'>N.B. The discretized matrix </span>\(\underline {A}^{-1}\) <span class='p1xb-x-x-109'>for </span>\(A^{-1}\) <span class='p1xb-x-x-109'>and the inverse matrix
</span><span class='p1xb-x-x-109'>of the discretized matrix </span>\(\mathcal {A}^{-1}\) <span class='p1xb-x-x-109'>for </span>\(A\) <span class='p1xb-x-x-109'>are two different things. </span>(see <a href='/math/2024/07/24/understanding-about-operator-discretization.html#x1-30002'>Discretization of the inverse of an
operator</a>)
</p><!-- l. 60 --><p class='indent'>   Let \(B\) be the preconditioner for \(A\). Its inverse operator \(B^{-1}: X \rightarrow X'\) should be spectrally equivalent to \(A\): \begin{equation}  \gamma _1 \langle B^{-1}u,u \rangle \leq \langle Au,u \rangle \leq \gamma _2 \langle B^{-1}u,u \rangle \quad \forall u\in V.  \end{equation}<a id='x1-1010r8'></a> If the spectrum of \(B\) is \([c_1^B,c_2^B]\),
the spectrum of its inverse is \([\frac {1}{c_2^B},\frac {1}{c_1^B}]\). Then we have \begin{equation}  \gamma _1=c_1^A c_1^B, \gamma _2=c_2^A c_2^B.  \end{equation}<a id='x1-1011r9'></a>
</p><!-- l. 69 --><p class='indent'>   In the finite dimensional space \(V_h\), the spectral equivalence condition is \begin{equation}  \gamma _1 \langle B^{-1}u_{h},u_{h} \rangle \leq \langle Au_{h},u_{h} \rangle \leq \gamma _2 \langle B^{-1}u_{h},u_{h} \rangle \quad \forall u_{h}\in V_{h}.  \end{equation}<a id='x1-1012r10'></a> After discretization, the spectral
equivalence condition becomes \begin{equation}  \gamma _1 \langle \underline {B}^{-1} \underline {u},\underline {u} \rangle \leq \langle \mathcal {A}\underline {u},\underline {u} \rangle \leq \gamma _2 \langle \underline {B}^{-1}\underline {u},\underline {u} \rangle \quad \forall \underline {u}\in \mathbb {R}^N.  \end{equation}<a id='x1-1013r11'></a>
                                                                                               
                                                                                               
</p><!-- l. 78 --><p class='indent'>   According to (<a href='#XSteinbachConstruction1998'>Steinbach and Wendland</a>), the spectral equivalence condition determines the condition
number of the preconditioned system, which does not depend on the discretization of the system, i.e. both
geometric discretization and function discretization. \begin{equation}  \kappa (\underline {B}^{-1}\mathcal {A}) \leq \frac {\gamma _2}{\gamma _1} = \frac {c_2^Ac_2^B}{c_1^Ac_1^B}.  \end{equation}<a id='x1-1014r12'></a> This is a very important and nice feature, which means
when an iterative solver is adopted to solve the preconditioned system, the number of iterations is stable and
will not increase with the problem size.
</p><!-- l. 1 --><p class='noindent'>
</p>
   <h3 class='likesectionHead'><a id='x1-2000'></a>References</h3>
<!-- l. 1 --><p class='noindent'>
  </p><div class='thebibliography'>
  <p class='bibitem'><span class='biblabel'>
<a id='XBrennerMathematical2009'></a><span class='bibsp'>   </span></span>Susanne C. Brenner. <span class='p1xi-x-x-109'>The Mathematical Theory of Finite Element Methods</span>. Springer New York, 3 edition.
  ISBN 978-1-4419-2611-1. URL <a class='url' href='https://book.douban.com/subject/4219448/#'><span class='t1xtt-x-x-109'>https://book.douban.com/subject/4219448/#</span></a>.
  </p>
  <p class='bibitem'><span class='biblabel'>
<a id='XSteinbachConstruction1998'></a><span class='bibsp'>   </span></span>Olaf               Steinbach               and               Wolfgang L.               Wendland.                                      The
  construction of some efficient preconditioners in the boundary element method.  9(1-2):191–216.  URL
  <a class='url' href='http://link.springer.com/article/10.1023/A:1018937506719'><span class='t1xtt-x-x-109'>http://link.springer.com/article/10.1023/A:1018937506719</span></a>.
</p>
  </div>
   <div class='footnotes'><a id='x1-1002x'></a>
<!-- l. 24 --><p class='indent'>      <span class='footnote-mark'><a href='#fn1x0-bk' id='fn1x0'><sup class='textsuperscript'>1</sup></a></span><span class='p1xr-x-x-90'>Duality pairing </span>\(\langle \cdot ,\cdot \rangle _{X',X}\) <span class='p1xr-x-x-90'>is an operation which applies a function/element in the dual space </span>\(X'\) <span class='p1xr-x-x-90'>to a function/element in the primal space </span>\(X\)<span class='p1xr-x-x-90'>. </span>\(X\)
<span class='p1xr-x-x-90'>is a Banach space and needs not be a Hilbert space. The order of the dual function and primal function in the angle brackets does not
</span><span class='p1xr-x-x-90'>matter. When the context is clear, the script </span>\(X',X\) <span class='p1xr-x-x-90'>can be omitted.</span></p>                                                                                                                           </div>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="PDE" /><summary type="html"><![CDATA[For a bounded bilinear form \(a ( \cdot ,\cdot ): X\times X \rightarrow \mathbb {R}\) where \(X\) is a Hilbert space, it has an associated bounded linear operator \(A: X \rightarrow X'\) which satisfies \(a(u,v)=\langle Au,v \rangle \) for all \(u\) and \(v\) in \(X\). In other words, the behavior of the bilinear form \(a(\cdot ,\cdot )\) as a binary function (usually the operand is also a function) can be fully determined by the linear operator \(A\) as a unary function plus a definition of the duality pairing 1. Because of the boundedness of \(a(\cdot ,\cdot )\), there exists a positive constant \(c_2^A\) such that \begin{equation} a ( u,u ) = \langle Au,u \rangle \leq c_2^A \lVert u \rVert _X^2 \quad \forall u\in X. \end{equation} Meanwhile, if \(A\) is \(X\)-elliptic, there exists a positive constant \(c_1^A\) such that \begin{equation} \langle Au,u \rangle \geq c_1^A \lVert u \rVert _X^2 \quad \forall u\in X. \end{equation} We can consider \(a(u,u)\) or \(\langle Au,u \rangle \) as an energy norm for \(u\) (Brenner, page 5) (see also Understanding about energy norm used in Galerkin method), while \(a(u,u)/\lVert u \rVert _X^2\) or \(\langle Au,u \rangle /\lVert u \rVert _X^2\) as a normalized energy norm. Therefore, \([c_1^A,c_2^A]\) is the range of this normalized energy norm, which is also the range of the spectrum of the operator \(A\). If we consider a finite dimensional subspace \(V_h\) of \(V\), the corresponding function for \(u\) becomes \(u_h\). Let \(\{ \varphi _k \}_{k=1}^N\) be the basis of \(V_h\), then the function \(u_h\) has this expansion \begin{equation} u_h=\sum _{k=1}^N u_k\varphi _k. \end{equation} and can be discretized into a vector \(\underline {u}\in \mathbb {R}^N\). The bilinear form associated with \(A\) can be discretized into a matrix \(\mathcal {A}\), whose coefficient is \begin{equation} \mathcal {A}_{ij}=\langle A\varphi _j,\varphi _i \rangle . \end{equation} \(\mathcal {A}\) has similar boundedness and ellipticity properties as the operator \(A\): \begin{equation} \tilde {c}_1^A \lVert \underline {u} \rVert ^{2} \leq \underline {u}^{\mathrm {T}}\mathcal {A}\underline {u} \leq \tilde {c}_2^A \lVert \underline {u} \rVert ^{2} \quad \forall \underline {u}\in \mathbb {R}^N. \end{equation} According to Understanding about ellipticity of operators, \(\tilde {c}_1^A\) and \(\tilde {c}_2^A\) correspond to the minimum and maximum eigenvalues of \(\mathcal {A}\) and the range \([\tilde {c}_1^A,\tilde {c}_2^A]\) is the spectrum. Because \(\mathcal {A}\) is a finite dimensional approximation to \(A\), it lies in a smaller space and we must have \(c_1^A\leq \tilde {c}_1^A\) and \(\tilde {c}_2^A\leq c_2^A\). Therefore, the matrix spectrum is contained within the operator spectrum, \([\tilde {c}_1^A,\tilde {c}_2^A] \subset [c_1^A,c_2^A]\). If \(A^{-1}: X' \rightarrow X\) is the inverse operator of \(A\), its operator spectrum is the reciprocal of the spectrum for \(A\), i.e. \([\frac {1}{c_2^A},\frac {1}{c_1^A}]\). We should have \begin{equation} \frac {1}{c_2^A}\lVert v \rVert _{X'}^{2} \leq \langle A^{-1}v,v \rangle \leq \frac {1}{c_1^A} \lVert v \rVert _{X'}^2 \quad \forall v\in X'. \end{equation} The discrete version is \begin{equation} \tilde {c}_1^{A^{-1}} \lVert \underline {v} \rVert ^2 \leq \underline {v}^{\mathrm {T}} \underline {A}^{-1} \underline {v} \leq \tilde {c}_2^{A^{-1}} \lVert \underline {v} \rVert ^2 \quad \forall \underline {v}\in \mathbb {R}^M. \end{equation} N.B. The discretized matrix \(\underline {A}^{-1}\) for \(A^{-1}\) and the inverse matrix of the discretized matrix \(\mathcal {A}^{-1}\) for \(A\) are two different things. (see Discretization of the inverse of an operator) Let \(B\) be the preconditioner for \(A\). Its inverse operator \(B^{-1}: X \rightarrow X'\) should be spectrally equivalent to \(A\): \begin{equation} \gamma _1 \langle B^{-1}u,u \rangle \leq \langle Au,u \rangle \leq \gamma _2 \langle B^{-1}u,u \rangle \quad \forall u\in V. \end{equation} If the spectrum of \(B\) is \([c_1^B,c_2^B]\), the spectrum of its inverse is \([\frac {1}{c_2^B},\frac {1}{c_1^B}]\). Then we have \begin{equation} \gamma _1=c_1^A c_1^B, \gamma _2=c_2^A c_2^B. \end{equation} In the finite dimensional space \(V_h\), the spectral equivalence condition is \begin{equation} \gamma _1 \langle B^{-1}u_{h},u_{h} \rangle \leq \langle Au_{h},u_{h} \rangle \leq \gamma _2 \langle B^{-1}u_{h},u_{h} \rangle \quad \forall u_{h}\in V_{h}. \end{equation} After discretization, the spectral equivalence condition becomes \begin{equation} \gamma _1 \langle \underline {B}^{-1} \underline {u},\underline {u} \rangle \leq \langle \mathcal {A}\underline {u},\underline {u} \rangle \leq \gamma _2 \langle \underline {B}^{-1}\underline {u},\underline {u} \rangle \quad \forall \underline {u}\in \mathbb {R}^N. \end{equation} According to (Steinbach and Wendland), the spectral equivalence condition determines the condition number of the preconditioned system, which does not depend on the discretization of the system, i.e. both geometric discretization and function discretization. \begin{equation} \kappa (\underline {B}^{-1}\mathcal {A}) \leq \frac {\gamma _2}{\gamma _1} = \frac {c_2^Ac_2^B}{c_1^Ac_1^B}. \end{equation} This is a very important and nice feature, which means when an iterative solver is adopted to solve the preconditioned system, the number of iterations is stable and will not increase with the problem size. References    Susanne C. Brenner. The Mathematical Theory of Finite Element Methods. Springer New York, 3 edition. ISBN 978-1-4419-2611-1. URL https://book.douban.com/subject/4219448/#.    Olaf Steinbach and Wolfgang L. Wendland. The construction of some efficient preconditioners in the boundary element method. 9(1-2):191–216. URL http://link.springer.com/article/10.1023/A:1018937506719. 1Duality pairing \(\langle \cdot ,\cdot \rangle _{X',X}\) is an operation which applies a function/element in the dual space \(X'\) to a function/element in the primal space \(X\). \(X\) is a Banach space and needs not be a Hilbert space. The order of the dual function and primal function in the angle brackets does not matter. When the context is clear, the script \(X',X\) can be omitted.]]></summary></entry><entry><title type="html">Kernel and range of a matrix and its transpose</title><link href="https://jihuan-tian.github.io/math/2024/11/03/kernel-and-range-of-a-matrix-and-its-transpose.html" rel="alternate" type="text/html" title="Kernel and range of a matrix and its transpose" /><published>2024-11-03T00:00:00+08:00</published><updated>2024-11-03T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/11/03/kernel-and-range-of-a-matrix-and-its-transpose</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/11/03/kernel-and-range-of-a-matrix-and-its-transpose.html"><![CDATA[<p><strong>Abstract</strong></p>

<ol>
  <li>The meaning of full column rank and full row rank of a matrix, which are related to the injective and surjective property of the linear map defined by the matrix.</li>
  <li>The relationship between the injective or surjective property and the left or right inverse of a matrix.</li>
  <li>The relationship between the kernel and range spaces of a matrix and its transpose. In linear operator theory, this corresponds to a linear operator and its adjoint operator.</li>
</ol>

<p>Let \(A\) be a matrix in \(\mathbb{K}^{m\times n}\) mapping from the vector space \(V^n\) to \(W^m\). \(A^{\ast}\) is its transpose matrix, when \(\mathbb{K}=\mathbb{R}\). When \(\mathbb{K}=\mathbb{C}\), \(A^{\ast}\) is its Hermite transpose matrix, i.e. complex conjugate with transpose. \(A\) can also be a general bounded linear operator mapping from \(V\) to \(W\), where \(V\) and \(W\) are Hilbert spaces. \(A^{\ast}\) is the Hilbert adjoint operator mapping from \(W\) to \(V\). N.B. \(V\) and \(W\) should not be Banach spaces and the Banach adjoint operator \(A'\) maps from \(W^{\ast}\) to \(V^{\ast}\), instead of \(W\) to \(V\).</p>

<p>Then we have the following conclusions:</p>

<ul>
  <li>The row space of \(A\) is the column space of \(A^{\ast}\) and the column space of \(A\) is the row space of \(A^{\ast}\).</li>
  <li>If a linear map is injective, it has a left inverse operator. If it is surjective, it has a right inverse operator.</li>
  <li>
    <p>When a matrix \(A\) has full row rank \(r=m\):</p>

    <ul>
      <li>The column rank of \(A\) is also \(r\) and \(n\geq r\). For any vector \(y\in \mathbb{R}^m=\mathbb{R}^r\), it can be linearly represented by the \(n\) column vectors of \(A\). If \(n=r\), such a representation is unique. If \(n&gt;r\), the \(n\) column vectors of \(A\) are not linearly independent and the representation is not unique. In both cases, the representation is achievable and the linear map defined by \(A\) must be surjective.</li>
      <li>\(A^{\ast}\) has full column rank and the linear map defined by \(A^{\ast}\) is injective.</li>
      <li>\(A\) has a right inverse and \(A^{\ast}\) has a left inverse.</li>
    </ul>

    <p><img src="/figures/2024-11-02_18-48-43-matrix-has-full-row-rank.png" alt="img" /></p>
  </li>
  <li>
    <p>When a matrix \(A\) has full column rank \(r=n\):</p>

    <ul>
      <li>The linear map represented by \(A\) is injective.</li>
      <li>The linear map represented by \(A^{\ast}\) is surjective.</li>
      <li>\(A\) has a left inverse and \(A^{\ast}\) has a right inverse.</li>
    </ul>

    <p><img src="/figures/2024-11-02_18-49-10-matrix-has-full-column-rank.png" alt="img" /></p>
  </li>
  <li>
    <p>General case</p>

    <p>We have</p>

\[\begin{equation}
  \begin{aligned}
    \mathrm{Im}(A) &amp;= \left( \mathrm{ker}(A^{\ast}) \right)^{\perp} \\
    \mathrm{Im}(A^{\ast}) &amp;= \left( \mathrm{ker}(A) \right)^{\perp}
  \end{aligned}.
\end{equation}\]

    <p><img src="/figures/2024-11-02_18-51-49-matrix-do-not-have-full-rank.png" alt="img" /></p>
  </li>
</ul>

<p>Backlinks: <a href="/math/2024/11/11/moore-penrose-pseudoinverse-and-generalized-inverse.html">《Moore-Penrose pseudoinverse and generalized inverse》</a></p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="linear-algebra" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">Variational problems</title><link href="https://jihuan-tian.github.io/math/2024/10/26/variational-problems.html" rel="alternate" type="text/html" title="Variational problems" /><published>2024-10-26T00:00:00+08:00</published><updated>2024-10-26T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/10/26/variational-problems</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/10/26/variational-problems.html"><![CDATA[<h3 class='likesectionHead'><a id='x1-1000'></a>Contents</h3>
   <div class='tableofcontents'>
    <span class='sectionToc'>1 <a href='#x1-20001' id='QQ2-1-2'>Basic ideas</a></span>
<br />    <span class='sectionToc'>2 <a href='#x1-30002' id='QQ2-1-3'>When there is no constraint</a></span>
<br />     <span class='subsectionToc'>2.1 <a href='#x1-40002.1' id='QQ2-1-4'>Preconditions</a></span>
<br />     <span class='subsectionToc'>2.2 <a href='#x1-50002.2' id='QQ2-1-5'>Problem definitions</a></span>
<br />     <span class='subsectionToc'>2.3 <a href='#x1-60002.3' id='QQ2-1-6'>Unique solvability and stability</a></span>
<br />    <span class='sectionToc'>3 <a href='#x1-70003' id='QQ2-1-7'>When there is a constraint \(Bu=g\)</a></span>
<br />     <span class='subsectionToc'>3.1 <a href='#x1-80003.1' id='QQ2-1-8'>Preconditions</a></span>
<br />     <span class='subsectionToc'>3.2 <a href='#x1-90003.2' id='QQ2-1-10'>Problem definitions</a></span>
<br />     <span class='subsectionToc'>3.3 <a href='#x1-100003.3' id='QQ2-1-11'>Unique solvability and stability</a></span>
   </div>
<!-- l. 9 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>1    </span> <a id='x1-20001'></a>Basic ideas</h3>
     <ul class='itemize1'>
     <li class='itemize'>
     <!-- l. 12 --><p class='noindent'>There are three types of problems: </p>
         <ul class='itemize2'>
         <li class='itemize'>
         <!-- l. 14 --><p class='noindent'>The  original  operator  equation  \(Au=f\).  The  two  sides  of  this  equation  are  functions  and  should  be  equal
         pointwise. This is the strong form problem.
         </p></li>
         <li class='itemize'>
         <!-- l. 16 --><p class='noindent'>The variational problem is obtained by projecting the two sides of the operator equation to a test function
         space \(V\). The “projection” here means multiply the two sides of the equation with an arbitrary function in
         the test function space, then integrate them over the domain, i.e. \(\left \langle Au,v \right \rangle =\left \langle f,v \right \rangle \). Therefore, infinitely many equations
         are established and we say the equality is in the sense of projection. This is the weak form problem.
                                                                                               
                                                                                               
         </p><!-- l. 24 --><p class='noindent'>We can also understand the test function space \(V\) as a multidimensional ruler. The right hand side function
         \(f\) of the operator equation and the output function returned from applying the partial differential operator
         to the solution function, i.e. \(Au\), will be measured with this ruler.
         </p></li>
         <li class='itemize'>
         <!-- l. 28 --><p class='noindent'>The problem of minimizing an energy functional, such as \(\mathcal {L}(v)=\frac {1}{2}\left \langle Av,v \right \rangle -\left \langle f,v \right \rangle \).</p></li></ul>
     </li>
     <li class='itemize'>
     <!-- l. 32 --><p class='noindent'>The operator equation and the variational problem is equivalent due to the boundedness of the bilinear form
     \(a(u,v):= \left \langle Au,v \right \rangle \).
     </p></li>
     <li class='itemize'>
     <!-- l. 34 --><p class='noindent'>The variation problem is further equivalent to the minimization problem, if the bilinear form \(a(u,v)\) is symmetric and
     positive definite. This condition is equivalent to that the linear operator \(A\) associated with this bilinear form \(a(u,v)\) is self-dual
     and elliptic. Of course, the boundedness of the bilinear form \(a(u,v)\) is equivalent to the boundedness of the linear operator
     \(A\).</p></li></ul>
<!-- l. 41 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>2    </span> <a id='x1-30002'></a>When there is no constraint</h3>
<!-- l. 43 --><p class='noindent'>
</p>
   <h4 class='subsectionHead'><span class='titlemark'>2.1    </span> <a id='x1-40002.1'></a>Preconditions</h4>
     <ul class='itemize1'>
     <li class='itemize'>
     <!-- l. 46 --><p class='noindent'>\(X\) is a Hilbert space and \(X'\) is the dual space of \(X\).
     </p></li>
     <li class='itemize'>
     <!-- l. 47 --><p class='noindent'>\(\left \langle \cdot ,\cdot \right \rangle _X\) is the inner product in \(X\).
     </p></li>
     <li class='itemize'>
     <!-- l. 48 --><p class='noindent'>\(\left \langle \cdot ,\cdot \right \rangle \) is the duality pairing, which applies a linear functional \(f\in X'\) to an element \(u\in X\), i.e. \(\left \langle f,u \right \rangle =\left \langle u,f \right \rangle =f(u)\).
     </p></li>
     <li class='itemize'>
     <!-- l. 50 --><p class='noindent'>\(A: X \rightarrow X'\) is a bounded linear operator satisfying \begin{equation}  \norm {Av}_{X'}\leq c_2^A\norm {v}_X \quad \forall v \in X.  \end{equation}<a id='x1-4001r1'></a>
     </p></li>
     <li class='itemize'>
     <!-- l. 54 --><p class='noindent'>\(f\in X'\)</p></li></ul>
                                                                                               
                                                                                               
<!-- l. 57 --><p class='noindent'>
</p>
   <h4 class='subsectionHead'><span class='titlemark'>2.2    </span> <a id='x1-50002.2'></a>Problem definitions</h4>
   <div class='Definition'><div class='newtheorem'>
<!-- l. 59 --><p class='noindent'><span class='head'>
<span class='p1xb-'>Definition 1 (Operator equation with no constraint)</span> </span><a id='x1-5002'></a>Find \(u \in X\) satisfying <a id='x1-50011'></a>\begin{equation}  Au = f.  \end{equation}<a id='x1-5003r2'></a>
</p>
   </div>
<!-- l. 65 --><p class='indent'>   </p></div>
   <div class='Definition'><div class='newtheorem'>
<!-- l. 67 --><p class='noindent'><span class='head'>
<span class='p1xb-'>Definition 2 (Variational problem with no constraint)</span> </span><a id='x1-5005'></a>Find \(u\in X\) such that <a id='x1-50042'></a>\begin{equation}  \left \langle Au,v \right \rangle = \left \langle f,v \right \rangle \quad \forall v\in X.  \end{equation}<a id='x1-5006r3'></a>
</p>
   </div>
<!-- l. 73 --><p class='indent'>   </p></div>
   <div class='Definition'><div class='newtheorem'>
<!-- l. 75 --><p class='noindent'><span class='head'>
<span class='p1xb-'>Definition 3 (Minimization problem for no constraint case)</span> </span><a id='x1-5008'></a>Let \(F\) be the functional <a id='x1-50073'></a>\begin{equation}  F(v) := \frac {1}{2}\left \langle Av,v \right \rangle - \left \langle f,v \right \rangle \quad (v\in X).  \end{equation}<a id='x1-5009r4'></a> The minimization problem is defined
as \begin{equation}  F(u) =\min _{v\in X} F(v).  \end{equation}<a id='x1-5010r5'></a>
</p>
   </div>
<!-- l. 86 --><p class='indent'>   </p></div>
<!-- l. 88 --><p class='noindent'>Relations between the above problems: </p>
     <ul class='itemize1'>
     <li class='itemize'>
     <!-- l. 90 --><p class='noindent'>Operator equation (Definition <a href='#x1-50011'>1<!-- tex4ht:ref: def:op-eqn-no-constraint  --></a>) \(\Longleftrightarrow \) variational problem (Definition <a href='#x1-50042'>2<!-- tex4ht:ref: def:var-eqn-no-constraint  --></a>)
     </p></li>
     <li class='itemize'>
     <!-- l. 92 --><p class='noindent'>When \(A\) is further self-adjoint <span class='footnote-mark'><a href='#fn1x0' id='fn1x0-bk'><sup class='textsuperscript'>1</sup></a></span><a id='x1-5011f1'></a>
     and positive semi-definite, variational problem (Definition <a href='#x1-50042'>2<!-- tex4ht:ref: def:var-eqn-no-constraint  --></a>) \(\Longleftrightarrow \) minimization problem (Definition <a href='#x1-50073'>3<!-- tex4ht:ref: def:min-eqn-no-constraint  --></a>)</p></li></ul>
<!-- l. 98 --><p class='noindent'>
</p>
   <h4 class='subsectionHead'><span class='titlemark'>2.3    </span> <a id='x1-60002.3'></a>Unique solvability and stability</h4>
<!-- l. 100 --><p class='noindent'>The unique solvability of the operator equation (Definition <a href='#x1-50011'>1<!-- tex4ht:ref: def:op-eqn-no-constraint  --></a>) (and thus the variational problem (Definition <a href='#x1-50042'>2<!-- tex4ht:ref: def:var-eqn-no-constraint  --></a>)) for any \(f\in X'\) is
governed by the Lax-Milgram Lemma, which requires that \(A\) should be \(X\)-elliptic, i.e. \begin{equation}  \label {eq:ellipticity-condition} \left \langle Av,v \right \rangle \geq c_1^A\norm {v}_X^2 \quad \forall v\in X.  \end{equation}<a id='x1-6001r6'></a> It also prescribes a stability condition on
the solution \(u\): \begin{equation}  \norm {u}_X\leq \frac {1}{c_1^A}\norm {f}_{X'}.  \end{equation}<a id='x1-6002r7'></a> It can be seen that the smaller the ellipticity constant \(c_1^A\), the less the stability for \(u\). <span id='textcolor1'>\(c_1^A\) is related to the
smallest eigenvalue of the discretized matrix for the operator \(A\). A small \(c_1^A\) is caused by an increase in the number
of DoFs and sharp corners in the geometry of the model. The error in the data for \(f\) will be amplified by
\(\frac {1}{c_1^A}\).</span>
</p>
   <div class='remark'><div class='newtheorem'>
<!-- l. 117 --><p class='noindent'><span class='head'>
                                                                                               
                                                                                               
<span class='p1xb-'>Remark 1 (Understanding about ellipticity of operators)</span> </span><a id='x1-6004'></a>We are going to compare the ellipticity condition in Equation
(<a href='#x1-6001r6'>6<!-- tex4ht:ref: eq:ellipticity-condition  --></a>) with its discrete counterpart. Let \(\mathscr {A}\) be the matrix in \(\mathbb {R}^{n\times n}\) corresponding to \(A\) which is discretized via the Galerkin method. Let \(x\)
be the vector in \(\mathbb {R}^n\) related to \(v\). Assume \(\mathscr {A}\) has \(n\) eigenvalues \(\lambda _1,\cdots ,\lambda _n\) and \(n\) eigenvectors \(v_1,\cdots ,v_n\) (N.B. Even though the multiplicity of some of the
eigenvalues may be larger than one, for the same number of eigenvectors spanning its eigenspace, we can still explicitly use
different symbols to represent their same eigenvalue. Therefore, we allow duplicated values in the list \(\lambda _1,\cdots ,\lambda _n\). Because the sum of
the multiplicity of all eigenvalues is \(n\) and all the eigenspaces are orthogonal to each other, we can choose those eigenvectors \(v_1,\cdots ,v_n\)
to form an orthonormal set. Then any vector \(x\) in \(\mathbb {R}^n\) can be expanded by this basis: \begin{equation}  x = \sum _{i=1}^n c_i v_i.  \end{equation}<a id='x1-6005r8'></a> And the duality pairing
can be represented as \begin{equation}  x^{\mathrm {T}} \mathscr {A} x = \left ( \sum _{i=1}^n c_iv_i^{\mathrm {T}} \right ) \mathscr {A} \left ( \sum _{j=1}^n c_jv_j \right ) = \sum _{i,j} c_ic_j v_i^{\mathrm {T}} \mathscr {A} v_j = \sum _{i,j} c_ic_j \lambda _j (v_i,v_j),  \end{equation}<a id='x1-6006r9'></a> where \((\cdot ,\cdot )\) is the inner product in \(\mathbb {R}^n\). Because \(v_1,\cdots ,v_n\) are orthonormal, we have \((v_i,v_j) =\delta _{ij}\). Hence, \begin{equation}  x^{\mathrm {T}} \mathscr {A} x = \sum _{i=1}^n \lambda _i c_i^2.  \end{equation}<a id='x1-6007r10'></a> If \(\mathscr {A}\) is
positive definite, the solution to the linear system \(Ax = b\) exists and is unique. Since all eigenvalues of \(\mathscr {A}\) are larger
than zero, we let \(\lambda _{\min }\) be the minimum and then have \begin{equation}  \left ( \mathscr {A}x, x \right ) = x^{\mathrm {T}} \mathscr {A} x \geq \lambda _{\min } \sum _{i=1}^n c_i^2 = \lambda _{\min } \lVert x \rVert _{\mathbb {R}^n}^2.  \end{equation}<a id='x1-6008r11'></a> We can see that this discrete formulation is consistent
with the ellipticity condition. The ellipticity constant \(c_1^A\) of the bounded linear operator \(A\) corresponds to the
minimum eigenvalue of the positive definite matrix \(\mathscr {A}\). That’s why the ellipticity condition is the key in the
Lax-Milgram Lemma, which governs the existence and uniqueness of the solution for a PDE formed by
\(A\).
</p>
   </div>
<!-- l. 148 --><p class='indent'>   </p></div>
<!-- l. 150 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>3    </span> <a id='x1-70003'></a>When there is a constraint \(Bu=g\)</h3>
<!-- l. 152 --><p class='noindent'>
</p>
   <h4 class='subsectionHead'><span class='titlemark'>3.1    </span> <a id='x1-80003.1'></a>Preconditions</h4>
     <ul class='itemize1'>
     <li class='itemize'>
     <!-- l. 155 --><p class='noindent'>\(X\) and \(\Pi \) are Banach spaces <span class='footnote-mark'><a href='#fn2x0' id='fn2x0-bk'><sup class='textsuperscript'>2</sup></a></span><a id='x1-8001f2'></a>
     with their dual spaces \(X'\) and \(\Pi '\).
     </p></li>
     <li class='itemize'>
     <!-- l. 160 --><p class='noindent'>\(A: X \rightarrow X'\) is a bounded linear operator satisfying \begin{equation}  \norm {Av}_{X'}\leq c_2^A\norm {v}_X \quad \forall v \in X.  \end{equation}<a id='x1-8003r12'></a>
     </p></li>
     <li class='itemize'>
     <!-- l. 164 --><p class='noindent'>\(B: X \rightarrow \Pi '\) is a bounded linear operator satisfying \begin{equation}  \norm {Bv}_{\Pi '} \leq c_2^B\norm {v}_X \quad \forall v\in X.  \end{equation}<a id='x1-8004r13'></a>
     </p></li>
     <li class='itemize'>
     <!-- l. 168 --><p class='noindent'>\(B': \Pi \rightarrow X'\) is the adjoint operator of \(B\) <span class='footnote-mark'><a href='#fn3x0' id='fn3x0-bk'><sup class='textsuperscript'>3</sup></a></span><a id='x1-8005f3'></a>.
     </p></li>
     <li class='itemize'>
     <!-- l. 170 --><p class='noindent'>\(f\in X'\) and \(g \in \Pi '\)</p></li></ul>
                                                                                               
                                                                                               
<!-- l. 173 --><p class='noindent'>The relationship between various spaces is illustrated in Figure <a href='#x1-8009r1'>1<!-- tex4ht:ref: fig:space-relation-in-var-problems  --></a>, where \(J_X\) and \(J_{\Pi }\) are Riesz maps
<span class='footnote-mark'><a href='#fn4x0' id='fn4x0-bk'><sup class='textsuperscript'>4</sup></a></span><a id='x1-8007f4'></a>.
</p><figure class='figure'> 

                                                                                               
                                                                                               
<a id='x1-8009r1'></a>
                                                                                               
                                                                                               
<!-- l. 178 --><p class='noindent'><img alt='PIC'  src='/figures/2024-10-26-spaces-in-variational-problems.png'  />
</p>
<figcaption class='caption'><span class='id'>Figure 1: </span><span class='content'>Relationship between various spaces in the variational problem.</span></figcaption><!-- tex4ht:label?: x1-8009r1  -->
                                                                                               
                                                                                               
   </figure>
   <h4 class='subsectionHead'><span class='titlemark'>3.2    </span> <a id='x1-90003.2'></a>Problem definitions</h4>
   <div class='Definition'><div class='newtheorem'>
<!-- l. 185 --><p class='noindent'><span class='head'>
<span class='p1xb-'>Definition 4 (Operator equation with constraint</span> \(Bu=g\)<span class='p1xb-'>)</span> </span><a id='x1-9002'></a><a id='x1-90014'></a> Define the manifold \(V_g := \left \{ v\in X: Bv=g \right \}\) and the kernel space \(V_0 := \ker B\). Assume the solvability
condition \(g\in \image _XB\) and \(f\in \image _{V_g}A\). Then find \(u\in V_g\) satisfying \begin{equation}  Au = f.  \end{equation}<a id='x1-9003r14'></a> N.B. If we require \(u\) to be within the space \(V_g\), the constraint \(Bu=g\) is automatically
satisfied.
</p>
   </div>
<!-- l. 194 --><p class='indent'>   </p></div>
   <div class='Definition'><div class='newtheorem'>
<!-- l. 196 --><p class='noindent'><span class='head'>
<span class='p1xb-'>Definition 5 (Variational problem with constraint</span> \(Bu=g\)<span class='p1xb-'>)</span> </span><a id='x1-9005'></a><a id='x1-90045'></a> Assume the solvability condition \(g\in \image _XB\) and \(f\in \image _{V_g}A\). Then find \(u\in V_g\) satisfying
\begin{equation}  \left \langle Au,v \right \rangle = \left \langle f,v \right \rangle \quad \forall v\in V_0.  \end{equation}<a id='x1-9006r15'></a>
</p>
   </div>
<!-- l. 203 --><p class='indent'>   </p></div>
   <div class='remark'><div class='newtheorem'>
<!-- l. 205 --><p class='noindent'><span class='head'>
<span class='p1xb-'>Remark 2</span> </span><a id='x1-9008'></a>Such constraint \(Bu=g\) can be the essential boundary condition, i.e. Dirichlet boundary condition. Then the
solution \(u\) is sought in the space \(V_g\) which explicitly satisfies the Dirichlet boundary condition, while the test function
is chosen from the space \(V_0\) which satisfies the homogeneous Dirichlet boundary condition.
</p>
   </div>
<!-- l. 210 --><p class='indent'>   </p></div>
   <div class='Definition'><div class='newtheorem'>
<!-- l. 212 --><p class='noindent'><span class='head'>
<span class='p1xb-'>Definition 6 (Extended variational problem using Lagrange multiplier)</span> </span><a id='x1-9010'></a><a id='x1-90096'></a> Introduce the Lagrange multiplier \(p\in \Pi \). Find \((u,p)\in X\times \Pi \) such
that \begin{equation}  \begin {aligned} \left \langle Au,v \right \rangle + \left \langle Bv,p \right \rangle &amp;= \left \langle f,v \right \rangle \\ \left \langle Bu,q \right \rangle &amp;= \left \langle g,q \right \rangle \end {aligned} \quad \forall (v,q)\in X\times \Pi .  \end{equation}<a id='x1-9011r16'></a>
</p>
   </div>
<!-- l. 223 --><p class='indent'>   </p></div>
   <div class='remark'><div class='newtheorem'>
<!-- l. 225 --><p class='noindent'><span class='head'>
<span class='p1xb-'>Remark 3</span> </span><a id='x1-9013'></a>The solution \((u,p)\) is sought in the product space \(X \times \Pi \) but not \(V_g \times \Pi \).
</p>
   </div>
<!-- l. 227 --><p class='indent'>   </p></div>
   <div class='Definition'><div class='newtheorem'>
<!-- l. 229 --><p class='noindent'><span class='head'>
                                                                                               
                                                                                               
<span class='p1xb-'>Definition 7 (Saddle point problem related to the variational problem with constraint in Definition </span><a href='#x1-90045'><span class='p1xb-'>5</span><!-- tex4ht:ref: def:var-eqn-with-constraint  --></a><span class='p1xb-'>)</span> </span><a id='x1-9015'></a><a id='x1-90147'></a> Let \(\mathcal {L}\) be the
Lagrange functional \begin{equation}  \mathcal {L}(v,q) := \frac {1}{2}\left \langle Av,v \right \rangle - \left \langle f,v \right \rangle + \left \langle Bv,q \right \rangle - \left \langle g,q \right \rangle \quad (v,q)\in X\times \Pi .  \end{equation}<a id='x1-9016r17'></a> The saddle point problem is defined as: find \((u,p)\in X\times \Pi \) such that \begin{equation}  \mathcal {L}(u,q)\leq \mathcal {L}(u,p)\leq \mathcal {L}(v,p) \quad \forall (v,q)\in X\times \Pi .  \end{equation}<a id='x1-9017r18'></a>
</p>
   </div>
<!-- l. 242 --><p class='indent'>   </p></div>
<!-- l. 244 --><p class='noindent'>Relations between the above problems </p>
     <ul class='itemize1'>
     <li class='itemize'>
     <!-- l. 246 --><p class='noindent'>Operator equation with constraint (Definition <a href='#x1-90014'>4<!-- tex4ht:ref: def:op-eqn-with-constraint  --></a>) \(\Longleftrightarrow \) variational problem with constraint (Definition <a href='#x1-90045'>5<!-- tex4ht:ref: def:var-eqn-with-constraint  --></a>)
     </p></li>
     <li class='itemize'>
     <!-- l. 248 --><p class='noindent'>Variation problem with constraint (Definition <a href='#x1-90045'>5<!-- tex4ht:ref: def:var-eqn-with-constraint  --></a>) \(\Longleftrightarrow \) extended variational problem using Lagrange multiplier
     (Definition <a href='#x1-90096'>6<!-- tex4ht:ref: def:extended-var-eqn  --></a>)
     </p></li>
     <li class='itemize'>
     <!-- l. 251 --><p class='noindent'>When  \(A\)  is  further  self-adjoint  and  positive  semi-definite,  extended  variational  problem  using  Lagrange
     multiplier (Definition <a href='#x1-90096'>6<!-- tex4ht:ref: def:extended-var-eqn  --></a>) \(\Longleftrightarrow \) saddle point problem (Definition <a href='#x1-90147'>7<!-- tex4ht:ref: def:saddle-point-eqn  --></a>)</p></li></ul>
<!-- l. 256 --><p class='noindent'>
</p>
   <h4 class='subsectionHead'><span class='titlemark'>3.3    </span> <a id='x1-100003.3'></a>Unique solvability and stability</h4>
     <ul class='itemize1'>
     <li class='itemize'>
     <!-- l. 259 --><p class='noindent'>The unique solvability of the variational problem (Definition <a href='#x1-90045'>5<!-- tex4ht:ref: def:var-eqn-with-constraint  --></a>) (and thus the operator equation (Definition <a href='#x1-90014'>4<!-- tex4ht:ref: def:op-eqn-with-constraint  --></a>)) for any
     \(f\in \image _{V_g}A\) and \(g\in \image _XB\) is ensured when \(A\) is \(V_0\)-elliptic, \begin{equation}  \left \langle Av,v \right \rangle \geq c_1^A\norm {v}_X^2 \quad \forall v\in V_0 := \kernel B.  \end{equation}<a id='x1-10001r19'></a> This can be considered as an extension of the Lax-Milgram Lemma. If
     we further assume a condition on the constraint \(Bu=g\) relation: for any \(g\in \image _XB\), there exists a \(u_g\in V_g\) such that \begin{equation}  \label {eq:ug-norm-constraint} \norm {u_g}_X \leq c_B \norm {g}_{\Pi '},  \end{equation}<a id='x1-10002r20'></a> which
     can be considered as a stability condition on \(u_g\) derived from the constraint relation only (or we say
     the norm of the function \(u_g\) is controlled by the norm of the given data \(g\)), then the following stability
     condition holds for the unique solution \(u\) of the variational problem (Definition <a href='#x1-90045'>5<!-- tex4ht:ref: def:var-eqn-with-constraint  --></a>) or the operator equation
     (Definition <a href='#x1-90014'>4<!-- tex4ht:ref: def:op-eqn-with-constraint  --></a>): \begin{equation}  \norm {u}_X \leq \frac {1}{c_1^A}\norm {f}_{X'} + \left ( 1+\frac {c_2^A}{c_1^A} \right ) c_B \norm {g}_{\Pi '}.  \end{equation}<a id='x1-10003r21'></a> From this we can see that the stability of \(u\) is better when the \(V_0\)-ellipticity constant \(c_1^A\) is larger,
     the boundedness constant \(c_2^A\) of \(A\) is smaller and the stability constant \(c_B\) for \(u_g\) is smaller. It is obvious that
     when the operator \(A\) is unbounded, the solution \(u\) is unstable. Therefore, the boundedness of \(A\) is very
     important.
     </p></li>
     <li class='itemize'>
     <!-- l. 283 --><p class='noindent'>The unique solvability of the extended variational problem using Lagrange multiplier (Definition <a href='#x1-90096'>6<!-- tex4ht:ref: def:extended-var-eqn  --></a>) can be achieved
     for any \(f\in \image _{V_g}A\) and \(g\in \image _XB\), when \(V_0\)-ellipticity for \(A\), stability condition for \(u_g\in V_g\) as in (<a href='#x1-10002r20'>20<!-- tex4ht:ref: eq:ug-norm-constraint  --></a>) and the stability condition for \(q\in \Pi \) \begin{equation}  \label {eq:stability-for-q} c_S\norm {q}_{\Pi }\leq \sup _{0\neq v\in X}\frac {\abs {\left \langle Bv,q \right \rangle }}{\norm {v}_X} \quad (\forall q\in \Pi )  \end{equation}<a id='x1-10004r22'></a> are satisfied. Since \begin{equation}  \sup _{0\neq v\in X}\frac {\abs {\left \langle Bv,q \right \rangle }}{\norm {v}_X} = \sup _{0\neq v\in X}\frac {\abs {\left \langle v,B'q \right \rangle }}{\norm {v}_X} = \norm {B'q}_{X'}\leq \norm {B'}\norm {q}_{\Pi },  \end{equation}<a id='x1-10005r23'></a>
     the stability condition for \(q\) in (<a href='#x1-10004r22'>22<!-- tex4ht:ref: eq:stability-for-q  --></a>) is equivalent to \begin{equation}  \norm {B'}\geq c_S,  \end{equation}<a id='x1-10006r24'></a> which means the norm of the adjoint operator \(B'\) should have a
     non-zero lower bound.
     </p><!-- l. 303 --><p class='noindent'>Then the stability conditions for the solution \((u,p)\in X\times \Pi \) are \begin{equation}  \norm {u}_X\leq \frac {1}{c_1^A}\norm {f}_{X'}+\left ( 1+\frac {c_2^A}{c_1^A} \right )c_B\norm {g}_{\Pi '}  \end{equation}<a id='x1-10007r25'></a> and \begin{equation}  \norm {p}_{\Pi }\leq \frac {1}{c_S} \left ( 1+\frac {c_2^A}{c_1^A} \right ) \left \{ \norm {f}_{X'}+c_Bc_2^A\norm {g}_{\Pi '} \right \}.  \end{equation}<a id='x1-10008r26'></a> It can be seen that the larger the lower bound \(c_S\) for \(\norm {B'}\), the larger
     the stability for \(p\).
                                                                                               
                                                                                               
     </p></li>
     <li class='itemize'>
     <!-- l. 312 --><p class='noindent'>The unique solvability of the extended variational problem using Lagrange multiplier (Definition <a href='#x1-90096'>6<!-- tex4ht:ref: def:extended-var-eqn  --></a>) can
     also be achieved for any \(f\in X'\) and \(g\in \Pi '\), when \(X\)-ellipticity for \(A\), i.e. \begin{equation}  \left \langle Av,v \right \rangle \geq c_1^A\norm {v}_X^2 \quad \forall v\in X,  \end{equation}<a id='x1-10009r27'></a> and the stability condition for \(q\in \Pi \) as in (<a href='#x1-10004r22'>22<!-- tex4ht:ref: eq:stability-for-q  --></a>) are
     satisfied. The stability conditions for the solution \((u,p)\in X\times \Pi \) are \begin{equation}  \norm {u}_X\leq \frac {1}{c_1^A}\left ( 1+\frac {(c_2^B)^2}{c_1^Ac_1^S} \right )\norm {f}_{X'}+\frac {c_2^B}{c_1^Ac_1^S}\norm {g}_{\Pi '}  \end{equation}<a id='x1-10010r28'></a> and \begin{equation}  \norm {p}_{\Pi }\leq \frac {1}{c_1^S}\norm {BA^{-1}f-g}_{\Pi '}\leq \frac {1}{c_1^S}\left [ \frac {c_2^B}{c_1^A}\norm {f}_{X'}+\norm {g}_{\Pi '} \right ],  \end{equation}<a id='x1-10011r29'></a> where \(c_1^S\) is the coefficient for the \(\Pi \)-ellipticity of
     the operator \(S := BA^{-1}B': \Pi \rightarrow \Pi '\), i.e. \begin{equation}  \left \langle Sq,q \right \rangle \geq c_1^S \norm {q}_{\Pi }^2 \quad \forall q\in \Pi .  \end{equation}<a id='x1-10012r30'></a> This \(\Pi \)-ellipticity depends on the \(V_0\)-ellipticity for \(A\) and the stability condition for \(q\) as in
     (<a href='#x1-10004r22'>22<!-- tex4ht:ref: eq:stability-for-q  --></a>).</p></li></ul>
<!-- l. 1 --><p class='noindent'>
</p>
   <h3 class='likesectionHead'><a id='x1-11000'></a>References</h3>
<!-- l. 1 --><p class='noindent'>
  </p><div class='thebibliography'>
  <p class='bibitem'><span class='biblabel'>
<a id='XSteinbachNumerical2007'></a><span class='bibsp'>   </span></span>Olaf  Steinbach.    <span class='p1xi-'>Numerical  Approximation  Methods  for  Elliptic  Boundary  Value  Problems:  Finite  and  Boundary
  </span><span class='p1xi-'>Elements</span>. Springer Science &amp; Business Media. ISBN 978-0-387-31312-2.
</p>
  </div>
   <div class='footnotes'><a id='x1-5012x2.2'></a>
<!-- l. 93 --><p class='noindent'><span class='footnote-mark'><a href='#fn1x0-bk' id='fn1x0'><sup class='textsuperscript'>1</sup></a></span><span class='p1xr-x-x-80'>In (</span><a href='#XSteinbachNumerical2007'><span class='p1xr-x-x-80'>Steinbach</span></a><span class='p1xr-x-x-80'>), self-adjoint is actually self-dual in functional analysis.</span></p><a id='x1-8002x3.1'></a>
<!-- l. 158 --><p class='noindent'><span class='footnote-mark'><a href='#fn2x0-bk' id='fn2x0'><sup class='textsuperscript'>2</sup></a></span><span class='p1xr-x-x-80'>In Banach spaces, inner product is not defined, so according to functional analysis, the concept of adjointness is not available. This again
</span><span class='p1xr-x-x-80'>verifies my understanding that in (</span><a href='#XSteinbachNumerical2007'><span class='p1xr-x-x-80'>Steinbach</span></a><span class='p1xr-x-x-80'>), self-adjoint is actually self-dual.</span></p><a id='x1-8006x3.1'></a>
<!-- l. 169 --><p class='noindent'><span class='footnote-mark'><a href='#fn3x0-bk' id='fn3x0'><sup class='textsuperscript'>3</sup></a></span><span class='p1xr-x-x-80'>In functional analysis, this is actually the dual operator, not adjoint operator.</span></p><a id='x1-8008x3.1'></a>
<!-- l. 175 --><p class='indent'>        <span class='footnote-mark'><a href='#fn4x0-bk' id='fn4x0'><sup class='textsuperscript'>4</sup></a></span><span class='p1xr-x-x-80'>N.B. A Riesz map maps from the dual space to the primal space.</span></p>                                                                                                                        </div>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="PDE" /><summary type="html"><![CDATA[Contents  1 Basic ideas  2 When there is no constraint   2.1 Preconditions   2.2 Problem definitions   2.3 Unique solvability and stability  3 When there is a constraint \(Bu=g\)   3.1 Preconditions   3.2 Problem definitions   3.3 Unique solvability and stability 1 Basic ideas There are three types of problems: The original operator equation \(Au=f\). The two sides of this equation are functions and should be equal pointwise. This is the strong form problem. The variational problem is obtained by projecting the two sides of the operator equation to a test function space \(V\). The “projection” here means multiply the two sides of the equation with an arbitrary function in the test function space, then integrate them over the domain, i.e. \(\left \langle Au,v \right \rangle =\left \langle f,v \right \rangle \). Therefore, infinitely many equations are established and we say the equality is in the sense of projection. This is the weak form problem. We can also understand the test function space \(V\) as a multidimensional ruler. The right hand side function \(f\) of the operator equation and the output function returned from applying the partial differential operator to the solution function, i.e. \(Au\), will be measured with this ruler. The problem of minimizing an energy functional, such as \(\mathcal {L}(v)=\frac {1}{2}\left \langle Av,v \right \rangle -\left \langle f,v \right \rangle \). The operator equation and the variational problem is equivalent due to the boundedness of the bilinear form \(a(u,v):= \left \langle Au,v \right \rangle \). The variation problem is further equivalent to the minimization problem, if the bilinear form \(a(u,v)\) is symmetric and positive definite. This condition is equivalent to that the linear operator \(A\) associated with this bilinear form \(a(u,v)\) is self-dual and elliptic. Of course, the boundedness of the bilinear form \(a(u,v)\) is equivalent to the boundedness of the linear operator \(A\). 2 When there is no constraint 2.1 Preconditions \(X\) is a Hilbert space and \(X'\) is the dual space of \(X\). \(\left \langle \cdot ,\cdot \right \rangle _X\) is the inner product in \(X\). \(\left \langle \cdot ,\cdot \right \rangle \) is the duality pairing, which applies a linear functional \(f\in X'\) to an element \(u\in X\), i.e. \(\left \langle f,u \right \rangle =\left \langle u,f \right \rangle =f(u)\). \(A: X \rightarrow X'\) is a bounded linear operator satisfying \begin{equation} \norm {Av}_{X'}\leq c_2^A\norm {v}_X \quad \forall v \in X. \end{equation} \(f\in X'\) 2.2 Problem definitions Definition 1 (Operator equation with no constraint) Find \(u \in X\) satisfying \begin{equation} Au = f. \end{equation} Definition 2 (Variational problem with no constraint) Find \(u\in X\) such that \begin{equation} \left \langle Au,v \right \rangle = \left \langle f,v \right \rangle \quad \forall v\in X. \end{equation} Definition 3 (Minimization problem for no constraint case) Let \(F\) be the functional \begin{equation} F(v) := \frac {1}{2}\left \langle Av,v \right \rangle - \left \langle f,v \right \rangle \quad (v\in X). \end{equation} The minimization problem is defined as \begin{equation} F(u) =\min _{v\in X} F(v). \end{equation} Relations between the above problems: Operator equation (Definition 1) \(\Longleftrightarrow \) variational problem (Definition 2) When \(A\) is further self-adjoint 1 and positive semi-definite, variational problem (Definition 2) \(\Longleftrightarrow \) minimization problem (Definition 3) 2.3 Unique solvability and stability The unique solvability of the operator equation (Definition 1) (and thus the variational problem (Definition 2)) for any \(f\in X'\) is governed by the Lax-Milgram Lemma, which requires that \(A\) should be \(X\)-elliptic, i.e. \begin{equation} \label {eq:ellipticity-condition} \left \langle Av,v \right \rangle \geq c_1^A\norm {v}_X^2 \quad \forall v\in X. \end{equation} It also prescribes a stability condition on the solution \(u\): \begin{equation} \norm {u}_X\leq \frac {1}{c_1^A}\norm {f}_{X'}. \end{equation} It can be seen that the smaller the ellipticity constant \(c_1^A\), the less the stability for \(u\). \(c_1^A\) is related to the smallest eigenvalue of the discretized matrix for the operator \(A\). A small \(c_1^A\) is caused by an increase in the number of DoFs and sharp corners in the geometry of the model. The error in the data for \(f\) will be amplified by \(\frac {1}{c_1^A}\). Remark 1 (Understanding about ellipticity of operators) We are going to compare the ellipticity condition in Equation (6) with its discrete counterpart. Let \(\mathscr {A}\) be the matrix in \(\mathbb {R}^{n\times n}\) corresponding to \(A\) which is discretized via the Galerkin method. Let \(x\) be the vector in \(\mathbb {R}^n\) related to \(v\). Assume \(\mathscr {A}\) has \(n\) eigenvalues \(\lambda _1,\cdots ,\lambda _n\) and \(n\) eigenvectors \(v_1,\cdots ,v_n\) (N.B. Even though the multiplicity of some of the eigenvalues may be larger than one, for the same number of eigenvectors spanning its eigenspace, we can still explicitly use different symbols to represent their same eigenvalue. Therefore, we allow duplicated values in the list \(\lambda _1,\cdots ,\lambda _n\). Because the sum of the multiplicity of all eigenvalues is \(n\) and all the eigenspaces are orthogonal to each other, we can choose those eigenvectors \(v_1,\cdots ,v_n\) to form an orthonormal set. Then any vector \(x\) in \(\mathbb {R}^n\) can be expanded by this basis: \begin{equation} x = \sum _{i=1}^n c_i v_i. \end{equation} And the duality pairing can be represented as \begin{equation} x^{\mathrm {T}} \mathscr {A} x = \left ( \sum _{i=1}^n c_iv_i^{\mathrm {T}} \right ) \mathscr {A} \left ( \sum _{j=1}^n c_jv_j \right ) = \sum _{i,j} c_ic_j v_i^{\mathrm {T}} \mathscr {A} v_j = \sum _{i,j} c_ic_j \lambda _j (v_i,v_j), \end{equation} where \((\cdot ,\cdot )\) is the inner product in \(\mathbb {R}^n\). Because \(v_1,\cdots ,v_n\) are orthonormal, we have \((v_i,v_j) =\delta _{ij}\). Hence, \begin{equation} x^{\mathrm {T}} \mathscr {A} x = \sum _{i=1}^n \lambda _i c_i^2. \end{equation} If \(\mathscr {A}\) is positive definite, the solution to the linear system \(Ax = b\) exists and is unique. Since all eigenvalues of \(\mathscr {A}\) are larger than zero, we let \(\lambda _{\min }\) be the minimum and then have \begin{equation} \left ( \mathscr {A}x, x \right ) = x^{\mathrm {T}} \mathscr {A} x \geq \lambda _{\min } \sum _{i=1}^n c_i^2 = \lambda _{\min } \lVert x \rVert _{\mathbb {R}^n}^2. \end{equation} We can see that this discrete formulation is consistent with the ellipticity condition. The ellipticity constant \(c_1^A\) of the bounded linear operator \(A\) corresponds to the minimum eigenvalue of the positive definite matrix \(\mathscr {A}\). That’s why the ellipticity condition is the key in the Lax-Milgram Lemma, which governs the existence and uniqueness of the solution for a PDE formed by \(A\). 3 When there is a constraint \(Bu=g\) 3.1 Preconditions \(X\) and \(\Pi \) are Banach spaces 2 with their dual spaces \(X'\) and \(\Pi '\). \(A: X \rightarrow X'\) is a bounded linear operator satisfying \begin{equation} \norm {Av}_{X'}\leq c_2^A\norm {v}_X \quad \forall v \in X. \end{equation} \(B: X \rightarrow \Pi '\) is a bounded linear operator satisfying \begin{equation} \norm {Bv}_{\Pi '} \leq c_2^B\norm {v}_X \quad \forall v\in X. \end{equation} \(B': \Pi \rightarrow X'\) is the adjoint operator of \(B\) 3. \(f\in X'\) and \(g \in \Pi '\) The relationship between various spaces is illustrated in Figure 1, where \(J_X\) and \(J_{\Pi }\) are Riesz maps 4.]]></summary></entry><entry><title type="html">Two kinds of coordinate transformations in differential geometry</title><link href="https://jihuan-tian.github.io/math/2024/10/10/two-kinds-of-coordinate-transformations-in-differential-geometry.html" rel="alternate" type="text/html" title="Two kinds of coordinate transformations in differential geometry" /><published>2024-10-10T00:00:00+08:00</published><updated>2024-10-10T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/10/10/two-kinds-of-coordinate-transformations-in-differential-geometry</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/10/10/two-kinds-of-coordinate-transformations-in-differential-geometry.html"><![CDATA[<p>In differential geometry, the coordinate transformation between different local charts is merged into an entity itself, such as tangent vector, tensor and differential form, instead of being explicitly written out. This makes the expression of physical laws concise and help reveal their invariant properties.</p>

<p>Typically, there are two kinds of coordinate transformations:</p>

<ol>
  <li>
    <p>For a physical entity, such as tangent vector, it has different coordinates or numerical representations in different coordinate charts. Then there is the rule of transformation between these representations.</p>

    <p>For example, we have two coordinate charts \((U,x)\) and \((V,y)\). The transition map from \(U\) to \(V\) is \(\Phi_{VU}: (U,x) \rightarrow (V,y)\), which is actually</p>

\[\begin{equation}
  \begin{aligned}
    y^1 &amp;= y^1(x^1,\cdots,x^{m}) \\
    &amp;\vdots \\
    y^n &amp;= y^n(x^1,\cdots,x^m)
  \end{aligned}.
\end{equation}\]

    <p>N.B. The number of coordinate components in \(v_x\) and \(v_y\) may not be the same. This is natural: \(p\) in \(\mathbb{R}^3\) has three coordinate components, while if \(p\) is on a surface with a local 2D coordinate chart, it has two components.</p>

    <p>Let \(p\) be a point (as a physical entity) in the overlap of \(U\) and \(V\). Let \(v\) be a tangent vector rooted at \(p\). The numerical representations of \(v\) in these two coordinate charts are</p>

\[\begin{equation}
v_x = v_x^i \boldsymbol{\partial} x^i, \; v_y = v_y^i \boldsymbol{\partial} y^i.
\end{equation}\]

    <p>Then the transformation from \(v_x\) to \(v_y\) is the Jacobian map of \(\Phi_{VU}\):</p>

\[\begin{equation}
v_y^i = \frac{\partial y^i}{\partial x^j} v_x^j.
\end{equation}\]
  </li>
  <li>
    <p>For two physical entities, there is also a transformation between them, such as a basis vector in a coordinate chart expanded using the basis of another coordinate chart.</p>

    <p>Assume we have two coordinate charts \((U,x)\) and \((V,y)\), both containing a point \(p\) in \(M^n\). For the tangent space \(M_p^n\) at \(p\), the bases of these two coordinate charts are \(\left\{ \boldsymbol{\partial}x^i \right\}_{i=1}^n\) and \(\left\{ \boldsymbol{\partial}y^j \right\}_{j=1}^{n'}\), where \(n'\) may not be equal to \(n\). In their respective chart, \(\boldsymbol{\partial}x^i\) or \(\boldsymbol{\partial}y^j\) is numerically the same as \(\boldsymbol{e}_i\) or \(\boldsymbol{e}_j\). The numerical representation of \(\boldsymbol{\partial}x^i\) in \((V,y)\) is</p>

\[\begin{equation}
  \boldsymbol{\partial}x^i = \sum_j \frac{\partial y^j}{\partial x^i} \boldsymbol{\partial}y^j.
\end{equation}\]

    <p>Another example is the transformation of a metric tensor \(g\), which is a covariant tensor of 2nd rank. Under the above two coordinate charts, it is expressed as</p>

\[\begin{equation}
  g_{rs}^U = \left\langle \boldsymbol{\partial}x_U^r, \boldsymbol{\partial}x_U^s \right\rangle
\end{equation}\]

    <p>and</p>

\[\begin{equation}
  g_{ij}^V = \left\langle \boldsymbol{\partial}y_V^i, \boldsymbol{\partial}y_V^j \right\rangle.
\end{equation}\]

    <p>The metric tensor comprises basis vectors in the tangent space, so the relationship between \(g\) in different coordinate charts follows the same transformation rule for a basis vector in the tangent space as above. Therefore, we have</p>

\[\begin{equation}
  \begin{aligned}
    \boldsymbol{\partial}y_V^i &amp;= \sum_r \frac{\partial x_U^r}{\partial y_V^i}
    \boldsymbol{\partial}x_U^r \\
    \boldsymbol{\partial}y_V^j &amp;= \sum_s \frac{\partial x_U^s}{\partial y_V^j}
    \boldsymbol{\partial}x_U^s
  \end{aligned}.
\end{equation}\]

    <p>Then</p>

\[\begin{equation}
  g_{ij}^V = \left\langle \sum_r \frac{\partial x_U^r}{\partial y_V^i} \boldsymbol{\partial}x_U^r,
    \sum_s \frac{\partial x_U^s}{\partial y_V^j} \boldsymbol{\partial}x_U^s \right\rangle =
  \sum_r\sum_s \frac{\partial x_U^r}{\partial y_V^i} \frac{\partial x_U^s}{\partial y_V^j}
  \left\langle \boldsymbol{\partial}x_U^r, \boldsymbol{\partial}x_U^s \right\rangle = \sum_r\sum_s
  \frac{\partial x_U^r}{\partial y_V^i} \frac{\partial x_U^s}{\partial y_V^j} g_{rs}^U.
\end{equation}\]

    <p>Its matrix form is</p>

\[\begin{equation}
  G^V = \left[ \frac{\partial x_U}{\partial y_V} \right]^{\mathrm{T}} G^U \left[ \frac{\partial
      x_U}{\partial y_V} \right] = J_{UV}^{\mathrm{T}} G^U J_{UV}.
\end{equation}\]

    <p>N.B. We should not directly substitute the numerical representation of a tangent space basis vector in its respective coordinate chart into the expression of the metric tensor. For example, in the expression of \(g_{rs}^U\), the numerical representations for \(\boldsymbol{\partial}x_U^r\) and \(\boldsymbol{\partial}x_U^s\) in their own coordinate chart \((U,x)\) are \(\boldsymbol{e}_r\) and \(\boldsymbol{e}_s\). If we use \(\boldsymbol{e}_r\) and \(\boldsymbol{e}_s\) to compute \(g_{rs}^U\), we’ll find out its matrix form is simply an identity matrix, which is obviously wrong if the coordinate chart is not orthonormal.</p>

    <p>The correct way to compute \(g_{rs}^U\) is to start from the metric tensor for the Cartesian space \(\mathbb{R}^n\), which is already known as \(g_{pq} = \delta_{pq}\), where \(\delta_{pq}\) is the Kronecker delta. Because the coordinate transformation between the chart \((U,x)\) and the Cartesian space is known, we can compute \(g_{rs}^U\) from \(g_{pq}\) using the above transformation formula. With \(g_{rs}^U\) and the coordinate transformation between \((U,x)\) and \((V,y)\) at hand, we can further compute \(g_{ij}^V\) from \(g_{rs}^U\).</p>

    <p>The above explanation indicates that the computation of a metric tensor for an arbitrary coordinate chart cannot <strong>bootstrap</strong>. It must start from the metric tensor for a Cartesian space \(\mathbb{R}^n\) and a coordinate transformation rule between this chart and \(\mathbb{R}^n\).</p>
  </li>
</ol>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="differential-geometry" /><summary type="html"><![CDATA[In differential geometry, the coordinate transformation between different local charts is merged into an entity itself, such as tangent vector, tensor and differential form, instead of being explicitly written out. This makes the expression of physical laws concise and help reveal their invariant properties.]]></summary></entry><entry><title type="html">Riemannian metric for a submanifold within \(\mathbb{R}^N\)</title><link href="https://jihuan-tian.github.io/math/2024/10/06/riemannian-metric-for-a-submanifold-within-rn.html" rel="alternate" type="text/html" title="Riemannian metric for a submanifold within \(\mathbb{R}^N\)" /><published>2024-10-06T00:00:00+08:00</published><updated>2024-10-06T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/10/06/riemannian-metric-for-a-submanifold-within-rn</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/10/06/riemannian-metric-for-a-submanifold-within-rn.html"><![CDATA[<p><strong>Abstract</strong> The Riemannian metric for a submanifold embedded in a Cartesian space with a higher dimension can be induced from the Riemannian metric for the Cartesian space by pulling it back with respect to the corresponding inclusion map of the submanifold.</p>

<p>The Riemannian metric is defined as a second order covariant tensor, not a 2-form (which is a second order anti-symmetric covariant tensor). Taking \(\mathbb{R}^3\) as example, the Riemannian metric is</p>

\[\begin{equation}
  ds^2 = dx^2+dy^2+dz^2 = dx\otimes dx + dy\otimes dy + dz\otimes dz.
\end{equation}\]

<p>Even though the concept of pull back is initially introduced for differential forms in Frankel’s book “The Geometry of Physics”, its properties still apply to covariant tensors such as the Riemannian metric, because a covariant tensor merely loses the anti-symmetry of a differential form, while the coordinate transformation and other properties are still preserved. If we assign a local coordinate chart \((x,y)\) to a submanifold \(M^2\) embedded in \(\mathbb{R}^3\) and assume the coordinate \(z\) can be differentiably represented by \(x\) and \(y\) as \(z=z(x,y)\), an inclusion map \(i: M^2 \rightarrow \mathbb{R}^3\) can be defined as \((x,y) \rightarrow (x,y,z(x,y))\). Then we want to pull back the Riemannian metric from \(\mathbb{R}^3\) to \(M^2\) with respect to \(i\).</p>

<p>Because the pull back operation is algebraically homeomorphic with respect to the wedge product \(\wedge\), it is also algebraically homeomorphic with respect to the tensor product \(\otimes\). Therefore,</p>

\[\begin{equation}
  i^{*}(dx^2) = i^{*}(dx\otimes dx) = i^{*}(dx)\otimes i^{*}(dx),
\end{equation}\]

<p>where \(i^{\ast}(dx) = \frac{\partial x}{\partial x} dx + \frac{\partial x}{\partial y} dy = dx\). Hence</p>

\[\begin{equation}
  i^{*}(dx^2) = dx^2.
\end{equation}\]

<p>Similarly, \(i^{\ast}(dy^2) = dy^2\). Because \(i^{\ast}(dz) = \frac{\partial z}{\partial x} dx + \frac{\partial z}{\partial y} dy\),</p>

\[\begin{equation}
  \begin{aligned}
    i^{\ast}(dz^2) &amp;= \left( \frac{\partial z}{\partial x} dx + \frac{\partial z}{\partial y} dy \right) \otimes \left( \frac{\partial z}{\partial x} dx + \frac{\partial z}{\partial y} dy \right) \\
    &amp;= \left( \frac{\partial z}{\partial x} \right)^2 dx^2 + \left( \frac{\partial z}{\partial y} \right)^2 dy^2 + 2 \frac{\partial z}{\partial x} \frac{\partial z}{\partial y} dxdy.
  \end{aligned}
\end{equation}\]

<p>Finally,</p>

\[\begin{equation}
  \begin{aligned}
    i^{\ast}(ds^2) &amp;= dx^2 + dy^2 + \left( \frac{\partial z}{\partial x} \right)^2 dx^2 +
  \left( \frac{\partial z}{\partial y} \right)^2 dy^2 + 2 \frac{\partial z}{\partial x}
  \frac{\partial z}{\partial y} dxdy \\
    &amp;= \left( 1 + \left( \frac{\partial z}{\partial x} \right)^2 \right)dx^2 + \left( 1 + \left( \frac{\partial z}{\partial y} \right)^2 \right) dy^2 + 2 \frac{\partial z}{\partial x} \frac{\partial z}{\partial y} dxdy.
  \end{aligned}
\end{equation}\]]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="differential-geometry" /><summary type="html"><![CDATA[Abstract The Riemannian metric for a submanifold embedded in a Cartesian space with a higher dimension can be induced from the Riemannian metric for the Cartesian space by pulling it back with respect to the corresponding inclusion map of the submanifold.]]></summary></entry><entry><title type="html">Integration of differential forms and its relationship with classical calculus</title><link href="https://jihuan-tian.github.io/math/2024/09/26/integration-of-differential-forms-and-its-relationship-with-classical-calculus.html" rel="alternate" type="text/html" title="Integration of differential forms and its relationship with classical calculus" /><published>2024-09-26T00:00:00+08:00</published><updated>2024-09-26T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/09/26/integration-of-differential-forms-and-its-relationship-with-classical-calculus</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/09/26/integration-of-differential-forms-and-its-relationship-with-classical-calculus.html"><![CDATA[<ul>
  <li><a href="#org52a65b5">Introduction</a></li>
  <li><a href="#orgfd454b1">Integration of a \(k\)-form over an oriented region in \(\mathbb{R}^k\)</a></li>
  <li><a href="#orgbf32e8c">Integration of a \(k\)-form over an oriented parameterized \(k\)-subset in a manifold \(M^n\)</a>
    <ul>
      <li><a href="#orgdb71cf7">Line integral</a></li>
      <li><a href="#org063cec0">Surface integral</a></li>
      <li><a href="#orgdf771ed">Volume integral</a></li>
    </ul>
  </li>
</ul>

<p><a id="org52a65b5"></a></p>

<h1 id="introduction">Introduction</h1>

<p>Exterior differential forms are entities to be integrated, not differentiated. For a \(k\)-form, it is to be integrated on a manifold or a subset of a manifold which has \(k\) dimensions. Therefore, a local coordinate chart with \(k\) coordinate variables can be assigned to the integration domain.</p>

<p>Force belongs to the cotangent space, while displacement belongs to the tangent space. The application of a force to a displacement is actually a projection of the force to the displacement vector or taking the inner product of these two quantities, and a scalar value, the work done, is returned. This situation fits well with the application of a cotangent vector to a tangent vector where the concept of duality is involved (see <a href="/math/2024/08/16/concept-of-duality.html">Concept of duality</a>).</p>

<p>For electric field, if it is exerted on a point charge, we get the electric force. Therefore, the electric force is just scaling of the electric field. Hence, the electric field should belong to the cotangent space, i.e. it is a 1-form. If we integrate the electric field 1-form along a path, we obtain the potential drop between the starting and ending points of the path.</p>

<p>For magnetic induction or magnetic flux density, it is associated with a 2-form. When it is applied to a 2-vector, it returns the signed area spanned by the 2-vector which is further scaled by the normal component of the magnetic induction. Therefore, the returned scalar value is the magnetic flux through the area. If we integrate the magnetic induction 2-form on a surface, we obtained the total magnetic flux though the surface.</p>

<p><a id="orgfd454b1"></a></p>

<h1 id="integration-of-a-k-form-over-an-oriented-region-in-mathbbrk">Integration of a \(k\)-form over an oriented region in \(\mathbb{R}^k\)</h1>

<p>For a coordinate chart \((U, u)\) with \(k\) variables embedded in a manifold \(M^n\), it is locally homeomorphic to \(\mathbb{R}^k\). Therefore, the integration on this oriented coordinate chart is just like the integration in a region in \(\mathbb{R}^k\). So we first examine the integration of a \(k\)-form \(\alpha^k = a(u)du^1\wedge\cdots\wedge du^k\) over an oriented region in \(\mathbb{R}^k\).</p>

\[\begin{equation}
\int_{(U,o)} \alpha^k = \int_{(U,o)} a(u) du^1\wedge\cdots\wedge du^k := o(u) \int_{U} a(u) du^1\cdots du^k.
\end{equation}\]

<p>\(o(u)\) determines the orientation of the selected coordinate chart \((u)\).</p>

<p><a id="orgbf32e8c"></a></p>

<h1 id="integration-of-a-k-form-over-an-oriented-parameterized-k-subset-in-a-manifold-mn">Integration of a \(k\)-form over an oriented parameterized \(k\)-subset in a manifold \(M^n\)</h1>

<p>Let \(F\) be a map from \(U\) to \(M^n\), where \(U\) is assigned a coordinate chart \((u)\) with \(k\) variables. The Jacobian matrix of this map \(F\) may not have a full rank, hence the range \(F(U)\) may not have \(k\) dimensions for all points in it. Therefore, \(F(U)\) is called a \(k\)-subset of \(M^n\) and the actual dimension of the geometric entity can be smaller than \(k\).</p>

<p>The \(k\)-form \(\alpha^k\) is to be integrated over this \(k\)-subset. The integral should be pulled back to the local coordinate chart \((u)\) as below.</p>

\[\begin{equation}
  \begin{aligned}
    \int_{(U,o; F)} \alpha^k &amp;= \int_{(U,o)} F^{\ast} \alpha^k \\
    &amp;= \int_{(U,o)}
    (F^{\ast}\alpha^k)(\boldsymbol{\partial}u^1,\cdots,\boldsymbol{\partial}u^k)
    du^1\wedge\cdots\wedge du^k \\
    &amp;= o(u) \int_U
    (F^{\ast}\alpha^k)(\boldsymbol{\partial}u^1,\cdots,\boldsymbol{\partial}u^k)
    du^1\cdots du^k \\
    &amp;= o(u) \int_U
    \alpha^k(F_{\ast}(\boldsymbol{\partial}u^1),\cdots,F_{\ast}(\boldsymbol{\partial}u^k))
    du^1\cdots du^k.
  \end{aligned}
\end{equation}\]

<p>In the above integral, the \(k\)-subset in \(M^n\) is written as \((U,o; F)\), which explicitly specifies the map \(F\) from \(U\) to the \(k\)-subset and the orientation function \(o(\cdot)\). \(F_{\ast}(\boldsymbol{\partial}u^i)\) is the push forward of the basis vector in the tangent space with the coordinate chart \((u)\) to a vector in the manifold \(M^n\). The matrix representation of \(F_{\ast}\) is the Jacobian matrix of \(F\), while the vector representation of \(\boldsymbol{\partial}u^i\) is \((0,\cdots,1,\cdots,0)^{\mathrm{T}}\) with \(1\) being the \(i\)-th component. Therefore, \(F_{\ast}(\boldsymbol{\partial}u^i)\) is just the \(i\)-th column of the Jacobian matrix.</p>

<p><a id="orgdb71cf7"></a></p>

<h2 id="line-integral">Line integral</h2>

<p>In classical calculus, the line integral or path integral of a vector field \(\boldsymbol{A}\) along a path \(c\) is</p>

\[\begin{equation}
  \int_c \boldsymbol{A} \cdot d \boldsymbol{l}.
\end{equation}\]

<p>In the Cartesian space \((x) = (x^1,\cdots,x^d)\), we assume the path \(c\) has a parametric representation \(x(t)\), which depends on the unique parameter \(t\). \(t\) can be scaled to the range \([0, 1]\). Therefore, the infinitesimal directed line segment \(d \boldsymbol{l}\) is</p>

\[\begin{equation}
  d \boldsymbol{l} = \frac{dx}{dt} dt,
\end{equation}\]

<p>where \(t\) can be considered as time and \(\frac{dx}{dt}\) is the velocity vector. Therefore, their multiplication produces the infinitesimal directed line segment \(d \boldsymbol{l}\), which is the displacement within time \(dt\).</p>

<p>We notice that in the above line integral, the inner product is used, which involves the metric of the space. In classical calculus, only Cartesian space is considered, which has an orthonormal basis, so the inner product possesses a simple form. For example, if \(d=3\), we have</p>

\[\begin{equation}
  \int_{c} \boldsymbol{A}\cdot d \boldsymbol{l} = \int_0^1 \sum_{i=1}^3 A^i \frac{dx^i}{dt} dt.
\end{equation}\]

<p>However, if an arbitrary coordinate frame is selected for the space which may not be orthonormal, the metric tensor \(g_{ij}\) will appear, which makes the expression look more complicated:</p>

\[\begin{equation}
\int_c\boldsymbol{A} \cdot d \boldsymbol{l} =  \int_0^1 \sum_{i=1}^3 \left(\sum_{j=1}^3 g_{ij}A^j\right) \frac{dx^i}{dt} dt.
\end{equation}\]

<p>Using the Einstein summation notation, this becomes</p>

\[\begin{equation}
\int_c \boldsymbol{A}\cdot d \boldsymbol{l} = \int_0^1 g_{ij}A^j \frac{dx^i}{dt} dt,
\end{equation}\]

<p>where \(g_{ij}A^j\) is just the 1-form associated with the tangent vector \(\boldsymbol{A}\).</p>

<p>By introducing differential form, the above integral can be simplified, since the metric tensor is inherent in the coordinate transformation property of the differential form. Let \(\alpha^1 = \sum_{i=1}^d a_i dx^i\) be the 1-form associated with \(\boldsymbol{A}\). We pull back its integration from the global frame \((x)\) to the local chart \((t)\):</p>

\[\begin{equation}
  \begin{aligned}
    \int_c \alpha^1 &amp;= \int_c \sum_{i=1}^d a_i(x) dx^i \\
    &amp;= o(t) \int_0^1 F^{\ast} \left( \sum_{i=1}^d a_i dx^i \right) \\
    &amp;= o(t) \int_0^1 \sum_{i=1}^d a_i(x(t)) F^{\ast}(dx^i) \\
    &amp;= o(t) \int_0^1 \sum_{i=1}^d a_i(x(t)) \frac{dx^i}{dt} dt,
  \end{aligned}
\end{equation}\]

<p>where \(a_i(x(t)) = g_{ij}(x(t))A^j(x(t))\). In the local coordinate chart \((t)\), there is only one variable. The orientation of this chart depends on the direction of the unique tangent space basis vector \(dt\). If we assume \(dt\) defines the positive orientation, the integration of a 1-form along a path is equivalent to the line integral of a vector field in classical calculus.</p>

<p><a id="org063cec0"></a></p>

<h2 id="surface-integral">Surface integral</h2>

<p>In classical calculus, the surface integral of a vector field \(\boldsymbol{B}\) is only considered in \(\mathbb{R}^3\), where the coordinate chart is orthonormal.</p>

\[\begin{equation}
  \int_{S} \boldsymbol{B}\cdot d \boldsymbol{S}.
\end{equation}\]

<p>The surface element \(d \boldsymbol{S}\) has a direction, which is the outward unit normal vector \(\hat{\boldsymbol{n}}\) of the surface. The classical method projects \(d \boldsymbol{S}\) to three orthogonal planes, i.e. \(yz\) , \(zx\) and \(xy\) (or we say \(d \boldsymbol{S}\) is projected along three axes, \(x\), \(y\) and \(z\)). If \(\hat{\boldsymbol{n}} = (\cos\alpha,\cos\beta,\cos\gamma)\), the projected surface elements (with signs) are</p>

\[\begin{equation}
  \begin{aligned}
    dy\wedge dz &amp;= \cos\alpha dS \\
    dz\wedge dx &amp;= \cos\beta dS \\
    dx\wedge dy &amp;= \cos\gamma dS,
  \end{aligned}
\end{equation}\]

<p>where \(dS\) is the non-negative area of the surface element \(d \boldsymbol{S}\). Obviously, if \(0 \leq \alpha \leq \frac{\pi}{2}\), the projection of \(d \boldsymbol{S}\) along the \(x\) axis is larger than or equal to 0; when \(\frac{\pi}{2} &lt; \alpha \leq \pi\), the projection is negative. Cases for \(\beta\) and \(\gamma\) are similar.</p>

<p>Then the surface integral is the sum of three parts,</p>

\[\begin{equation}
  \begin{aligned}
    \int_S \boldsymbol{B}\cdot d \boldsymbol{S} &amp;= \int_S B^1 dy\wedge dz + B^2 dz\wedge
    dx + B^3 dx\wedge dy \\
    &amp;= \int_S B^1 \sgn(\cos\alpha) dydz + B^2 \sgn(\cos\beta) dzdx + B^3 \sgn(\cos\gamma)
    dxdy.
  \end{aligned}
\end{equation}\]

<p>We can see that the signs of those direction cosines are explicitly written out, so that those signed wedge products \(dy\wedge dz\), \(dz\wedge dx\) and \(dx\wedge dy\) are replaced with unsigned surface elements \(dydz\), \(dzdx\) and \(dxdy\). The physical meaning of such surface integral is the flux of the vector field \(\boldsymbol{B}\) passing through the surface \(\boldsymbol{S}\).</p>

<p>For the surface integral expressed in differential form, we need a 2-form \(\beta^2\). Let \(F\) be a map from the local coordinate patch \(U\) to a 2-subset in \(\mathbb{R}^3\). This 2-subset is the same as the surface \(S\). The integral is</p>

\[\begin{equation}
  \int_{F(U)} \beta^2 = \int_{F(U)} b_1 dx^2\wedge dx^3 + b_2 dx^3\wedge dx^1 + b_3 dx^1\wedge dx^2.
\end{equation}\]

<p>Pull back to the coordinate chart \((U, u)\):</p>

\[\begin{equation}
  \begin{aligned}
    \int_{F(U)} \beta^2 &amp;= \int_{(U,o)} \left\{ b_1(x(u)) \det \left\{
        \frac{\partial(x^2,x^3)}{\partial(u^1,u^2)} \right\} + b_2(x(u)) \det \left\{
        \frac{\partial(x^3,x^1)}{\partial(u^1,u^2)} \right\} + b_3(x(u)) \det \left\{
        \frac{\partial(x^1,x^2)}{\partial(u^1,u^2)} \right\} \right\}  du^1\wedge du^2 \\
    &amp;= o(u) \int_U \left\{ b_1(x(u)) \det \left\{
        \frac{\partial(x^2,x^3)}{\partial(u^1,u^2)} \right\} + b_2(x(u)) \det \left\{
        \frac{\partial(x^3,x^1)}{\partial(u^1,u^2)} \right\} + b_3(x(u)) \det \left\{
        \frac{\partial(x^1,x^2)}{\partial(u^1,u^2)} \right\} \right\}  du^1du^2.
  \end{aligned}
\end{equation}\]

<p>Or we can write it as</p>

\[\begin{equation}
  \begin{aligned}
    \int_{F(U)} \beta^2 &amp;= \int_{(U,o)} F^{\ast}\beta^2 \\
    &amp;= \int_{(U,o)} (F^{\ast}\beta^2)(\boldsymbol{\partial}u^1, \boldsymbol{\partial}u^2)
    du^1\wedge du^2 \\
    &amp;= o(u) \int_U (F^{\ast}\beta^2)(\boldsymbol{\partial}u^1, \boldsymbol{\partial}u^2)
    du^1du^2 \\
    &amp;= o(u) \int_U \beta^2(F_{\ast}\boldsymbol{\partial}u^1, F_{\ast}\boldsymbol{\partial}u^2)
    du^1du^2 \\
    &amp;= o(u) \int_U \beta^2\left(\frac{\partial x}{\partial u^1}, \frac{\partial x}{\partial
      u^2}\right) du^1du^2.
  \end{aligned}
\end{equation}\]

<p>We already know that in an \(n\)-dimensional manifold, a tangent vector \(\boldsymbol{B}\) can be identified with a pseudo \((n-1)\)-form via its interior product with the pseudo volume \(n\)-form. Because the above integral obviously depends on the orientation of the coordinate chart, \(\beta^2\) should be a true form. Then the association of a tangent vector and an \((n-1)\)-form should be modified a little like this: a <strong>pseudo</strong> tangent vector \(\boldsymbol{B}\) can be identified with a <strong>true</strong> \((n-1)\)-form via its interior product with the pseudo volume \(n\)-form. Hence, when \(n=3\), the pseudo vector \(\boldsymbol{B}\) corresponds to a true \(2\)-form \(\beta^2\). N.B. The selection of \(\boldsymbol{B}\) as a pseudo vector is meaningful. For example, in electromagnetics, the magnetic flux density \(\boldsymbol{B}\) is a pseudo vector not a true vector, because when the orientation of the coordinate charge changes, its direction in the space flips.</p>

<p>Then the 2-form \(\beta^2\) is</p>

\[\begin{equation}
  \beta^2 = i_{\boldsymbol{B}}\mathrm{vol}^3.
\end{equation}\]

<p>Its integration over \(F(U)\) is</p>

\[\begin{equation}
  \begin{aligned}
    \int_{F(U)} \beta^2 &amp;= \int_{F(U)} i_{\boldsymbol{B}}\mathrm{vol}^3 \\
    &amp;= o(u) \int_U i_{\boldsymbol{B}}\mathrm{vol}^3 \left( \frac{\partial x}{\partial u^1}, \frac{\partial x}{\partial
        u^2} \right) du^1du^2 \\
    &amp;= o(u) \int_U \mathrm{vol}^3\left(\vect{B}, \frac{\partial x}{\partial u^1}, \frac{\partial x}{\partial
        u^2}\right) du^1du^2.
  \end{aligned}
\end{equation}\]

<p>In the above expression, both \(\mathrm{vol}^3\) and \(\boldsymbol{B}\) are pseudo and depend on the orientation of \(\mathbb{R}^3\). However, because they appear together, the above integral does not depend on the orientation of \(\mathbb{R}^3\) but only depends on the orientation of the local coordinate chart \((U,u)\). This means the flux of a vector field through a directed surface only depends on the orientation of the surface but not on the 3D space, which is reasonable.</p>

<p>We can select the surface unit normal vector \(\hat{\boldsymbol{n}}\) such that the three vectors \((\hat{\boldsymbol{n}}, \frac{\partial x}{\partial u^{1}}, \frac{\partial x}{\partial u^{2}})\) (which have been pushed forward into \(\mathbb{R}^3\)) have the same orientation as that for \(\mathbb{R}^3\). With the normal vector defined, we can decompose the vector field \(\boldsymbol{B}\) into a normal component \(\boldsymbol{B}_{\mathrm{n}}\) and a tangential component \(\boldsymbol{B}_{\mathrm{t}}\), where \(\boldsymbol{B}_{\mathrm{n}} = (\boldsymbol{B}\cdot\hat{\boldsymbol{n}})\hat{\boldsymbol{n}}\) and \(\boldsymbol{B}_{\mathrm{t}} = \boldsymbol{B} - \boldsymbol{B}_{\mathrm{n}}\). Then the volume form in the above integral can be further expanded as</p>

\[\begin{equation}
  o(u) \int_U \mathrm{vol}^3 \left(\boldsymbol{B}, \frac{\partial x}{\partial u^{1}}, \frac{\partial
      x}{\partial u^{2}} \right) du^1du^2 = o(u) \int_{U} \mathrm{vol}^3 \left(
    \vect{B}_{\mathrm{n}} + \vect{B}_{\mathrm{t}}, \frac{\partial
      x}{\partial u^{1}}, \frac{\partial x}{\partial u^{2}} \right) du^1du^2.
\end{equation}\]

<p>Because \(\boldsymbol{B}_{\mathrm{t}}\) lies in the plane spanned by the two tangent vectors \(\frac{\partial x}{\partial u^{1}}\) and \(\frac{\partial x}{\partial u^{2}}\), \(\boldsymbol{B}_{\mathrm{t}}\) does not contribute to the volume form. Therefore, the integral becomes</p>

\[\begin{equation}
  \begin{aligned}
    o(u) \int_{U} \mathrm{vol}^3 \left( \vect{B}_{\mathrm{n}}, \frac{\partial  x}{\partial
      u^{1}}, \frac{\partial x}{\partial u^{2}} \right) du^1du^2 &amp;=  o(u) \int_{U}
  \mathrm{vol}^3 \left( (\vect{B}\cdot\hat{\vect{n}})\hat{\vect{n}}, \frac{\partial
      x}{\partial u^{1}}, \frac{\partial x}{\partial u^{2}} \right) du^1du^2 \\
  &amp;= o(u)
  \int_{U} (\vect{B}\cdot\hat{\vect{n}}) \mathrm{vol}^3 \left( \hat{\vect{n}},
    \frac{\partial x}{\partial u^{1}}, \frac{\partial x}{\partial u^{2}} \right) du^1du^2
  \\
  &amp;= o(u)
  \int_{U} B_{\mathrm{n}} \left( i_{\hat{\vect{n}}}\mathrm{vol}^3 \right) \left(
    \frac{\partial x}{\partial u^{1}}, \frac{\partial x}{\partial u^{2}} \right) du^1du^2,
  \end{aligned}
\end{equation}\]

<p>where \(B_{\mathrm{n}}\) is the normal component (in scalar value) of the vector field \(\boldsymbol{B}\) and \(i_{\hat{\boldsymbol{n}}}\mathrm{vol}^{3}\) is the area 2-form. Because \(\hat{\boldsymbol{n}}\) is a pseudo vector and \(\mathrm{vol}^3\) is a pseudo form, this area 2-form is a true form. If we let</p>

\[\begin{equation}
  dS := \left( i_{\hat{\vect{n}}}\mathrm{vol}^3 \right) \left(
    \frac{\partial x}{\partial u^{1}}, \frac{\partial x}{\partial u^{2}} \right) du^1du^2,
\end{equation}\]

<p>we have</p>

\[\begin{equation}
\int_{F(U)} \beta^2 = o(u) \int_U B_n dS,
\end{equation}\]

<p>which is consistent with the flux integral we’ve learned in classical calculus.</p>

<p><a id="orgdf771ed"></a></p>

<h2 id="volume-integral">Volume integral</h2>

<p>A typical example of the volume integral of a scalar field in \(\mathbb{R}^3\) is the integral of charge density \(\rho\). In classical calculus, this is simply a tri-fold integral</p>

\[\begin{equation}
 \int_V \rho dxdydz.
\end{equation}\]

<p>It is obvious that the integral does not depends on the orientation of the coordinate frame.</p>

<p>If differential form is used, we identify the charge density \(\rho\) with the pseudo volume 3-form \(\rho \mathrm{vol}^3\). Then its integral over a volume \(F(U)\) is</p>

\[\begin{equation}
  \int_{F(U)} \rho \mathrm{vol}^3 = \int_U \rho(u) \mathrm{vol}^3 \left( \frac{\partial
      x}{\partial u^{1}}, \frac{\partial x}{\partial u^{2}}, \frac{\partial x}{\partial
      u^{3}} \right) du^1du^2du^3.
\end{equation}\]

<p>where \(o(u)\) in the pseudo volume form cancels with the \(o(u)\) in the integral and the final result does not depend on the orientation. Or the integral can be written as</p>

\[\begin{equation}
  \begin{aligned}
    \int_{(F(U), o)} \rho \mathrm{vol}^3 &amp;= \int_{(F(U), o)} \rho(x) o(x) dx^1\wedge x^2\wedge dx^3 \\
    &amp;= \int_{(U,o)} F^{\ast}\left( \rho(x) o(x) dx^1\wedge dx^2\wedge dx^3 \right) \\
    &amp;= \int_{(U,o)} \rho(x(u)) o(u) \Abs{\det \left\{
        \frac{\partial(x^1,x^2,x^3)}{\partial(u^1,u^2,u^3)} \right\}} du^1\wedge
    du^2\wedge du^3\\
    &amp;= \int_U \rho(x(u)) \Abs{\det \left\{
        \frac{\partial(x^1,x^2,x^3)}{\partial(u^1,u^2,u^3)} \right\}} du^1du^2du^3.
  \end{aligned}
\end{equation}\]]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="differential-geometry" /><summary type="html"><![CDATA[Introduction Integration of a \(k\)-form over an oriented region in \(\mathbb{R}^k\) Integration of a \(k\)-form over an oriented parameterized \(k\)-subset in a manifold \(M^n\) Line integral Surface integral Volume integral]]></summary></entry><entry><title type="html">Understanding about the orientation dependence of pseudo differential forms</title><link href="https://jihuan-tian.github.io/math/2024/09/25/understanding-about-the-orientation-dependence-of-pseudo-differential-forms.html" rel="alternate" type="text/html" title="Understanding about the orientation dependence of pseudo differential forms" /><published>2024-09-25T00:00:00+08:00</published><updated>2024-09-25T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/09/25/understanding-about-the-orientation-dependence-of-pseudo-differential-forms</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/09/25/understanding-about-the-orientation-dependence-of-pseudo-differential-forms.html"><![CDATA[<p>Consider a \(d\)-dimensional manifold \(M^d\) with two overlapping coordinate charts \((U, u)\) and \((V,v)\). Both of them are orthonormal and \((V,v)\) has a different orientation from \((U,u)\). Let \(o(\cdot)\) be the orientation function which returns 1 or -1 for a coordinate chart. Then \(o(v) = -o(u)\).</p>

<p>In \((U,u)\), when the full degree differential form \(du^1\wedge \cdots \wedge du^n\) is applied to an \(n\)-vector \((v_1,\cdots,v_n)\) rooted at a point \(p\) in \(U\cap V\), the returned scalar is the signed volume spanned by the \(n\)-vector. To measure the signed volume in the coordinate chart \((V,v)\), we still apply the full degree differential from \(dv^1\wedge\cdots\wedge dv^n\) to \(v_1,\cdots,v_n\). Then the returned volume must have a different sign from that using \((U,u)\), because \(o(v) = -o(u)\). N.B. Even though the \(n\)-vector has different numerical representations in these two coordinate charts, it is a fixed physical entity in the space.</p>

<p>Because the \(n\)-vector is a fixed entity, we want to make the two returned scalar values consistent under different coordinate charts. To make <strong>the returned volume not dependent on the orientation</strong>, we need to make <strong>the full degree differential form orientation dependent</strong> by adding the orientation function \(o(\cdot)\) as a factor. In this way, we obtain the volume form for an arbitrary coordinate chart \((u)\) as</p>

\[\begin{equation}
\mathrm{vol}_u^n = o(u) du^1\wedge\cdots\wedge du^n.
\end{equation}\]

<p>N.B. The indices of the basis elements in the wedge product must be in an increasing order. Because the volume form is made orientation dependent to ensure its measurement of signed volume compatible in different coordinate charts, it is different from a normal differential form and is called a <strong>pseudo form</strong>.</p>

<p>Take the Cartesian space \(\mathbb{R}^3\) as an example.</p>

<p><img src="/figures/2024-09-25-19-47-volume-forms-in-different-charts.png" alt="img" /></p>

<p>As a convention, we adopt the frame \((u) = (x, y, z)\) with the right handedness and assume that \(o(u) = 1\). The volume form is \(\mathrm{vol}_u^3 = o(u) dx\wedge dy \wedge dz = dx\wedge dy \wedge dz\). Then we want to get the signed volume spanned by the basis vectors of the three axes, \(\boldsymbol{v}_1 = (1,0,0)\), \(\boldsymbol{v}_2 = (0,1,0)\) and \(\boldsymbol{v}_3 = (0,0,1)\). We have</p>

\[\begin{equation}
\mathrm{vol}_u^3(\boldsymbol{v}_1,\boldsymbol{v}_2,\boldsymbol{v}_3) = \det \begin{pmatrix}
  1 &amp; 0 &amp; 0\\
  0 &amp; 1 &amp; 0\\
  0 &amp; 0 &amp; 1
\end{pmatrix} = 1.
\end{equation}\]

<p>If we choose another frame \((v) = (x, z, y)\) with a different orientation \(o(v)=-1\) by swapping \(y\) and \(z\) coordinates, the volume form becomes</p>

\[\begin{equation}
\label{eq:volume-form-in-v}
\mathrm{vol}_v^3 = o(v) dv^1\wedge dv^2 \wedge dv^3 = -dv^1\wedge dv^2 \wedge dv^3.
\end{equation}\]

<p>In this new chart, the 3-vector becomes \(\boldsymbol{v}'_1 = (1,0,0)\), \(\boldsymbol{v}'_2 = (0,0,1)\) and \(\boldsymbol{v}'_3 = (0,1,0)\). Then the signed volume spanned by them is</p>

\[\begin{equation}
\mathrm{vol}_v^3(\boldsymbol{v}'_1,\boldsymbol{v}'_2,\boldsymbol{v}'_3) = -\det \begin{pmatrix}
  1 &amp; 0 &amp; 0\\
  0 &amp; 0 &amp; 1\\
  0 &amp; 1 &amp; 0
\end{pmatrix} = 1.
\end{equation}\]

<p>So the returned volume in \((V,v)\) is the same as that in \((U,u)\).</p>

<p>If we substitute \((x, z, y)\) for \(v^1,v^2,v^3\) in Equation \eqref{eq:volume-form-in-v}, using the anti symmetry of wedge product,</p>

\[\begin{equation}
\mathrm{vol}_v^3 = -dx\wedge dz \wedge dy = dx\wedge dy \wedge dz = \mathrm{vol}_u^3,
\end{equation}\]

<p>we find out \(\mathrm{vol}_v^3\) is just equivalent to \(\mathrm{vol}_u^3\).</p>

<p>Also note that for a given pseudo form, with the anti symmetry of wedge product, we can arbitrarily change the order of basis elements in the wedge product \(du^1\wedge\cdots\wedge du^n\) while keeping the differential form unchanged, as long as we add a factor \((-1)^m\) to it, where \(m\) is the number of permutations. However, such an operation has nothing to do with the orientation of the local coordinate chart, since the chart does not change in this process. So this is only an algebraic operation.</p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="differential-geometry" /><summary type="html"><![CDATA[Consider a \(d\)-dimensional manifold \(M^d\) with two overlapping coordinate charts \((U, u)\) and \((V,v)\). Both of them are orthonormal and \((V,v)\) has a different orientation from \((U,u)\). Let \(o(\cdot)\) be the orientation function which returns 1 or -1 for a coordinate chart. Then \(o(v) = -o(u)\).]]></summary></entry><entry><title type="html">The interplay between triangulation, manifold, mapping, quadrature point and finite element</title><link href="https://jihuan-tian.github.io/math/2024/09/24/the-interplay-between-triangulation-manifold-mapping-quadrature-point-and-finite-element.html" rel="alternate" type="text/html" title="The interplay between triangulation, manifold, mapping, quadrature point and finite element" /><published>2024-09-24T00:00:00+08:00</published><updated>2024-09-24T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/09/24/the-interplay-between-triangulation-manifold-mapping-quadrature-point-and-finite-element</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/09/24/the-interplay-between-triangulation-manifold-mapping-quadrature-point-and-finite-element.html"><![CDATA[<p>In FEM or BEM, the five entities, namely triangulation, manifold, mapping, quadrature point and finite element, work together to discretize the spatial domain and prepare necessary data for cellwise computation. This article explains their functions and relationships.</p>

<p><img src="/figures/2024-09-23_21-14-52-interplay-of-triangulation-manifold-mapping-etc.png" alt="img" /></p>

<ol>
  <li>
    <p>Triangulation</p>

    <p>A triangulation describes the topological relationship between vertices and cells. It contains these data:</p>
    <ul>
      <li>Coordinates for vertices</li>
      <li>A cell is represented as an ordered list of vertex indices. This order is consistent with the normal direction of the cell and the cell normal inherits from the normal direction of the geometric entity to which the cell belongs.</li>
    </ul>
  </li>
  <li>
    <p>Manifold</p>

    <p>A manifold is used to generate new <strong>geometry-conforming</strong> points in a cell in the triangulation. For example, two vertices defines an edge of a cell and a new point in the middle of this edge is computed as a weighted average of the two vertices. Similarly, given four vertices of a quadrangular cell, a new point in the center of this cell is computed as a weighted average of the four vertices.</p>

    <p>Generating new points in a cell is needed in two cases:</p>

    <ul>
      <li>Mesh refinement</li>
      <li>Define the support points of mapping shape functions, if the Lagrange type mapping is adopted.</li>
    </ul>
  </li>
  <li>
    <p>Mapping</p>

    <p>A mapping object has two purposes:</p>

    <ul>
      <li>Defines a map from the unit/reference cell to a real cell. When coming to a different real cell in the triangulation, the mathematical function implied by this map changes because the coordinates of the cell vertices are different. With the language of differential geometry, the mapping object assigns a locate coordinate chart to each cell in the triangulation. The local coordinate charts from all cells form an atlas over the triangulation.</li>
      <li>Compute quantities such as Jacobian matrices and determinants, normal vectors and covariant transformation matrices, etc.
        <ul>
          <li>A Jacobian matrix is used to transform a tangent vector from the local coordinate chart to the global Cartesian frame.</li>
          <li>The Jacobian determinant is used when an integral is pulled back from the global Cartesian frame to the local coordinate chart.</li>
          <li>For a surface cell with a manifold dimension 2 and a spatial dimension 3, the normal vector at a point in this cell is the cross product of the two basis vectors for the local coordinate chart. Some BEM kernel functions such as the double layer kernel and its adjoint need the normal vector information.</li>
          <li>
            <p>The covariant matrix is used to transform a gradient vector from the local coordinate chart to the global Cartesian frame. The procedure is</p>

            <ul>
              <li>Transform the 1-form in the local coordinate chart to tangent vector via the inverse of the metric tensor $G^{-1} $.</li>
              <li>Push forward the tangent vector in the local coordinate chart to the global Cartesian frame via the Jacobian matrix $J $.</li>
            </ul>

            <p>The total transformation is the covariant matrix $J G^{-1} $.</p>
          </li>
        </ul>
      </li>
    </ul>

    <p>Examples of mapping objects:</p>

    <ul>
      <li>
        <p>Lagrange type mapping</p>

        <p>It uses Lagrange polynomials as shape functions to construct the map from the unit cell to a real cell. The non-zero part of each shape function concentrates around a support point. Therefore, the higher the order of the polynomials, the more the shape functions and hence the more the support points. When the polynomial order is 1, i.e. the map is linear, the four vertices of a cell are just the support points of all the four shape functions. When the polynomial order is larger than 1, more support points are needed which are located either in the middle of edges or in the cell interior. As mentioned above, the generation of these intermediate support points needs to consult the manifold object.</p>
      </li>
      <li>
        <p>Manifold conforming mapping</p>

        <p>In this case, the map from the unit cell to a real cell conforms exactly with the manifold. The computation of quantities such as Jacobian matrices and determinants etc. will depend on how the manifold is defined.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Quadrature points</p>

    <p>Quadrature points are generated in the unit cell. With a mapping object, they can be transformed to points in a real cell.</p>
  </li>
  <li>
    <p>Finite element</p>

    <p>The evaluation of the shape functions and their derivatives for a finite element is performed at all quadrature points in the unit cell. These are fixed values, which can be precomputed and stored. The vector forms of differential forms in the global Cartesian frame can be computed from these fixed values and transformed to a real cell with the help of the mapping object. And the mapping object further depends on the manifold object.</p>
  </li>
</ol>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="BEM" /><summary type="html"><![CDATA[In FEM or BEM, the five entities, namely triangulation, manifold, mapping, quadrature point and finite element, work together to discretize the spatial domain and prepare necessary data for cellwise computation. This article explains their functions and relationships.]]></summary></entry><entry><title type="html">Concept of duality</title><link href="https://jihuan-tian.github.io/math/2024/08/16/concept-of-duality.html" rel="alternate" type="text/html" title="Concept of duality" /><published>2024-08-16T00:00:00+08:00</published><updated>2024-08-16T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/08/16/concept-of-duality</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/08/16/concept-of-duality.html"><![CDATA[<p class='indent'>   Duality can be considered as a weak form of symmetry. According to Wikipedia, in mathematics, a duality
translates concepts, theorems or mathematical structures into other concepts, theorems or structures in a
one-to-one fashion, often (but not always) by means of an involution operation: if the dual of A is B, then the
dual of B is A, i.e. if the involution is represented as \(f\), we have \(f(f(A)) = A\).
</p><!-- l. 28 --><p class='indent'>   Taking the familiar inner product in linear algebra as an example (see Figure <a href='#x1-2r1'>1<!-- tex4ht:ref: fig:inner-product-of-vectors  --></a>). The two vectors \(a\) and \(b\) have
equal status, both of which are vectors in \(\mathbb {R}^n\). The meaning of inner product \(\left \langle a,b \right \rangle \) or \(a\cdot b\) is projecting \(b\) to \(a\). From the view of
measurement, \(a\) is the measurement device and \(b\) is the object to be measured. For example, \(a\) can be imagined as an
electric field meter, \(b\) is the electric field. When the meter is aligned with the electric field line, it registers
maximum value. With this understanding, even though both \(a\) and \(b\) are vectors in \(\mathbb {R}^n\), we have a feeling that they
have something different. If we adopt a matrix form to represent their inner product , \(a\) is a row vector, i.e. an \(1 \times n\)
matrix, and \(b\) is a column vector, i.e. \(n\times 1\) matrix. Such a formal difference implies they are different
types of vectors and the concept of duality is embodied in the correspondence between row and
column.
</p>
   <figure class='figure'> 

                                                                                               
                                                                                               
<a id='x1-2r1'></a>
                                                                                               
                                                                                               
<!-- l. 44 --><p class='noindent'><img alt='PIC'  src='/figures/inner-product-of-vectors-draft.png'  />
</p>
<figcaption class='caption'><span class='id'>Figure 1: </span><span class='content'></span></figcaption><!-- tex4ht:label?: x1-2r1  -->
                                                                                               
                                                                                               
   </figure>
<!-- l. 49 --><p class='indent'>   Adopting the functional analysis point of view, \(\left \langle a,b \right \rangle \) or \(a\cdot b\) can be written as \(a(b)\). Now \(b\) is an element in the space \(X\) and \(a: X \rightarrow \mathbb {R}\) is
a linear functional or operator in the dual space \(X'\). In essence, \(a\) and \(b\) have different status and they are dual to each
other.
</p><!-- l. 54 --><p class='indent'>   In computer programming, \(a\) can be considered as a function or more generally a function object, while \(b\) is the
data or operand to be manipulated.
</p><!-- l. 57 --><p class='indent'>   In differential geometry, the core concepts of tangent space and cotangent space are dual to each other. Let \(M^n\) be
an \(n\)-dimensional manifold which contains the point \(p\). \(M_p^n\) is the tangent space at \(p\). The dual space of \(M_p^n\) is the called
the cotangent space at \(p\), which is written as \(M_p^{n*}\). A linear operator \(\alpha : M_p^n \rightarrow \mathbb {R}\) in the cotangent space \(M_p^{n*}\) is called a
cotangent vector or covariant vector or covector or 1-form. Applying this 1-form \(\alpha \) to a tangent vector
\(b\), we obtain a scalar value. Due to Riesz representation theorem <a href='#x1-31'>1<!-- tex4ht:ref: theo:riesz-representation  --></a>, there exists a unique tangent
vector \(a\) such that \(\alpha (b) = \left \langle a, b \right \rangle \). This means the behavior of \(\alpha \) is governed by the inner product with respect to
\(a\).
</p>
   <div class='Theorem'><div class='newtheorem'>
<!-- l. 68 --><p class='noindent'><span class='head'>
<span class='p1xb-x-x-109'>Theorem 1 (Riesz representation theorem)</span> </span><a id='x1-4'></a><a id='x1-31'></a> <span class='p1xi-x-x-109'>Let</span> \(X\) <span class='p1xi-x-x-109'>be a Hilbert space and</span> \(f\) <span class='p1xi-x-x-109'>be a bounded linear functional on</span> \(X\)<span class='p1xi-x-x-109'>,
</span><span class='p1xi-x-x-109'>i.e.</span> \(f \in X'\)<span class='p1xi-x-x-109'>. There exists a unique vector</span> \(a\) <span class='p1xi-x-x-109'>in</span> \(X\)<span class='p1xi-x-x-109'>, such that for all</span> \(b\) <span class='p1xi-x-x-109'>in</span> \(X\)<span class='p1xi-x-x-109'>,</span> \(f(b) = \left \langle a, b \right \rangle \) <span class='p1xi-x-x-109'>and</span> \(\norm {f} = \norm {a}\)<span class='p1xi-x-x-109'>.</span>
</p>
   </div>
<!-- l. 73 --><p class='indent'>   </p></div>
<!-- l. 75 --><p class='indent'>   In quantum mechanics, \(\langle \varphi |\) is in the dual space and \(|\psi \rangle \) is in the original space.
</p><!-- l. 78 --><p class='indent'>   In FEM or BEM, the mass matrix \(\mathcal {M}\) involves the projection of a basis function \(\varphi _j\) in the ansatz or trial space to a
basis function \(\psi _i\) in the test space \begin{equation}  \mathcal {M}_{ij} = \int _\Omega \psi _i(x) \varphi _j(x) dx.  \end{equation}<a id='x1-5r1'></a> \(\psi _i\) and \(\varphi _j\) can also be considered dual to each other.
</p>

<p>Related: <a href="/math/2023/07/07/measurement-and-duality.html">Measurement and duality</a></p>

<p>Backlinks: <a href="/math/2024/09/26/integration-of-differential-forms-and-its-relationship-with-classical-calculus.html">《Integration of differential forms and its relationship with classical calculus》</a></p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="duality" /><summary type="html"><![CDATA[Duality can be considered as a weak form of symmetry. According to Wikipedia, in mathematics, a duality translates concepts, theorems or mathematical structures into other concepts, theorems or structures in a one-to-one fashion, often (but not always) by means of an involution operation: if the dual of A is B, then the dual of B is A, i.e. if the involution is represented as \(f\), we have \(f(f(A)) = A\). Taking the familiar inner product in linear algebra as an example (see Figure 1). The two vectors \(a\) and \(b\) have equal status, both of which are vectors in \(\mathbb {R}^n\). The meaning of inner product \(\left \langle a,b \right \rangle \) or \(a\cdot b\) is projecting \(b\) to \(a\). From the view of measurement, \(a\) is the measurement device and \(b\) is the object to be measured. For example, \(a\) can be imagined as an electric field meter, \(b\) is the electric field. When the meter is aligned with the electric field line, it registers maximum value. With this understanding, even though both \(a\) and \(b\) are vectors in \(\mathbb {R}^n\), we have a feeling that they have something different. If we adopt a matrix form to represent their inner product , \(a\) is a row vector, i.e. an \(1 \times n\) matrix, and \(b\) is a column vector, i.e. \(n\times 1\) matrix. Such a formal difference implies they are different types of vectors and the concept of duality is embodied in the correspondence between row and column.]]></summary></entry><entry><title type="html">Shape functions and basis functions in FEM</title><link href="https://jihuan-tian.github.io/math/2024/08/07/shape-functions-and-basis-functions-in-fem.html" rel="alternate" type="text/html" title="Shape functions and basis functions in FEM" /><published>2024-08-07T00:00:00+08:00</published><updated>2024-08-07T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/08/07/shape-functions-and-basis-functions-in-fem</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/08/07/shape-functions-and-basis-functions-in-fem.html"><![CDATA[<div class="comment">According to my graduate student experience, the difference between shape functions and basis functions in FEM was not clarified in my numerical method course. Therefore, in this article, I illustrate and explain this point with the help of the symbolic math software <a href="https://maxima.sourceforge.io/">Maxima</a>.</div>

<h2>
  1 Lagrange polynomials as shape functions on reference cells
</h2>


<!-- Text cell -->


<div class="comment">
Load packages.
</div>


<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
(%i1)	
  </span></td>
  <td><span class="input">
<span class="code_function">kill</span><span class="code_operator">(</span><span class="code_variable">all</span><span class="code_operator">)</span><span class="code_endofline">$</span>  </span></td>
</tr></table>



<!-- Text cell -->


<div class="comment">

</div>


<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
(%i6)	
  </span></td>
  <td><span class="input">
<span class="code_function">load</span><span class="code_operator">(</span><span class="code_string">&quot;eigen&quot;</span><span class="code_operator">)</span><span class="code_endofline">$</span><span class="code_endofline"><br/>
</span><span class="code_function">load</span><span class="code_operator">(</span><span class="code_string">&quot;fem.mac&quot;</span><span class="code_operator">)</span><span class="code_endofline">$</span><span class="code_endofline"><br/>
</span><span class="code_function">load</span><span class="code_operator">(</span><span class="code_string">&quot;draw&quot;</span><span class="code_operator">)</span><span class="code_endofline">$</span><span class="code_endofline"><br/>
</span><span class="code_function">load</span><span class="code_operator">(</span><span class="code_string">&quot;color_map.mac&quot;</span><span class="code_operator">)</span><span class="code_endofline">$</span><span class="code_endofline"><br/>
</span><span class="code_function">load</span><span class="code_operator">(</span><span class="code_string">&quot;mnewton&quot;</span><span class="code_operator">)</span><span class="code_endofline">$</span><span class="code_endofline"><br/>
</span><span class="code_function">load</span><span class="code_operator">(</span><span class="code_string">&quot;tex_format.mac&quot;</span><span class="code_operator">)</span><span class="code_endofline">$</span>  </span></td>
</tr></table>



<!-- Text cell -->


<div class="comment">
Plot configurations
</div>


<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
(%i12)	
  </span></td>
  <td><span class="input">
<span class="code_variable">fig_size</span> <span class="code_operator">:</span> <span class="code_operator">[</span><span class="code_number">1280</span><span class="code_endofline">,</span><span class="code_number">1024</span><span class="code_operator">]</span><span class="code_endofline">$</span><span class="code_endofline"><br/>
</span><span class="code_variable">wxplot_size</span> <span class="code_operator">:</span> <span class="code_variable">fig_size</span><span class="code_endofline">$</span><span class="code_endofline"><br/>
</span><span class="code_variable">fig_font</span> <span class="code_operator">:</span> <span class="code_string">&quot;Helvetica&quot;</span><span class="code_endofline">$</span><span class="code_endofline"><br/>
</span><span class="code_variable">fig_font_size</span> <span class="code_operator">:</span> <span class="code_number">20</span><span class="code_endofline">$</span><span class="code_endofline"><br/>
</span><span class="code_variable">wxanimate_autoplay</span> <span class="code_operator">:</span> <span class="code_function">false</span><span class="code_endofline">$</span><span class="code_endofline"><br/>
</span><span class="code_variable">wxanimate_framerate</span> <span class="code_operator">:</span> <span class="code_number">1</span><span class="code_endofline">$</span>  </span></td>
</tr></table>



<!-- Subsection cell -->


<h3>
  1.1 1D
</h3>


<!-- Text cell -->


<div class="comment">
Generate the four 3rd order Lagrange polynomials on $[0, 1]$.
</div>


<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
(%i13)	
  </span></td>
  <td><span class="input">
<span class="code_variable">reference_cell_1d</span> <span class="code_operator">:</span> <span class="code_operator">[</span><span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_operator">]</span><span class="code_endofline">$</span>  </span></td>
</tr></table>



<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
(%i14)	
  </span></td>
  <td><span class="input">
<span class="code_variable">support_points_1d</span> <span class="code_operator">:</span> <span class="code_function">GenLagrangeNodes1D</span><span class="code_operator">(</span><span class="code_number">3</span><span class="code_endofline">,</span> <span class="code_variable">reference_cell_1d</span><span class="code_operator">)</span><span class="code_endofline">;</span>  </span></td>
</tr></table>
<p>
\[\operatorname{(support\_ points\_ 1d)	}\left[ 0\operatorname{,}\frac{1}{3}\operatorname{,}\frac{2}{3}\operatorname{,}1\right] \]
</p>


<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
(%i16)	
  </span></td>
  <td><span class="input">
<span class="code_variable">shape_functions_1d</span> <span class="code_operator">:</span> <span class="code_function">GenLagrangeBases1D</span><span class="code_operator">(</span><span class="code_number">3</span><span class="code_endofline">,</span> <span class="code_variable">reference_cell_1d</span><span class="code_endofline">,</span> <span class="code_operator">'</span><span class="code_variable">ξ</span><span class="code_operator">)</span><span class="code_endofline">$</span><span class="code_endofline"><br/>
</span><span class="code_function">PrintEquationList</span><span class="code_operator">(</span><span class="code_variable">shape_functions_1d</span><span class="code_operator">)</span><span class="code_endofline">;</span>  </span></td>
</tr></table>
<p>
\[\]\[-\frac{9 \left( \xi -1\right) \, \left( \xi -\frac{2}{3}\right) \, \left( \xi -\frac{1}{3}\right) }{2}
\]\[\frac{27 \left( \xi -1\right) \, \left( \xi -\frac{2}{3}\right)  \xi }{2}
\]\[-\frac{27 \left( \xi -1\right) \, \left( \xi -\frac{1}{3}\right)  \xi }{2}
\]\[\frac{9 \left( \xi -\frac{2}{3}\right) \, \left( \xi -\frac{1}{3}\right)  \xi }{2}\]
</p>



<!-- Text cell -->


<div class="comment">
We can see that a Lagrange polynomial is evaluated to 1 at its own support points and 0 at the support points of the other polynomails.
</div>


<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
(%i17)	
  </span></td>
  <td><span class="input">
<span class="code_function">wxdraw2d</span><span class="code_operator">(</span><span class="code_variable">grid</span><span class="code_operator">=</span><span class="code_function">true</span><span class="code_endofline">,</span> <span class="code_variable">xrange</span><span class="code_operator">=</span><span class="code_variable">reference_cell_1d</span><span class="code_endofline">,</span> <span class="code_variable">yrange</span><span class="code_operator">=</span><span class="code_operator">[</span><span class="code_operator">-</span><span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">5</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_endofline">.</span><span class="code_number">2</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">dimensions</span><span class="code_operator">=</span><span class="code_variable">fig_size</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">terminal</span><span class="code_operator">=</span><span class="code_variable">wxt</span><span class="code_endofline">,</span> <span class="code_variable">font</span><span class="code_operator">=</span><span class="code_variable">font_name</span><span class="code_endofline">,</span> <span class="code_variable">font_size</span><span class="code_operator">=</span><span class="code_variable">fig_font_size</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">color</span><span class="code_operator">=</span><span class="code_variable">clist</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">key</span><span class="code_operator">=</span><span class="code_string">&quot;Shape function with support point at 0&quot;</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_function">explicit</span><span class="code_operator">(</span><span class="code_variable">shape_functions_1d</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_variable">reference_cell_1d</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">reference_cell_1d</span><span class="code_operator">[</span><span class="code_number">2</span><span class="code_operator">]</span><span class="code_operator">)</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">color</span><span class="code_operator">=</span><span class="code_variable">clist</span><span class="code_operator">[</span><span class="code_number">2</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">key</span><span class="code_operator">=</span><span class="code_string">&quot;Shape function with support point at 1/3&quot;</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_function">explicit</span><span class="code_operator">(</span><span class="code_variable">shape_functions_1d</span><span class="code_operator">[</span><span class="code_number">2</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_variable">reference_cell_1d</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">reference_cell_1d</span><span class="code_operator">[</span><span class="code_number">2</span><span class="code_operator">]</span><span class="code_operator">)</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">color</span><span class="code_operator">=</span><span class="code_variable">clist</span><span class="code_operator">[</span><span class="code_number">3</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">key</span><span class="code_operator">=</span><span class="code_string">&quot;Shape function with support point at 2/3&quot;</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_function">explicit</span><span class="code_operator">(</span><span class="code_variable">shape_functions_1d</span><span class="code_operator">[</span><span class="code_number">3</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_variable">reference_cell_1d</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">reference_cell_1d</span><span class="code_operator">[</span><span class="code_number">2</span><span class="code_operator">]</span><span class="code_operator">)</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">color</span><span class="code_operator">=</span><span class="code_variable">clist</span><span class="code_operator">[</span><span class="code_number">4</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">key</span><span class="code_operator">=</span><span class="code_string">&quot;Shape function with support point at 1&quot;</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_function">explicit</span><span class="code_operator">(</span><span class="code_variable">shape_functions_1d</span><span class="code_operator">[</span><span class="code_number">4</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_variable">reference_cell_1d</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">reference_cell_1d</span><span class="code_operator">[</span><span class="code_number">2</span><span class="code_operator">]</span><span class="code_operator">)</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">color</span><span class="code_operator">=</span><span class="code_variable">black</span><span class="code_endofline">,</span> <span class="code_variable">line_type</span> <span class="code_operator">=</span> <span class="code_variable">dots</span><span class="code_endofline">,</span> <span class="code_variable">key</span><span class="code_operator">=</span><span class="code_string">&quot;&quot;</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_function">implicit</span><span class="code_operator">(</span><span class="code_variable">x</span><span class="code_operator">=</span><span class="code_variable">support_points_1d</span><span class="code_operator">[</span><span class="code_number">2</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">x</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_endofline">,</span> <span class="code_variable">y</span><span class="code_endofline">,</span> <span class="code_operator">-</span><span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">5</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_endofline">.</span><span class="code_number">2</span><span class="code_operator">)</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">color</span><span class="code_operator">=</span><span class="code_variable">black</span><span class="code_endofline">,</span> <span class="code_variable">line_type</span> <span class="code_operator">=</span> <span class="code_variable">dots</span><span class="code_endofline">,</span> <span class="code_variable">key</span><span class="code_operator">=</span><span class="code_string">&quot;&quot;</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_function">implicit</span><span class="code_operator">(</span><span class="code_variable">x</span><span class="code_operator">=</span><span class="code_variable">support_points_1d</span><span class="code_operator">[</span><span class="code_number">3</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">x</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_endofline">,</span> <span class="code_variable">y</span><span class="code_endofline">,</span> <span class="code_operator">-</span><span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">5</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_endofline">.</span><span class="code_number">2</span><span class="code_operator">)</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">color</span><span class="code_operator">=</span><span class="code_variable">blue</span><span class="code_endofline">,</span> <span class="code_variable">point_type</span><span class="code_operator">=</span><span class="code_variable">filled_circle</span><span class="code_endofline">,</span> <span class="code_variable">point_size</span><span class="code_operator">=</span><span class="code_number">3</span><span class="code_endofline">,</span> <span class="code_function">points</span><span class="code_operator">(</span><span class="code_function">makelist</span><span class="code_operator">(</span><span class="code_operator">[</span><span class="code_variable">support_points_1d</span><span class="code_operator">[</span><span class="code_variable">i</span><span class="code_operator">]</span><span class="code_endofline">,</span><span class="code_number">0</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">i</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_endofline">,</span> <span class="code_number">4</span><span class="code_operator">)</span><span class="code_operator">)</span><span class="code_endofline"><br/>
</span><span class="code_operator">)</span><span class="code_endofline">;</span>  </span></td>
</tr></table>

  <img src="/figures/shape-functions-and-basis-functions-in-fem-publish_4.png"  style="max-width:90%;" loading="lazy" alt=" (Graphics) " /><br/>




<!-- Subsection cell -->


<h3>
  1.2 2D
</h3>


<!-- Text cell -->


<div class="comment">
First, we generate two sets of 2nd order Lagrange polynomials in the 1D reference cell $[0, 1]$. Then by taking their tensor product, we can generate the 2nd order Lagrange polynomials on the 2D unit cell $[0, 1] \times [0, 1]$
</div>


<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
(%i18)	
  </span></td>
  <td><span class="input">
<span class="code_variable">support_points_2d</span> <span class="code_operator">:</span> <span class="code_function">GenLagrangeNodes2D</span><span class="code_operator">(</span><span class="code_operator">[</span><span class="code_number">2</span><span class="code_endofline">,</span> <span class="code_number">2</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">reference_cell_1d</span><span class="code_endofline">,</span> <span class="code_variable">reference_cell_1d</span><span class="code_operator">)</span><span class="code_endofline">;</span>  </span></td>
</tr></table>
<p>
\[\operatorname{(support\_ points\_ 2d)	}\begin{pmatrix}0 &amp; 0\\
\frac{1}{2} &amp; 0\\
1 &amp; 0\\
0 &amp; \frac{1}{2}\\
\frac{1}{2} &amp; \frac{1}{2}\\
1 &amp; \frac{1}{2}\\
0 &amp; 1\\
\frac{1}{2} &amp; 1\\
1 &amp; 1\end{pmatrix}\]
</p>


<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
(%i20)	
  </span></td>
  <td><span class="input">
<span class="code_variable">shape_functions_2d</span> <span class="code_operator">:</span> <span class="code_endofline"><br/>
</span><span class="code_function">GenLagrangeBases2D</span><span class="code_operator">(</span><span class="code_operator">[</span><span class="code_number">2</span><span class="code_endofline">,</span> <span class="code_number">2</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">reference_cell_1d</span><span class="code_endofline">,</span> <span class="code_variable">reference_cell_1d</span><span class="code_endofline">,</span> <span class="code_operator">'</span><span class="code_operator">[</span><span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_variable">η</span><span class="code_operator">]</span><span class="code_operator">)</span><span class="code_endofline">$</span><span class="code_endofline"><br/>
</span><span class="code_function">PrintEquationList</span><span class="code_operator">(</span><span class="code_variable">shape_functions_2d</span><span class="code_operator">)</span><span class="code_endofline">;</span>  </span></td>
</tr></table>
<p>
\[\]\[4 \left( \eta -1\right) \, \left( \eta -\frac{1}{2}\right) \, \left( \xi -1\right) \, \left( \xi -\frac{1}{2}\right) 
\]\[-8 \left( \eta -1\right) \, \left( \eta -\frac{1}{2}\right) \, \left( \xi -1\right)  \xi 
\]\[4 \left( \eta -1\right) \, \left( \eta -\frac{1}{2}\right) \, \left( \xi -\frac{1}{2}\right)  \xi 
\]\[-8 \left( \eta -1\right)  \eta \, \left( \xi -1\right) \, \left( \xi -\frac{1}{2}\right) 
\]\[16 \left( \eta -1\right)  \eta \, \left( \xi -1\right)  \xi 
\]\[-8 \left( \eta -1\right)  \eta \, \left( \xi -\frac{1}{2}\right)  \xi 
\]\[4 \left( \eta -\frac{1}{2}\right)  \eta \, \left( \xi -1\right) \, \left( \xi -\frac{1}{2}\right) 
\]\[-8 \left( \eta -\frac{1}{2}\right)  \eta \, \left( \xi -1\right)  \xi 
\]\[4 \left( \eta -\frac{1}{2}\right)  \eta \, \left( \xi -\frac{1}{2}\right)  \xi \]
</p>



<!-- Text cell -->


<div class="comment">
The shape functions are generated in the tensor product order or lexicographic order, i.e. for the list of support points of these shape functions, their first coordinate component ξ runs faster than the second η.
</div>


<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
(%i30)	
  </span></td>
  <td><span class="input">
<span class="code_function">with_slider_draw3d</span><span class="code_operator">(</span><span class="code_variable">i</span><span class="code_endofline">,</span> <span class="code_function">linspace</span><span class="code_operator">(</span><span class="code_number">1</span><span class="code_endofline">,</span> <span class="code_function">length</span><span class="code_operator">(</span><span class="code_variable">shape_functions_2d</span><span class="code_operator">)</span><span class="code_endofline">,</span> <span class="code_function">length</span><span class="code_operator">(</span><span class="code_variable">shape_functions_2d</span><span class="code_operator">)</span><span class="code_operator">)</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span><span class="code_variable">grid</span><span class="code_operator">=</span><span class="code_function">true</span><span class="code_endofline">,</span> <span class="code_variable">enhanced3d</span><span class="code_operator">=</span><span class="code_function">false</span><span class="code_endofline">,</span> <span class="code_variable">color</span><span class="code_operator">=</span><span class="code_variable">red</span><span class="code_endofline">,</span> <span class="code_variable">dimensions</span><span class="code_operator">=</span><span class="code_variable">fig_size</span><span class="code_endofline">,</span> <span class="code_variable">font</span><span class="code_operator">=</span><span class="code_variable">font_name</span><span class="code_endofline">,</span> <span class="code_variable">font_size</span><span class="code_operator">=</span><span class="code_variable">fig_font_size</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">title</span><span class="code_operator">=</span><span class="code_function">sconcat</span><span class="code_operator">(</span><span class="code_string">&quot;Polynomial No.&quot;</span><span class="code_endofline">,</span> <span class="code_variable">i</span><span class="code_operator">)</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">xlabel</span><span class="code_operator">=</span><span class="code_string">&quot;ξ&quot;</span><span class="code_endofline">,</span> <span class="code_variable">ylabel</span><span class="code_operator">=</span><span class="code_string">&quot;η&quot;</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">transparent</span><span class="code_operator">=</span><span class="code_function">true</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_function">explicit</span><span class="code_operator">(</span><span class="code_variable">shape_functions_2d</span><span class="code_operator">[</span><span class="code_variable">i</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_endofline">,</span> <span class="code_variable">η</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_operator">)</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">color</span><span class="code_operator">=</span><span class="code_variable">blue</span><span class="code_endofline">,</span> <span class="code_variable">point_type</span><span class="code_operator">=</span><span class="code_variable">filled_circle</span><span class="code_endofline">,</span> <span class="code_variable">point_size</span><span class="code_operator">=</span><span class="code_number">3</span><span class="code_endofline">,</span> <span class="code_variable">enhanced3d</span><span class="code_operator">=</span><span class="code_function">false</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_function">points</span><span class="code_operator">(</span><span class="code_function">makelist</span><span class="code_operator">(</span><span class="code_operator">[</span><span class="code_variable">support_points_2d</span><span class="code_operator">[</span><span class="code_variable">p</span><span class="code_endofline">,</span><span class="code_number">1</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">support_points_2d</span><span class="code_operator">[</span><span class="code_variable">p</span><span class="code_endofline">,</span><span class="code_number">2</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">p</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_endofline">,</span> <span class="code_number">9</span><span class="code_operator">)</span><span class="code_operator">)</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">points_joined</span><span class="code_operator">=</span><span class="code_function">true</span><span class="code_endofline">,</span> <span class="code_variable">line_type</span><span class="code_operator">=</span><span class="code_variable">solid</span><span class="code_endofline">,</span> <span class="code_variable">color</span><span class="code_operator">=</span><span class="code_variable">purple</span><span class="code_endofline">,</span> <span class="code_variable">line_width</span><span class="code_operator">=</span><span class="code_number">3</span><span class="code_endofline">,</span> <span class="code_variable">point_size</span><span class="code_operator">=</span><span class="code_number">2</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_function">points</span><span class="code_operator">(</span><span class="code_operator">[</span><span class="code_operator">[</span><span class="code_variable">support_points_2d</span><span class="code_operator">[</span><span class="code_variable">i</span><span class="code_endofline">,</span><span class="code_number">1</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">support_points_2d</span><span class="code_operator">[</span><span class="code_variable">i</span><span class="code_endofline">,</span><span class="code_number">2</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_operator">[</span><span class="code_variable">support_points_2d</span><span class="code_operator">[</span><span class="code_variable">i</span><span class="code_endofline">,</span><span class="code_number">1</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">support_points_2d</span><span class="code_operator">[</span><span class="code_variable">i</span><span class="code_endofline">,</span><span class="code_number">2</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_operator">]</span><span class="code_operator">]</span><span class="code_operator">)</span><span class="code_endofline"><br/>
</span><span class="code_operator">)</span><span class="code_endofline">;</span>  </span></td>
</tr></table>

  <img src="/figures/shape-functions-and-basis-functions-in-fem-publish_10.gif"  alt="Animated Diagram" loading="lazy" style="max-width:90%;" />




<!-- Section cell -->


<h2>
  2 Map from reference cell to real cell
</h2>


<!-- Subsection cell -->


<h3>
  2.1 1D
</h3>


<!-- Text cell -->


<div class="comment">
A 1D real cell is a line segment, which is embedded in $\mathbb{R}^3$. When this line segment is straight, the map $\chi_{\tau}$ from $[0,1]$ to this real cell is simple, which is merely a translation and rotation from the reference cell $[0,1]$. When this line segment is bent, the map is not trivial. Since a linear combination of the above Lagrange shape functions can be used to approximate any continuous functions defined on the reference cell, it can also be used to approximate the map $\chi_{\tau}$ for this line segment. Because we are in 3D space, we need three such linear combinations of shape functions to represent $x$, $y$ and $z$ coordinate components respectively.
</div>


<!-- Text cell -->


<div class="comment">
A Lagrange shape function has the property that it is evaluated to 1 at its own support point and to 0 at the support points of the other shape functions. So to make sure that the map $\chi_{\tau}$ produces points located exactly on the bent real cell that correspond to those support points in the reference cell, for each coordinate component respectively, the coefficient before each shape function in the linear combination should be the coodinate component of the point on the real cell which corresponds to the support point of the current shape function in the reference cell.
</div>


<!-- Text cell -->


<div class="comment">

</div>


<!-- Text cell -->


<div class="comment">
Now let's define a parametric cubic curve in 3D space. N.B. This corresponds the manifold description of the geometric entity.
</div>


<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_function">curve</span><span class="code_operator">(</span><span class="code_variable">ξ</span><span class="code_operator">)</span> <span class="code_operator">:</span><span class="code_operator">=</span> <span class="code_operator">[</span><span class="code_number">1</span><span class="code_endofline">.</span><span class="code_number">2</span> <span class="code_operator">*</span> <span class="code_variable">ξ</span><span class="code_operator">^</span><span class="code_number">3</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_endofline">.</span><span class="code_number">5</span> <span class="code_operator">*</span> <span class="code_variable">ξ</span><span class="code_operator">^</span><span class="code_number">2</span> <span class="code_operator">+</span> <span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">5</span> <span class="code_operator">*</span> <span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_operator">-</span><span class="code_number">2</span> <span class="code_operator">*</span> <span class="code_variable">ξ</span><span class="code_operator">^</span><span class="code_number">3</span> <span class="code_operator">-</span> <span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">3</span><span class="code_operator">]</span><span class="code_endofline">;</span>  </span></td>
</tr></table>
<p>

\[\operatorname{	}\operatorname{curve}\left( \xi \right) \operatorname{:=}\left[ 1.2 {{\xi }^{3}}\operatorname{,}1.5 {{\xi }^{2}}+0.5 \xi \operatorname{,}\left( -2\right)  {{\xi }^{3}}-0.3\right] \]

</p>


<!-- Text cell -->


<div class="comment">
The curve looks like this. Here we colorize the curve based on the value of its parameter ξ, which is specified via the option enhanced3d=[ξ, ξ].
</div>


<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_function">wxdraw3d</span><span class="code_operator">(</span><span class="code_variable">grid</span><span class="code_operator">=</span><span class="code_function">true</span><span class="code_endofline">,</span> <span class="code_variable">dimensions</span><span class="code_operator">=</span><span class="code_variable">fig_size</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">terminal</span><span class="code_operator">=</span><span class="code_variable">wxt</span><span class="code_endofline">,</span> <span class="code_variable">font</span><span class="code_operator">=</span><span class="code_variable">font_name</span><span class="code_endofline">,</span> <span class="code_variable">font_size</span><span class="code_operator">=</span><span class="code_variable">fig_font_size</span><span class="code_endofline">,</span> <span class="code_variable">proportional_axes</span><span class="code_operator">=</span><span class="code_variable">xyz</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">xlabel</span><span class="code_operator">=</span><span class="code_string">&quot;x&quot;</span><span class="code_endofline">,</span> <span class="code_variable">ylabel</span><span class="code_operator">=</span><span class="code_string">&quot;y&quot;</span><span class="code_endofline">,</span> <span class="code_variable">zlabel</span><span class="code_operator">=</span><span class="code_string">&quot;z&quot;</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">enhanced3d</span><span class="code_operator">=</span><span class="code_operator">[</span><span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_variable">ξ</span><span class="code_operator">]</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_function">parametric</span><span class="code_operator">(</span><span class="code_function">curve</span><span class="code_operator">(</span><span class="code_variable">ξ</span><span class="code_operator">)</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_function">curve</span><span class="code_operator">(</span><span class="code_variable">ξ</span><span class="code_operator">)</span><span class="code_operator">[</span><span class="code_number">2</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_function">curve</span><span class="code_operator">(</span><span class="code_variable">ξ</span><span class="code_operator">)</span><span class="code_operator">[</span><span class="code_number">3</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_operator">)</span><span class="code_endofline"><br/>
</span><span class="code_operator">)</span><span class="code_endofline">;</span>  </span></td>
</tr></table>

  <img src="/figures/shape-functions-and-basis-functions-in-fem-publish_15.png"  style="max-width:90%;" loading="lazy" alt=" (Graphics) " /><br/>




<!-- Text cell -->


<div class="comment">
If we want to use the second order shape functions to represent this curve, besides the two end points, an additional point approximately in the middle of the curve should be provided. This point can be acquired by first taking a convex combination of the two two end points. Then project this new point onto the curve.
</div>


<!-- Text cell -->


<div class="comment">
Starting and ending points of the curve are
</div>


<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_variable">p1</span> <span class="code_operator">:</span> <span class="code_function">curve</span><span class="code_operator">(</span><span class="code_number">0</span><span class="code_operator">)</span><span class="code_endofline">;</span><span class="code_endofline"><br/>
</span><span class="code_variable">p2</span> <span class="code_operator">:</span> <span class="code_function">curve</span><span class="code_operator">(</span><span class="code_number">1</span><span class="code_operator">)</span><span class="code_endofline">;</span>  </span></td>
</tr></table>
<p>
\[\operatorname{(p1)	}\left[ 0\operatorname{,}0\operatorname{,}-0.3\right] \]
</p>
<p>
\[\operatorname{(p2)	}\left[ 1.2\operatorname{,}2.0\operatorname{,}-2.3\right] \]
</p>


<!-- Text cell -->


<div class="comment">
Convex combination of $p_1$ and $p_2$ is
</div>


<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_variable">p_middle</span> <span class="code_operator">:</span> <span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">5</span> <span class="code_operator">*</span> <span class="code_variable">p1</span> <span class="code_operator">+</span> <span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">5</span> <span class="code_operator">*</span> <span class="code_variable">p2</span><span class="code_endofline">;</span>  </span></td>
</tr></table>
<p>
\[\operatorname{(p\_ middle)	}\left[ 0.6\operatorname{,}1.0\operatorname{,}-1.3\right] \]
</p>


<!-- Text cell -->


<div class="comment">
Initial guess of the parameter ξ for generating the projection of the middle point on the curve:
</div>


<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_variable">ξ0</span> <span class="code_operator">:</span> <span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">5</span><span class="code_endofline">$</span>  </span></td>
</tr></table>



<!-- Text cell -->


<div class="comment">
Here we let the projection satisfies the condition that the connection line of the above middle point and its projection point is perpendicular to the tangent vector at this projection point. So let's define an objective function which minimizes the dot product of the tangent vector and the connection line.
</div>


<!-- Text cell -->


<div class="comment">
First, define a function for computing the tangent vector at a point on the curve:
</div>


<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_function">curve_tangent</span><span class="code_operator">(</span><span class="code_variable">ξ</span><span class="code_operator">)</span> <span class="code_operator">:</span><span class="code_operator">=</span> <span class="code_function">subst</span><span class="code_operator">(</span><span class="code_variable">x</span><span class="code_operator">=</span><span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_function">diff</span><span class="code_operator">(</span><span class="code_function">curve</span><span class="code_operator">(</span><span class="code_variable">x</span><span class="code_operator">)</span><span class="code_endofline">,</span> <span class="code_variable">x</span><span class="code_operator">)</span><span class="code_operator">)</span><span class="code_endofline">;</span>  </span></td>
</tr></table>
<p>
\[\operatorname{	}\operatorname{curve\_ tangent}\left( \xi \right) \operatorname{:=}\operatorname{subst}\left( x=\xi \operatorname{,}\frac{d}{d x} \operatorname{curve}(x)\right) \]
</p>


<!-- Text cell -->


<div class="comment">
Then, the objective function is
</div>


<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_variable">project_middle_obj</span> <span class="code_operator">:</span> <span class="code_function">innerproduct</span><span class="code_operator">(</span><span class="code_operator">(</span><span class="code_function">curve</span><span class="code_operator">(</span><span class="code_variable">ξ</span><span class="code_operator">)</span> <span class="code_operator">-</span> <span class="code_variable">p_middle</span><span class="code_operator">)</span><span class="code_endofline">,</span> <span class="code_function">curve_tangent</span><span class="code_operator">(</span><span class="code_variable">ξ</span><span class="code_operator">)</span><span class="code_operator">)</span><span class="code_endofline">;</span>  </span></td>
</tr></table>
<p>
\[\]\[rat: replaced 0.5 by 1/2 = 0.5
\]\[rat: replaced 3.0 by 3/1 = 3.0
\]\[rat: replaced -1.0 by -1/1 = -1.0
\]\[rat: replaced 0.5 by 1/2 = 0.5
\]\[rat: replaced 1.5 by 3/2 = 1.5
\]\[rat: replaced 0.9999999999999998 by 1/1 = 1.0
\]\[rat: replaced 3.6 by 18/5 = 3.6
\]\[rat: replaced -0.6 by -3/5 = -0.6
\]\[rat: replaced 1.2 by 6/5 = 1.2\]
</p>
<p>

\[\operatorname{(project\_ middle\_ obj)	}\frac{1632 {{\xi }^{5}}+450 {{\xi }^{3}}-591 {{\xi }^{2}}-275 \xi -50}{100}\]

</p>


<!-- Text cell -->


<div class="comment">
The minimization problem can be solved by the Newton's method.
</div>


<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_variable">sol</span> <span class="code_operator">:</span> <span class="code_function">mnewton</span><span class="code_operator">(</span><span class="code_operator">[</span><span class="code_variable">project_middle_obj</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_operator">[</span><span class="code_variable">ξ</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_operator">[</span><span class="code_variable">ξ0</span><span class="code_operator">]</span><span class="code_operator">)</span><span class="code_endofline">;</span>  </span></td>
</tr></table>
<p>
\[\operatorname{(sol)	}\left[ \left[ \xi =0.7552127951245737\right] \right] \]
</p>


<!-- Text cell -->


<div class="comment">
Sustitute this ξ into the curve's parametric equation, we obtain the projection point.
</div>


<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_variable">project_middle</span> <span class="code_operator">:</span> <span class="code_function">curve</span><span class="code_operator">(</span><span class="code_function">rhs</span><span class="code_operator">(</span><span class="code_variable">sol</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span><span class="code_operator">)</span><span class="code_operator">)</span><span class="code_endofline">;</span>  </span></td>
</tr></table>
<p>
\[\operatorname{(project\_ middle)	}\left[ 0.5168794478345868\operatorname{,}1.233125946442094\operatorname{,}-1.161465746390978\right] \]
</p>


<!-- Text cell -->


<div class="comment">
The support points in the real cell as well as the middle point are shown below. Meanwhile, the support points in the reference cell associated with Lagrange functions are always $[0, 0.5, 1]$. This indicates the curve to be interpolated by Lagrange functions has a different parameterization from its original parametric representation.
</div>


<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_function">wxdraw3d</span><span class="code_operator">(</span><span class="code_variable">grid</span><span class="code_operator">=</span><span class="code_function">true</span><span class="code_endofline">,</span> <span class="code_variable">dimensions</span><span class="code_operator">=</span><span class="code_variable">fig_size</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">terminal</span><span class="code_operator">=</span><span class="code_variable">wxt</span><span class="code_endofline">,</span> <span class="code_variable">font</span><span class="code_operator">=</span><span class="code_variable">font_name</span><span class="code_endofline">,</span> <span class="code_variable">font_size</span><span class="code_operator">=</span><span class="code_variable">fig_font_size</span><span class="code_endofline">,</span> <span class="code_variable">proportional_axes</span><span class="code_operator">=</span><span class="code_variable">xyz</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">xlabel</span><span class="code_operator">=</span><span class="code_string">&quot;x&quot;</span><span class="code_endofline">,</span> <span class="code_variable">ylabel</span><span class="code_operator">=</span><span class="code_string">&quot;y&quot;</span><span class="code_endofline">,</span> <span class="code_variable">zlabel</span><span class="code_operator">=</span><span class="code_string">&quot;z&quot;</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">enhanced3d</span><span class="code_operator">=</span><span class="code_operator">[</span><span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_variable">ξ</span><span class="code_operator">]</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_function">parametric</span><span class="code_operator">(</span><span class="code_function">curve</span><span class="code_operator">(</span><span class="code_variable">ξ</span><span class="code_operator">)</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_function">curve</span><span class="code_operator">(</span><span class="code_variable">ξ</span><span class="code_operator">)</span><span class="code_operator">[</span><span class="code_number">2</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_function">curve</span><span class="code_operator">(</span><span class="code_variable">ξ</span><span class="code_operator">)</span><span class="code_operator">[</span><span class="code_number">3</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_operator">)</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">color</span><span class="code_operator">=</span><span class="code_variable">red</span><span class="code_endofline">,</span> <span class="code_variable">point_type</span><span class="code_operator">=</span><span class="code_variable">circle</span><span class="code_endofline">,</span> <span class="code_variable">point_size</span><span class="code_operator">=</span><span class="code_number">2</span><span class="code_endofline">,</span> <span class="code_variable">enhanced3d</span><span class="code_operator">=</span><span class="code_function">false</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_function">points</span><span class="code_operator">(</span><span class="code_operator">[</span><span class="code_variable">p_middle</span><span class="code_operator">]</span><span class="code_operator">)</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">color</span><span class="code_operator">=</span><span class="code_variable">blue</span><span class="code_endofline">,</span> <span class="code_variable">point_type</span><span class="code_operator">=</span><span class="code_variable">filled_circle</span><span class="code_endofline">,</span> <span class="code_variable">point_size</span><span class="code_operator">=</span><span class="code_number">3</span><span class="code_endofline">,</span> <span class="code_variable">enhanced3d</span><span class="code_operator">=</span><span class="code_function">false</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_function">points</span><span class="code_operator">(</span><span class="code_operator">[</span><span class="code_variable">p1</span><span class="code_endofline">,</span> <span class="code_variable">project_middle</span><span class="code_endofline">,</span> <span class="code_variable">p2</span><span class="code_operator">]</span><span class="code_operator">)</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">color</span><span class="code_operator">=</span><span class="code_variable">blue</span><span class="code_endofline">,</span> <span class="code_variable">point_type</span><span class="code_operator">=</span><span class="code_operator">-</span><span class="code_number">1</span><span class="code_endofline">,</span> <span class="code_variable">enhanced3d</span><span class="code_operator">=</span><span class="code_function">false</span><span class="code_endofline">,</span> <span class="code_variable">points_joined</span><span class="code_operator">=</span><span class="code_function">true</span><span class="code_endofline">,</span> <span class="code_variable">line_type</span><span class="code_operator">=</span><span class="code_variable">dashes</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_function">points</span><span class="code_operator">(</span><span class="code_operator">[</span><span class="code_variable">p1</span><span class="code_endofline">,</span> <span class="code_variable">p2</span><span class="code_operator">]</span><span class="code_operator">)</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">color</span><span class="code_operator">=</span><span class="code_variable">blue</span><span class="code_endofline">,</span> <span class="code_variable">point_type</span><span class="code_operator">=</span><span class="code_operator">-</span><span class="code_number">1</span><span class="code_endofline">,</span> <span class="code_variable">enhanced3d</span><span class="code_operator">=</span><span class="code_function">false</span><span class="code_endofline">,</span> <span class="code_variable">points_joined</span><span class="code_operator">=</span><span class="code_function">true</span><span class="code_endofline">,</span> <span class="code_variable">line_type</span><span class="code_operator">=</span><span class="code_variable">dashes</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_function">points</span><span class="code_operator">(</span><span class="code_operator">[</span><span class="code_variable">p_middle</span><span class="code_endofline">,</span> <span class="code_variable">project_middle</span><span class="code_operator">]</span><span class="code_operator">)</span><span class="code_endofline"><br/>
</span><span class="code_operator">)</span><span class="code_endofline">;</span>  </span></td>
</tr></table>

  <img src="/figures/shape-functions-and-basis-functions-in-fem-publish_26.png"  style="max-width:90%;" loading="lazy" alt=" (Graphics) " /><br/>




<!-- Text cell -->


<div class="comment">
Then the map from the reference cell to the real cell can be constructed using 2nd order 1D Lagrange shape functions.
</div>


<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_variable">shape_functions_1d</span> <span class="code_operator">:</span> <span class="code_function">GenLagrangeBases1D</span><span class="code_operator">(</span><span class="code_number">2</span><span class="code_endofline">,</span> <span class="code_variable">reference_cell_1d</span><span class="code_endofline">,</span> <span class="code_operator">'</span><span class="code_variable">ξ</span><span class="code_operator">)</span><span class="code_endofline">;</span>  </span></td>
</tr></table>
<p>
\[\operatorname{(shape\_ functions\_ 1d)	}\left[ 2 \left( \xi -1\right) \, \left( \xi -\frac{1}{2}\right) \operatorname{,}-4 \left( \xi -1\right)  \xi \operatorname{,}2 \left( \xi -\frac{1}{2}\right)  \xi \right] \]
</p>


<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_variable">map_from_ref_to_real_1d</span> <span class="code_operator">:</span> <span class="code_variable">p1</span> <span class="code_operator">*</span> <span class="code_variable">shape_functions_1d</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span> <span class="code_operator">+</span> <span class="code_variable">project_middle</span> <span class="code_operator">*</span> <span class="code_variable">shape_functions_1d</span><span class="code_operator">[</span><span class="code_number">2</span><span class="code_operator">]</span> <span class="code_operator">+</span> <span class="code_variable">p2</span> <span class="code_operator">*</span> <span class="code_variable">shape_functions_1d</span><span class="code_operator">[</span><span class="code_number">3</span><span class="code_operator">]</span><span class="code_endofline">$</span><span class="code_endofline"><br/>
</span><span class="code_function">PrintEquationList</span><span class="code_operator">(</span><span class="code_variable">map_from_ref_to_real_1d</span><span class="code_operator">)</span><span class="code_endofline">;</span>  </span></td>
</tr></table>
<p>
\[\]\[2.4 \left( \xi -\frac{1}{2}\right)  \xi -2.067517791338348 \left( \xi -1\right)  \xi 
\]\[4.0 \left( \xi -\frac{1}{2}\right)  \xi -4.932503785768375 \left( \xi -1\right)  \xi 
\]\[-4.6 \left( \xi -\frac{1}{2}\right)  \xi +4.645862985563912 \left( \xi -1\right)  \xi -0.6 \left( \xi -1\right) \, \left( \xi -\frac{1}{2}\right) \]
</p>



<!-- Text cell -->


<div class="comment">
Now we plot the interpolated curve using 2nd order Lagrange functions together with the original curve. We can see that the interpolated curve exactly passes through the three support points in the real cell.
</div>


<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_function">wxdraw3d</span><span class="code_operator">(</span><span class="code_variable">grid</span><span class="code_operator">=</span><span class="code_function">true</span><span class="code_endofline">,</span> <span class="code_variable">dimensions</span><span class="code_operator">=</span><span class="code_variable">fig_size</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">terminal</span><span class="code_operator">=</span><span class="code_variable">wxt</span><span class="code_endofline">,</span> <span class="code_variable">font</span><span class="code_operator">=</span><span class="code_variable">font_name</span><span class="code_endofline">,</span> <span class="code_variable">font_size</span><span class="code_operator">=</span><span class="code_variable">fig_font_size</span><span class="code_endofline">,</span> <span class="code_variable">proportional_axes</span><span class="code_operator">=</span><span class="code_variable">xyz</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">xlabel</span><span class="code_operator">=</span><span class="code_string">&quot;x&quot;</span><span class="code_endofline">,</span> <span class="code_variable">ylabel</span><span class="code_operator">=</span><span class="code_string">&quot;y&quot;</span><span class="code_endofline">,</span> <span class="code_variable">zlabel</span><span class="code_operator">=</span><span class="code_string">&quot;z&quot;</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">enhanced3d</span><span class="code_operator">=</span><span class="code_operator">[</span><span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_variable">ξ</span><span class="code_operator">]</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_function">parametric</span><span class="code_operator">(</span><span class="code_function">curve</span><span class="code_operator">(</span><span class="code_variable">ξ</span><span class="code_operator">)</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_function">curve</span><span class="code_operator">(</span><span class="code_variable">ξ</span><span class="code_operator">)</span><span class="code_operator">[</span><span class="code_number">2</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_function">curve</span><span class="code_operator">(</span><span class="code_variable">ξ</span><span class="code_operator">)</span><span class="code_operator">[</span><span class="code_number">3</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_operator">)</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">color</span><span class="code_operator">=</span><span class="code_variable">blue</span><span class="code_endofline">,</span> <span class="code_variable">enhanced3d</span><span class="code_operator">=</span><span class="code_function">false</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_function">parametric</span><span class="code_operator">(</span><span class="code_variable">map_from_ref_to_real_1d</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">map_from_ref_to_real_1d</span><span class="code_operator">[</span><span class="code_number">2</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">map_from_ref_to_real_1d</span><span class="code_operator">[</span><span class="code_number">3</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_operator">)</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">color</span><span class="code_operator">=</span><span class="code_variable">blue</span><span class="code_endofline">,</span> <span class="code_variable">point_type</span><span class="code_operator">=</span><span class="code_variable">filled_circle</span><span class="code_endofline">,</span> <span class="code_variable">point_size</span><span class="code_operator">=</span><span class="code_number">2</span><span class="code_endofline">,</span> <span class="code_variable">enhanced3d</span><span class="code_operator">=</span><span class="code_function">false</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_function">points</span><span class="code_operator">(</span><span class="code_operator">[</span><span class="code_variable">p1</span><span class="code_endofline">,</span> <span class="code_variable">project_middle</span><span class="code_endofline">,</span> <span class="code_variable">p2</span><span class="code_operator">]</span><span class="code_operator">)</span><span class="code_endofline"><br/>
</span><span class="code_operator">)</span><span class="code_endofline">;</span>  </span></td>
</tr></table>

  <img src="/figures/shape-functions-and-basis-functions-in-fem-publish_32.png"  style="max-width:90%;" loading="lazy" alt=" (Graphics) " /><br/>




<!-- Subsection cell -->


<h3>
  2.2 2D
</h3>


<!-- Text cell -->


<div class="comment">
Similarto the 1D case, a 2D real cell embedded in $\mathbb{R}^3$ is an arbitrary quadrilateral, either flat or curved and not necessarily a parallelogram. Therefore, the map $\chi_{\tau}$ from the 2D reference cell $[0,1] \times [0,1]$ to a real cell can be expanded by a set of 2D Lagrange shape functions. When the Lagrange polynomial order is 1, the mapped real cell is planar. When the order is higher than 1, the mapped real cell is curved. Such a map is actually a linear combination of the shape functions weighted by corresponding support point coordinates. The basic philosophy of computing the projection of intermediate points which are to be used as support points of shape functions is similar and no examples are given here.
</div>


<!-- Section cell -->


<h2>
  3 Basis functions of finite element
</h2>


<!-- Text cell -->


<div class="comment">
The shape functions adopted by a discontinuous finite element, which conforms to the $L^2$ function space, can use Lagrange polynomials defined on the reference cell $[0,1] \times [0,1]$. Hence, the shape functions in each cell in the domain on which the finite element is defined corresponds to a unique basis function of the finite element space.
</div>


<!-- Text cell -->


<div class="comment">
Even though the continuous finite element adtops the same shape functions as the discontinuous finite element, because it conforms to the Sobolev space $H^1$, functions constructed from this finite element should be continuous across cell boundaries. Then for a basis function in the finite element space, when its support point is in the interior of a cell, the situation is simple and it is just the shape function with the same support point in this cell. When the support point of the basis function in on an edge, face or a vertex, such support point will be shared by several cells. Then the basis function of the finite element space determined by the continuous finite element is a concatenation of those shape functions in these cells whose support points are just the common support point of the basis function.
</div>


<!-- Text cell -->


<div class="comment">
Let's use the sphere model as an example.
</div>


<!-- Image cell -->


<div class="image">
Figure 1:Mesh for a sphere
<br/>
  <img src="/figures/shape-functions-and-basis-functions-in-fem-publish_34.png" alt="Diagram" style="max-width:90%;" loading="lazy" /></div>


<!-- Text cell -->


<div class="comment">
We select four cells from the mesh and construct the basis function situated at node #8 in the continuous finite element space.
</div>


<!-- Image cell -->


<div class="image">
Figure 2:Four elements from the mesh for the sphere
<br/>
  <img src="/figures/shape-functions-and-basis-functions-in-fem-publish_35.png" alt="Diagram" style="max-width:90%;" loading="lazy" /></div>


<!-- Text cell -->


<div class="comment">
The vertices in each cell are enumerated below.<br/>Cell 92: [9 71 77 8]<br/>Cell 94: [8 77 72 2]<br/>Cell 110: [2 84 90 8]<br/>Cell 112: [8 90 85 9]
</div>


<!-- Text cell -->


<div class="comment">
These vertices are arranged in the clockwise order. When the mesh is read into a FEM solver, these vertices will be rearranged into lexicographic order for example by swapping the last two vertices in a cell. Hence they become<br/>Cell 92: [9 71 8 77]<br/>Cell 94: [8 77 2 72]<br/>Cell 110: [2 84 8 90]<br/>Cell 112: [8 90 9 85]
</div>


<!-- Text cell -->


<div class="comment">
Then shape functions with indices [1,2,3,4] defined by a first order continuous finite element in the reference cell will be assigned to these vertices in order. Therefore, for the basis function at node #8 in the finite element space, it is a concatenation of 4 shape functions:<br/>Shape function #3 in the reference cell corresponding to cell 92<br/>Shape function #1 in the reference cell corresponding to cell 94<br/>Shape function #3 in the reference cell corresponding to cell 110<br/>Shape function #1 in the reference cell corresponding to cell 112
</div>


<!-- Text cell -->


<div class="comment">
To visualize the distribution of basis function on these four cells, which are actually its support set, we can plot four parametric surfaces using the map from each reference cell to its corresponding real cell. Then, we colorize the parametric surface using the value of related shape function.
</div>


<!-- Text cell -->


<div class="comment">
Vertex coordinates
</div>


<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_variable">node9</span> <span class="code_operator">:</span> <span class="code_operator">[</span><span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">7071067827963321</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">7071067795767629</span><span class="code_operator">]</span><span class="code_endofline">$</span>  </span></td>
</tr></table>



<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_variable">node71</span> <span class="code_operator">:</span> <span class="code_operator">[</span><span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">2959042210220728</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">6562956747206669</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">6940581238803165</span><span class="code_operator">]</span><span class="code_endofline">$</span>  </span></td>
</tr></table>



<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_variable">node77</span> <span class="code_operator">:</span> <span class="code_operator">[</span><span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">2069290967160619</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">4625779322341045</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">8620916456748057</span><span class="code_operator">]</span><span class="code_endofline">$</span>  </span></td>
</tr></table>



<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_variable">node8</span> <span class="code_operator">:</span> <span class="code_operator">[</span><span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">382683433164956</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">9238795321799713</span><span class="code_operator">]</span><span class="code_endofline">$</span>  </span></td>
</tr></table>



<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_variable">node72</span> <span class="code_operator">:</span> <span class="code_operator">[</span><span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">3048963989653106</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">3017651820890481</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">903313877219181</span><span class="code_operator">]</span><span class="code_endofline">$</span>  </span></td>
</tr></table>



<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_variable">node2</span> <span class="code_operator">:</span> <span class="code_operator">[</span><span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_operator">]</span><span class="code_endofline">$</span>  </span></td>
</tr></table>



<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_variable">node84</span> <span class="code_operator">:</span> <span class="code_operator">[</span><span class="code_operator">-</span><span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">3037257264799272</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">3035840488652949</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">9030987810581932</span><span class="code_operator">]</span><span class="code_endofline">$</span>  </span></td>
</tr></table>



<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_variable">node90</span> <span class="code_operator">:</span> <span class="code_operator">[</span><span class="code_operator">-</span><span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">2041682670359604</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">461491589969017</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">8633312406738223</span><span class="code_operator">]</span><span class="code_endofline">$</span>  </span></td>
</tr></table>



<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_variable">node85</span> <span class="code_operator">:</span> <span class="code_operator">[</span><span class="code_operator">-</span><span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">2959042210220861</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">6562956747206589</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">.</span><span class="code_number">6940581238803184</span><span class="code_operator">]</span><span class="code_endofline">$</span>  </span></td>
</tr></table>



<!-- Text cell -->


<div class="comment">
Because the cells are flat, we use first order shape function in the map from reference cell to real cells.
</div>


<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_variable">shape_functions_for_map_2d</span> <span class="code_operator">:</span> <span class="code_endofline"><br/>
</span><span class="code_function">GenLagrangeBases2D</span><span class="code_operator">(</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">reference_cell_1d</span><span class="code_endofline">,</span> <span class="code_variable">reference_cell_1d</span><span class="code_endofline">,</span> <span class="code_operator">'</span><span class="code_operator">[</span><span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_variable">η</span><span class="code_operator">]</span><span class="code_operator">)</span><span class="code_endofline">;</span>  </span></td>
</tr></table>
<p>
\[\operatorname{(shape\_ functions\_ for\_ map\_ 2d)	}\left[ \left( 1-\eta \right) \, \left( 1-\xi \right) \operatorname{,}\left( 1-\eta \right)  \xi \operatorname{,}\eta \, \left( 1-\xi \right) \operatorname{,}\eta  \xi \right] \]
</p>


<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_variable">cell92_map</span> <span class="code_operator">:</span> <span class="code_variable">node9</span> <span class="code_operator">*</span> <span class="code_variable">shape_functions_for_map_2d</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span> <span class="code_operator">+</span> <span class="code_variable">node71</span> <span class="code_operator">*</span> <span class="code_variable">shape_functions_for_map_2d</span><span class="code_operator">[</span><span class="code_number">2</span><span class="code_operator">]</span> <span class="code_operator">+</span><span class="code_endofline"><br/>
</span><span class="code_variable">node8</span> <span class="code_operator">*</span> <span class="code_variable">shape_functions_for_map_2d</span><span class="code_operator">[</span><span class="code_number">3</span><span class="code_operator">]</span> <span class="code_operator">+</span> <span class="code_variable">node77</span> <span class="code_operator">*</span> <span class="code_variable">shape_functions_for_map_2d</span><span class="code_operator">[</span><span class="code_number">4</span><span class="code_operator">]</span><span class="code_endofline">$</span>  </span></td>
</tr></table>



<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_variable">cell94_map</span> <span class="code_operator">:</span> <span class="code_variable">node8</span> <span class="code_operator">*</span> <span class="code_variable">shape_functions_for_map_2d</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span> <span class="code_operator">+</span> <span class="code_variable">node77</span> <span class="code_operator">*</span> <span class="code_variable">shape_functions_for_map_2d</span><span class="code_operator">[</span><span class="code_number">2</span><span class="code_operator">]</span> <span class="code_operator">+</span><span class="code_endofline"><br/>
</span><span class="code_variable">node2</span> <span class="code_operator">*</span> <span class="code_variable">shape_functions_for_map_2d</span><span class="code_operator">[</span><span class="code_number">3</span><span class="code_operator">]</span> <span class="code_operator">+</span> <span class="code_variable">node72</span> <span class="code_operator">*</span> <span class="code_variable">shape_functions_for_map_2d</span><span class="code_operator">[</span><span class="code_number">4</span><span class="code_operator">]</span><span class="code_endofline">$</span>  </span></td>
</tr></table>



<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_variable">cell110_map</span> <span class="code_operator">:</span> <span class="code_variable">node2</span> <span class="code_operator">*</span> <span class="code_variable">shape_functions_for_map_2d</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span> <span class="code_operator">+</span> <span class="code_variable">node84</span> <span class="code_operator">*</span> <span class="code_variable">shape_functions_for_map_2d</span><span class="code_operator">[</span><span class="code_number">2</span><span class="code_operator">]</span> <span class="code_operator">+</span><span class="code_endofline"><br/>
</span><span class="code_variable">node8</span> <span class="code_operator">*</span> <span class="code_variable">shape_functions_for_map_2d</span><span class="code_operator">[</span><span class="code_number">3</span><span class="code_operator">]</span> <span class="code_operator">+</span> <span class="code_variable">node90</span> <span class="code_operator">*</span> <span class="code_variable">shape_functions_for_map_2d</span><span class="code_operator">[</span><span class="code_number">4</span><span class="code_operator">]</span><span class="code_endofline">$</span>  </span></td>
</tr></table>



<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_variable">cell112_map</span> <span class="code_operator">:</span> <span class="code_variable">node8</span> <span class="code_operator">*</span> <span class="code_variable">shape_functions_for_map_2d</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span> <span class="code_operator">+</span> <span class="code_variable">node90</span> <span class="code_operator">*</span> <span class="code_variable">shape_functions_for_map_2d</span><span class="code_operator">[</span><span class="code_number">2</span><span class="code_operator">]</span> <span class="code_operator">+</span><span class="code_endofline"><br/>
</span><span class="code_variable">node9</span> <span class="code_operator">*</span> <span class="code_variable">shape_functions_for_map_2d</span><span class="code_operator">[</span><span class="code_number">3</span><span class="code_operator">]</span> <span class="code_operator">+</span> <span class="code_variable">node85</span> <span class="code_operator">*</span> <span class="code_variable">shape_functions_for_map_2d</span><span class="code_operator">[</span><span class="code_number">4</span><span class="code_operator">]</span><span class="code_endofline">$</span>  </span></td>
</tr></table>



<!-- Text cell -->


<div class="comment">
When the continuous finite element space is first order, its shape functions are the same as the shape functions used by the first order map.
</div>


<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_variable">shape_functions_for_finite_element</span> <span class="code_operator">:</span> <span class="code_variable">shape_functions_for_map_2d</span><span class="code_endofline">$</span>  </span></td>
</tr></table>



<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_function">wxdraw3d</span><span class="code_operator">(</span><span class="code_variable">grid</span><span class="code_operator">=</span><span class="code_function">true</span><span class="code_endofline">,</span> <span class="code_variable">dimensions</span><span class="code_operator">=</span><span class="code_variable">fig_size</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">terminal</span><span class="code_operator">=</span><span class="code_variable">wxt</span><span class="code_endofline">,</span> <span class="code_variable">font</span><span class="code_operator">=</span><span class="code_variable">font_name</span><span class="code_endofline">,</span> <span class="code_variable">font_size</span><span class="code_operator">=</span><span class="code_variable">fig_font_size</span><span class="code_endofline">,</span> <span class="code_variable">proportional_axes</span><span class="code_operator">=</span><span class="code_variable">xyz</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">xlabel</span><span class="code_operator">=</span><span class="code_string">&quot;x&quot;</span><span class="code_endofline">,</span> <span class="code_variable">ylabel</span><span class="code_operator">=</span><span class="code_string">&quot;y&quot;</span><span class="code_endofline">,</span> <span class="code_variable">zlabel</span><span class="code_operator">=</span><span class="code_string">&quot;z&quot;</span><span class="code_endofline">,</span> <span class="code_variable">view</span><span class="code_operator">=</span><span class="code_operator">[</span><span class="code_number">38</span><span class="code_endofline">,</span><span class="code_number">207</span><span class="code_operator">]</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">enhanced3d</span><span class="code_operator">=</span><span class="code_operator">[</span><span class="code_variable">shape_functions_for_finite_element</span><span class="code_operator">[</span><span class="code_number">3</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_variable">η</span><span class="code_operator">]</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_function">parametric_surface</span><span class="code_operator">(</span><span class="code_variable">cell92_map</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">cell92_map</span><span class="code_operator">[</span><span class="code_number">2</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">cell92_map</span><span class="code_operator">[</span><span class="code_number">3</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_endofline">,</span> <span class="code_variable">η</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_operator">)</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">enhanced3d</span><span class="code_operator">=</span><span class="code_operator">[</span><span class="code_variable">shape_functions_for_finite_element</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_variable">η</span><span class="code_operator">]</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_function">parametric_surface</span><span class="code_operator">(</span><span class="code_variable">cell94_map</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">cell94_map</span><span class="code_operator">[</span><span class="code_number">2</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">cell94_map</span><span class="code_operator">[</span><span class="code_number">3</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_endofline">,</span> <span class="code_variable">η</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_operator">)</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">enhanced3d</span><span class="code_operator">=</span><span class="code_operator">[</span><span class="code_variable">shape_functions_for_finite_element</span><span class="code_operator">[</span><span class="code_number">3</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_variable">η</span><span class="code_operator">]</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_function">parametric_surface</span><span class="code_operator">(</span><span class="code_variable">cell110_map</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">cell110_map</span><span class="code_operator">[</span><span class="code_number">2</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">cell110_map</span><span class="code_operator">[</span><span class="code_number">3</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_endofline">,</span> <span class="code_variable">η</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_operator">)</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">enhanced3d</span><span class="code_operator">=</span><span class="code_operator">[</span><span class="code_variable">shape_functions_for_finite_element</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_variable">η</span><span class="code_operator">]</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_function">parametric_surface</span><span class="code_operator">(</span><span class="code_variable">cell112_map</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">cell112_map</span><span class="code_operator">[</span><span class="code_number">2</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">cell112_map</span><span class="code_operator">[</span><span class="code_number">3</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_endofline">,</span> <span class="code_variable">η</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_operator">)</span><span class="code_endofline"><br/>
</span>    <span class="code_operator">)</span><span class="code_endofline">;</span>  </span></td>
</tr></table>

  <img src="/figures/shape-functions-and-basis-functions-in-fem-publish_38.png"  style="max-width:90%;" loading="lazy" alt=" (Graphics) " /><br/>




<!-- Text cell -->


<div class="comment">
If the finite element is 2nd order continuous, the shape functions in the reference cell are organized in the lexicographic order:<br/>^<br/>|  7 8 9<br/>|  4 5 6<br/>|  1 2 3<br/>----------&gt;
</div>


<!-- Text cell -->


<div class="comment">
Then for the basis function at node #8 in the finite element space, it is a concatenation of 4 shape functions:<br/>Shape function #7 in the reference cell corresponding to cell 92<br/>Shape function #1 in the reference cell corresponding to cell 94<br/>Shape function #7 in the reference cell corresponding to cell 110<br/>Shape function #1 in the reference cell corresponding to cell 112
</div>


<!-- Text cell -->


<div class="comment">
Define Lagrange shape functions for the 2nd order finite element:
</div>


<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_variable">shape_functions_for_finite_element</span> <span class="code_operator">:</span> <span class="code_function">GenLagrangeBases2D</span><span class="code_operator">(</span><span class="code_operator">[</span><span class="code_number">2</span><span class="code_endofline">,</span> <span class="code_number">2</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">reference_cell_1d</span><span class="code_endofline">,</span> <span class="code_variable">reference_cell_1d</span><span class="code_endofline">,</span> <span class="code_operator">'</span><span class="code_operator">[</span><span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_variable">η</span><span class="code_operator">]</span><span class="code_operator">)</span><span class="code_endofline">$</span><span class="code_endofline"><br/>
</span><span class="code_function">PrintEquationList</span><span class="code_operator">(</span><span class="code_variable">shape_functions_for_finite_element</span><span class="code_operator">)</span><span class="code_endofline">;</span>  </span></td>
</tr></table>
<p>
\[\]\[4 \left( \eta -1\right) \, \left( \eta -\frac{1}{2}\right) \, \left( \xi -1\right) \, \left( \xi -\frac{1}{2}\right) 
\]\[-8 \left( \eta -1\right) \, \left( \eta -\frac{1}{2}\right) \, \left( \xi -1\right)  \xi 
\]\[4 \left( \eta -1\right) \, \left( \eta -\frac{1}{2}\right) \, \left( \xi -\frac{1}{2}\right)  \xi 
\]\[-8 \left( \eta -1\right)  \eta \, \left( \xi -1\right) \, \left( \xi -\frac{1}{2}\right) 
\]\[16 \left( \eta -1\right)  \eta \, \left( \xi -1\right)  \xi 
\]\[-8 \left( \eta -1\right)  \eta \, \left( \xi -\frac{1}{2}\right)  \xi 
\]\[4 \left( \eta -\frac{1}{2}\right)  \eta \, \left( \xi -1\right) \, \left( \xi -\frac{1}{2}\right) 
\]\[-8 \left( \eta -\frac{1}{2}\right)  \eta \, \left( \xi -1\right)  \xi 
\]\[4 \left( \eta -\frac{1}{2}\right)  \eta \, \left( \xi -\frac{1}{2}\right)  \xi \]
</p>



<!-- Text cell -->


<div class="comment">
The basis function at node #8 can be plotted as below.
</div>


<!-- Code cell -->


<table><tr><td>
  <span class="prompt">
 -->	
  </span></td>
  <td><span class="input">
<span class="code_function">wxdraw3d</span><span class="code_operator">(</span><span class="code_variable">grid</span><span class="code_operator">=</span><span class="code_function">true</span><span class="code_endofline">,</span> <span class="code_variable">dimensions</span><span class="code_operator">=</span><span class="code_variable">fig_size</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">terminal</span><span class="code_operator">=</span><span class="code_variable">wxt</span><span class="code_endofline">,</span> <span class="code_variable">font</span><span class="code_operator">=</span><span class="code_variable">font_name</span><span class="code_endofline">,</span> <span class="code_variable">font_size</span><span class="code_operator">=</span><span class="code_variable">fig_font_size</span><span class="code_endofline">,</span> <span class="code_variable">proportional_axes</span><span class="code_operator">=</span><span class="code_variable">xyz</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">xlabel</span><span class="code_operator">=</span><span class="code_string">&quot;x&quot;</span><span class="code_endofline">,</span> <span class="code_variable">ylabel</span><span class="code_operator">=</span><span class="code_string">&quot;y&quot;</span><span class="code_endofline">,</span> <span class="code_variable">zlabel</span><span class="code_operator">=</span><span class="code_string">&quot;z&quot;</span><span class="code_endofline">,</span> <span class="code_variable">view</span><span class="code_operator">=</span><span class="code_operator">[</span><span class="code_number">38</span><span class="code_endofline">,</span><span class="code_number">207</span><span class="code_operator">]</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">enhanced3d</span><span class="code_operator">=</span><span class="code_operator">[</span><span class="code_variable">shape_functions_for_finite_element</span><span class="code_operator">[</span><span class="code_number">7</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_variable">η</span><span class="code_operator">]</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_function">parametric_surface</span><span class="code_operator">(</span><span class="code_variable">cell92_map</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">cell92_map</span><span class="code_operator">[</span><span class="code_number">2</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">cell92_map</span><span class="code_operator">[</span><span class="code_number">3</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_endofline">,</span> <span class="code_variable">η</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_operator">)</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">enhanced3d</span><span class="code_operator">=</span><span class="code_operator">[</span><span class="code_variable">shape_functions_for_finite_element</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_variable">η</span><span class="code_operator">]</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_function">parametric_surface</span><span class="code_operator">(</span><span class="code_variable">cell94_map</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">cell94_map</span><span class="code_operator">[</span><span class="code_number">2</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">cell94_map</span><span class="code_operator">[</span><span class="code_number">3</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_endofline">,</span> <span class="code_variable">η</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_operator">)</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">enhanced3d</span><span class="code_operator">=</span><span class="code_operator">[</span><span class="code_variable">shape_functions_for_finite_element</span><span class="code_operator">[</span><span class="code_number">7</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_variable">η</span><span class="code_operator">]</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_function">parametric_surface</span><span class="code_operator">(</span><span class="code_variable">cell110_map</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">cell110_map</span><span class="code_operator">[</span><span class="code_number">2</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">cell110_map</span><span class="code_operator">[</span><span class="code_number">3</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_endofline">,</span> <span class="code_variable">η</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_operator">)</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_variable">enhanced3d</span><span class="code_operator">=</span><span class="code_operator">[</span><span class="code_variable">shape_functions_for_finite_element</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_variable">η</span><span class="code_operator">]</span><span class="code_endofline">,</span><span class="code_endofline"><br/>
</span>    <span class="code_function">parametric_surface</span><span class="code_operator">(</span><span class="code_variable">cell112_map</span><span class="code_operator">[</span><span class="code_number">1</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">cell112_map</span><span class="code_operator">[</span><span class="code_number">2</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">cell112_map</span><span class="code_operator">[</span><span class="code_number">3</span><span class="code_operator">]</span><span class="code_endofline">,</span> <span class="code_variable">ξ</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_endofline">,</span> <span class="code_variable">η</span><span class="code_endofline">,</span> <span class="code_number">0</span><span class="code_endofline">,</span> <span class="code_number">1</span><span class="code_operator">)</span><span class="code_endofline"><br/>
</span>    <span class="code_operator">)</span><span class="code_endofline">;</span>  </span></td>
</tr></table>

  <img src="/figures/shape-functions-and-basis-functions-in-fem-publish_43.png"  style="max-width:90%;" loading="lazy" alt=" (Graphics) " /><br/>

<p></p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="FEM" /><category term="Maxima" /><summary type="html"><![CDATA[According to my graduate student experience, the difference between shape functions and basis functions in FEM was not clarified in my numerical method course. Therefore, in this article, I illustrate and explain this point with the help of the symbolic math software Maxima.]]></summary></entry><entry><title type="html">Summary of Green’s identities</title><link href="https://jihuan-tian.github.io/math/2024/08/02/summary-of-green-identities.html" rel="alternate" type="text/html" title="Summary of Green’s identities" /><published>2024-08-02T00:00:00+08:00</published><updated>2024-08-02T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/08/02/summary-of-green-identities</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/08/02/summary-of-green-identities.html"><![CDATA[<h3 class='likesectionHead'><a id='x1-1000'></a>Contents</h3>
   <div class='tableofcontents'>
    <span class='sectionToc'>1 <a href='#x1-20001' id='QQ2-1-2'>Classical formulation of Green’s identities</a></span>
<br />    <span class='sectionToc'>2 <a href='#x1-30002' id='QQ2-1-3'>Representation formula for the Laplace equation: the third Green’s identity</a></span>
<br />    <span class='sectionToc'>3 <a href='#x1-40003' id='QQ2-1-4'>General formulation of Green’s identities</a></span>
   </div>
<!-- l. 24 --><p class='indent'>   Green’s identities are indispensable for solving partial differential equations. For example, consider the
Poisson’s equation \begin{equation}  -\Delta u = f \quad \text {in \(\Omega \)},  \end{equation}<a id='x1-1001r1'></a> with a Dirichlet boundary condition \(\gamma _0 u=g_{\mathrm {D}}\) on \(\Gamma _{\mathrm {D}}\) and Neumann condition \(\gamma _1u = g_{\mathrm {N}}\) on \(\Gamma _{\mathrm {N}}\). We can apply a test
function \(v\in H_0^1(\Omega )\) to both sides of the equation: \begin{equation}  \int _{\Omega } -v\Delta u \intd x = \int _{\Omega } fv \intd x.  \end{equation}<a id='x1-1002r2'></a> Then use the Green’s first identity \begin{equation}  \int _{\Omega }v\Delta u \intd x =-\int _{\Omega } (\nabla u)\cdot (\nabla v) \intd x + \int _{\pdiff \Omega } v \frac {\pdiff u}{\pdiff \vect {n}} \intd s,  \end{equation}<a id='x1-1003r3'></a> and consider the fact that \(v\) vanishes on
\(\Gamma _{\mathrm {D}}\), we have the weak form or variational form of the original equation as \begin{equation}  \int _{\Omega } (\nabla u)\cdot (\nabla v) \intd x = -\int _{\Omega }v\Delta u \intd x + \int _{\pdiff \Omega } v \frac {\pdiff u}{\pdiff \vect {n}} \intd s = -\int _{\Omega } fv \intd x + \int _{\Gamma _{\mathrm {N}}} v \frac {\pdiff u}{\pdiff \vect {n}} \intd s.  \end{equation}<a id='x1-1004r4'></a> This equation can be directly solved
using the FEM.
</p><!-- l. 45 --><p class='indent'>   If the BEM is used, the procedure is to first represent the solution \(u\) in the domain \(\Omega \) using an integral form, then
take the limit \(x \rightarrow \Gamma \) , i.e. let the target point approach the domain boundary, and a boundary integral equation (BIE)
can be obtained using the limiting properties of integral operators. The derivation of the representation formula
also involves the second Green’s identity.
</p><!-- l. 47 --><p class='indent'>   Therefore, in this essay, I will summarize various forms of Green’s identities.
</p>
   <h3 class='sectionHead'><span class='titlemark'>1    </span> <a id='x1-20001'></a>Classical formulation of Green’s identities</h3>
   <div class='theorem'><div class='newtheorem'>
<!-- l. 50 --><p class='noindent'><span class='head'>
<span class='p1xb-x-x-109'>Theorem 1 (Gauss-Ostrogradski Theorem)</span> </span><a id='x1-2002'></a><span class='p1xi-x-x-109'>Let </span>\(u\in C^1(\bar \Omega )\)<span class='p1xi-x-x-109'>. Then </span>\begin{equation}  \int _{\Omega } \frac {\pdiff u}{\pdiff x_{i}} \intd x = \int _{\pdiff \Omega } \gamma _0^{\rm int}u n_i \intd s \quad i=1,\cdots ,d,  \end{equation}<a id='x1-2003r5'></a> <span class='p1xi-x-x-109'>where </span>\(n_i\) <span class='p1xi-x-x-109'>is the </span>\(i\)<span class='p1xi-x-x-109'>-th component of the outward unit normal vector
</span><span class='p1xi-x-x-109'>field at </span>\(x\)<span class='p1xi-x-x-109'>, </span>\(\gamma _0^{\rm int}\) <span class='p1xi-x-x-109'>is the Dirichlet trace operator.</span>
</p>
   </div>
                                                                                               
                                                                                               
<!-- l. 59 --><p class='indent'>   </p></div>
   <div class='proof'><div class='newtheorem'>
<!-- l. 61 --><p class='noindent'><span class='head'>
<span class='p1xsc-x-x-109'>P<span class='small-caps'>roof</span></span> </span><a id='x1-2005'></a>Assume the \(d\) dimensional domain \(\Omega \) is convex. Then the value range, i.e. lower bound and upper
bound, for the \(i\)-th coordinate \(x_i\) can be represented by the other coordinate components as \begin{equation}  x_i \in [f_{\mathrm {L}}(x_1,\cdots ,\hat {x_i},\cdots ,x_d), f_{\mathrm {H}}(x_1,\cdots ,\hat {x_i},\cdots ,x_d)].  \end{equation}<a id='x1-2006r6'></a> The two
equations \begin{equation}  \label {eq:lower-bound-xi-domain-boundary} x_i = f_{\mathrm {L}}(x_1,\cdots ,\hat {x_i},\cdots ,x_d)  \end{equation}<a id='x1-2007r7'></a> and \begin{equation}  \label {eq:upper-bound-xi-domain-boundary} x_i = f_{\mathrm {H}}(x_1,\cdots ,\hat {x_i},\cdots ,x_d)  \end{equation}<a id='x1-2008r8'></a> determine two subsets of the boundary \(\partial \Omega \), which are \(d-1\) dimensional manifolds in
\(\mathbb {R}^d\).
</p><!-- l. 78 --><p class='indent'>   Using the Fubini’s theorem, the left hand side integration on \(\Omega \) can be split into to two successive parts, one is
with respect to the coordinate \(x_i\), the other is with respect to remaining coordinates: \begin{equation}  \int _{\Omega } \frac {\pdiff u}{\pdiff x_i} \intd x = \int _U \intd x_1\cdots \intd \hat {x_i}\cdots \intd x_d \int _{f_{\mathrm {L}}(x_1,\cdots ,\hat {x_i},\cdots ,x_d)}^{f_{\mathrm {H}}(x_1,\cdots ,\hat {x_i},\cdots ,x_d)} \frac {\pdiff u}{\pdiff x_i} \intd x_i,  \end{equation}<a id='x1-2009r9'></a> where \(U\) is the integration
domain with respect to the remaining coordinate variables. Therefore, the left hand side of the
Gauss-Ostrogradski identity is equal to \begin{equation}  \int _U \left [ u(x_1,\cdots ,f_{\mathrm {H}},\cdots ,x_d) - u(x_1,\cdots ,f_{\mathrm {L}},\cdots ,x_d) \right ] \intd x_1\cdots \intd \hat {x_i}\cdots \intd x_d.  \end{equation}<a id='x1-2010r10'></a>
</p><!-- l. 92 --><p class='indent'>   For \(n^i\) in the boundary integral on the right hand side, it is the \(i\)-th component of the <span class='p1xb-x-x-109'>outward </span>normal vector of \(\partial \Omega \).
Because Equation (<a href='#x1-2007r7'>7<!-- tex4ht:ref: eq:lower-bound-xi-domain-boundary  --></a>) describes the lower domain boundary with respect to the \(i\)-th coordinate (written as \(\partial \Omega _{\mathrm {L}_i}\)) and
Equation (<a href='#x1-2008r8'>8<!-- tex4ht:ref: eq:upper-bound-xi-domain-boundary  --></a>) describes the upper domain boundary \(\partial \Omega _{\mathrm {H}_i}\), \(n^i &lt; 0\) on \(\partial \Omega _{\mathrm {L}_i}\) and \(n^i &gt; 0\) on \(\partial \Omega _{\mathrm {H}_i}\). For remaining parts of \(\partial \Omega \), \(n^i=0\). This is illustrated in
the following figure.
</p>
<div class='center'>
<!-- l. 94 --><p class='noindent'>
</p><!-- l. 95 --><p class='noindent'><img alt='PIC' src='/figures/2024-08-02-10-12-illustration-of-gauss-ostrogradski.png'' /></p></div>
<!-- l. 98 --><p class='indent'>   Furthermore, because the geometric meaning of \(n^{i}\) is the cosine of the angle between the outward normal
vector and the \(i\)-th coordinate axis, \(n^i \intd s\) is actually a signed projection of the surface integral element \(\intd s\) to the
multi-dimensional coordinate “plane” spanned by \((x_1,\cdots ,\hat {x_i},\cdots ,x_d)\).
</p><!-- l. 100 --><p class='indent'>   Therefore, the right hand side is \begin{equation}  \begin {aligned} \rhs &amp;= \int _{\partial \Omega _{\mathrm {H}_i}} u(x_1,\cdots ,f_{\mathrm {H}},\cdots ,x_d) n^i \intd s + \int _{\partial \Omega _{\mathrm {L}_i}} u(x_1,\cdots ,f_{\mathrm {L}},\cdots ,x_d) n^i \intd s \\ &amp;= \int _U \left [ u(x_1,\cdots ,f_{\mathrm {H}},\cdots ,x_d) - u(x_1,\cdots ,f_{\mathrm {L}},\cdots ,x_d) \right ] \intd x_1\cdots \intd \hat {x_i}\cdots \intd x_d \end {aligned}.  \end{equation}<a id='x1-2011r11'></a> This is just the same as the left hand side.
</p><!-- l. 113 --><p class='indent'>   When the domain is not convex, we can split it into several convex subdomains \(\left \{\Omega \right \}_{i=1}^n\) and apply the above
conclusion to each \(\Omega _i\). Then we add the identities for all subdomains. Because the boundary integrals cancel on
interior interfaces, the sum of these identities is the Gauss-Ostrogradski identity for the global domain
\(\Omega \).
</p>
   </div>
<!-- l. 114 --><p class='indent'>   </p></div>
   <div class='theorem'><div class='newtheorem'>
<!-- l. 116 --><p class='noindent'><span class='head'>
<span class='p1xb-x-x-109'>Theorem 2 (Gauss divergence Theorem)</span> </span><a id='x1-2013'></a><span class='p1xi-x-x-109'>Let </span>\(u\in \left [ C^1(\bar \Omega ) \right ]^d\)<span class='p1xi-x-x-109'>. Then </span>\begin{equation}  \int _{\Omega } \divergence \vect {u} \intd x = \int _{\pdiff \Omega } \gamma _0^{\rm int} \vect {u}\cdot \vect {n} \intd s.  \end{equation}<a id='x1-2014r12'></a>
</p>
   </div>
<!-- l. 122 --><p class='indent'>   </p></div>
   <div class='proof'><div class='newtheorem'>
<!-- l. 124 --><p class='noindent'><span class='head'>
                                                                                               
                                                                                               
<span class='p1xsc-x-x-109'>P<span class='small-caps'>roof</span></span> </span><a id='x1-2016'></a>Apply the Gauss-Ostrogradski Theorem <a href='#x1-2003r1'>1<!-- tex4ht:ref: theo:gauss-ostrogradski  --></a> to each term in \(\divergence \vect {u}\).
</p>
   </div>
<!-- l. 126 --><p class='indent'>   </p></div>
   <div class='theorem'><div class='newtheorem'>
<!-- l. 128 --><p class='noindent'><span class='head'>
<span class='p1xb-x-x-109'>Theorem 3 (Integration by parts)</span> </span><a id='x1-2018'></a><span class='p1xi-x-x-109'>Let </span>\(u,v\in C^1(\bar \Omega )\)<span class='p1xi-x-x-109'>. Then </span>\begin{equation}  \int _{\Omega }\frac {\pdiff u}{\pdiff x_i} v \intd x + \int _{\Omega } u \frac {\pdiff v}{\pdiff x_i} \intd x = \int _{\pdiff \Omega } \left ( \gamma _0^{\rm int}u \right ) \left ( \gamma _0^{\rm int}v \right ) n_i \intd s.  \end{equation}<a id='x1-2019r13'></a>
</p>
   </div>
<!-- l. 136 --><p class='indent'>   </p></div>
   <div class='proof'><div class='newtheorem'>
<!-- l. 138 --><p class='noindent'><span class='head'>
<span class='p1xsc-x-x-109'>P<span class='small-caps'>roof</span></span> </span><a id='x1-2021'></a>Apply the Gauss-Ostrogradski Theorem <a href='#x1-2003r1'>1<!-- tex4ht:ref: theo:gauss-ostrogradski  --></a> to \(uv\).
</p>
   </div>
<!-- l. 140 --><p class='indent'>   </p></div>
   <div class='proposition'><div class='newtheorem'>
<!-- l. 142 --><p class='noindent'><span class='head'>
<span class='p1xb-x-x-109'>Proposition 1</span> </span><a id='x1-2023'></a><span class='p1xi-x-x-109'>Let </span>\(u\in C^2(\bar \Omega )\)<span class='p1xi-x-x-109'>. </span>\begin{equation}  \int _{\Omega } \Delta u \intd x = \int _{\pdiff \Omega } \frac {\pdiff u}{\pdiff \vect {n}} \intd s.  \end{equation}<a id='x1-2024r14'></a>
</p>
   </div>
<!-- l. 147 --><p class='indent'>   </p></div>
   <div class='proof'><div class='newtheorem'>
<!-- l. 149 --><p class='noindent'><span class='head'>
<span class='p1xsc-x-x-109'>P<span class='small-caps'>roof</span></span> </span><a id='x1-2026'></a>Apply Gauss divergence Theorem <a href='#x1-2014r2'>2<!-- tex4ht:ref: theo:gauss-div  --></a> to \(\nabla u\) or replace \(u\) with \(\frac {\pdiff u}{\pdiff x_{i}}\) and \(v\) with \(1\) in Theorem <a href='#x1-2019r3'>3<!-- tex4ht:ref: theo:int-by-parts  --></a>.
</p>
   </div>
<!-- l. 152 --><p class='indent'>   </p></div>
   <div class='theorem'><div class='newtheorem'>
<!-- l. 154 --><p class='noindent'><span class='head'>
<span class='p1xb-x-x-109'>Theorem 4 (Green’s first identity)</span> </span><a id='x1-2028'></a><span class='p1xi-x-x-109'>Let </span>\(u,v\in C^2(\bar \Omega )\)<span class='p1xi-x-x-109'>. </span>\begin{equation}  \label {eq:green-1st-identity} \begin {aligned} \int _{\Omega }v\Delta u \intd x &amp;=-\int _{\Omega } (\nabla u)\cdot (\nabla v) \intd x + \int _{\pdiff \Omega } v \frac {\pdiff u}{\pdiff \vect {n}} \intd s \\ \int _{\Omega }u\Delta v \intd x &amp;=-\int _{\Omega } (\nabla u)\cdot (\nabla v) \intd x + \int _{\pdiff \Omega } u \frac {\pdiff v}{\pdiff \vect {n}} \intd s \end {aligned}.  \end{equation}<a id='x1-2029r15'></a>
                                                                                               
                                                                                               
</p>
   </div>
<!-- l. 166 --><p class='indent'>   </p></div>
   <div class='proof'><div class='newtheorem'>
<!-- l. 168 --><p class='noindent'><span class='head'>
<span class='p1xsc-x-x-109'>P<span class='small-caps'>roof</span></span> </span><a id='x1-2031'></a>To prove the first equation, replace \(u\) with \(\frac {\pdiff u}{\pdiff x_{i}}\) in Theorem <a href='#x1-2019r3'>3<!-- tex4ht:ref: theo:int-by-parts  --></a>, we have
                                                \[ \int _{\Omega }\frac {\pdiff ^2 u}{\pdiff x_i^2} v \intd x=\int _{\pdiff \Omega } \frac {\pdiff u}{\pdiff x_i} v n_i \intd s - \int _{\Omega }\frac {\pdiff u}{\pdiff x_i}\frac {\pdiff v}{\pdiff x_{i}} \intd x. \]
 Then sum up this relation for \(i=1,\cdots ,d\) and the first equation is obtained. The proof of the second equation is
similar.
</p>
   </div>
<!-- l. 177 --><p class='indent'>   </p></div>
   <div class='theorem'><div class='newtheorem'>
<!-- l. 179 --><p class='noindent'><span class='head'>
<span class='p1xb-x-x-109'>Theorem 5 (Green’s second identity)</span> </span><a id='x1-2033'></a><span class='p1xi-x-x-109'>Let </span>\(u,v\in C^2(\bar \Omega )\)<span class='p1xi-x-x-109'>. </span>\begin{equation}  \label {eq:green-2nd-identity} \int _{\Omega } \left ( v\Delta u - u\Delta v \right ) \intd x = \int _{\pdiff \Omega } \left [ v \frac {\pdiff u}{\pdiff \vect {n}} - u \frac {\pdiff v}{\pdiff \vect {n}} \right ] \intd s.  \end{equation}<a id='x1-2034r16'></a>
</p>
   </div>
<!-- l. 187 --><p class='indent'>   </p></div>
   <div class='proof'><div class='newtheorem'>
<!-- l. 189 --><p class='noindent'><span class='head'>
<span class='p1xsc-x-x-109'>P<span class='small-caps'>roof</span></span> </span><a id='x1-2036'></a>Subtract the two equations in Theorem <a href='#x1-2029r4'>4<!-- tex4ht:ref: theo:green-1st-identity  --></a>, we get the Green’s second identity.
</p>
   </div>
<!-- l. 191 --><p class='indent'>   </p></div>
                                                                                               
                                                                                               
<!-- l. 193 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>2    </span> <a id='x1-30002'></a>Representation formula for the Laplace equation: the third Green’s identity</h3>
<!-- l. 195 --><p class='noindent'>Consider the Green’s second identity (<a href='#x1-2034r16'>16<!-- tex4ht:ref: eq:green-2nd-identity  --></a>), we replace \(v\) with the fundamental solution \(U^{\ast }(x,y)=\frac {1}{\lvert x-y \rvert }\) of the Laplace equation
and \(u\) with a harmonic function \(\varphi (y)\): \begin{equation}  \int _{\Omega } \left ( U^{\ast }(x,y)\Delta _y \varphi - \varphi \Delta _y U^{\ast }(x,y) \right ) \intd y = \int _{\pdiff \Omega } \left [ U^{\ast }(x,y) \frac {\pdiff \varphi }{\pdiff \vect {n}_y} - \varphi \frac {\pdiff U^{\ast }(x,y)}{\pdiff \vect {n}_y} \right ] \intd s_y,  \end{equation}<a id='x1-3001r17'></a> where \(x\in \Omega \). Because \(\varphi \) is harmonic, the first term on the left hand side is zero. The
singularity in the second integral on the left hand side should be evaluated in the sense of Cauchy principal
value, i.e. we remove a ball \(B_{\varepsilon }(x)\) with a radius \(\varepsilon \) around the singularity \(x\) from \(\Omega \), then apply the second Green’s identity
in \(\tilde {\Omega }:= \Omega \backslash B_{\varepsilon }(x)\), and finally take the limit \(\varepsilon \rightarrow 0\).
</p>
<div class='center'>
<!-- l. 202 --><p class='noindent'>
</p><!-- l. 203 --><p class='noindent'><img alt='PIC' src='/figures/2024-08-02-16-21-representation-formula-cauchy-principal-value.png' /></p></div>
<!-- l. 206 --><p class='indent'>   The left hand side is \begin{equation}  -\int _{\tilde {\Omega }} \varphi \Delta _y U^{\ast }(x,y) \intd y.  \end{equation}<a id='x1-3002r18'></a> The fundamental solution \(U^{\ast }(x-y)\) is singular at \(x\) and harmonic elsewhere, so it is always zero in
\(\tilde {\Omega }\). When \(\varepsilon \rightarrow 0\), the left hand side is zero.
</p><!-- l. 212 --><p class='indent'>   Because the boundary of \(\tilde {\Omega }\) comprises two parts \(\partial \Omega \) and \(\partial B_{\varepsilon }(x)\), the right hand side is \begin{equation}  \int _{\partial \Omega } \left [ U^{\ast }(x,y) \frac {\pdiff \varphi }{\pdiff \vect {n}_y} - \varphi \frac {\pdiff U^{\ast }(x,y)}{\pdiff \vect {n}_y} \right ] \intd s_y + \int _{\partial B_{\varepsilon }(x)} \left [ U^{\ast }(x,y) \frac {\pdiff \varphi }{\pdiff \vect {n}_y} - \varphi \frac {\pdiff U^{\ast }(x,y)}{\pdiff \vect {n}_y} \right ] \intd s_y.  \end{equation}<a id='x1-3003r19'></a> Because \(x\in \Omega \) and \(y\in \partial \Omega \), the first term has no
singularity and we just keep it there.
</p><!-- l. 222 --><p class='indent'>   In the second term, the normal vector \(\vect {n}_y\) at \(\partial B_{\varepsilon }(x)\) points inward and is equal to \(\frac {x-y}{\lvert x-y \rvert }\). The normal derivative of the
harmonic function is \begin{equation}  \frac {\partial \varphi }{\partial \vect {n}_y} = \nabla \varphi \cdot \frac {x-y}{\lvert x-y \rvert }.  \end{equation}<a id='x1-3004r20'></a> Then \begin{equation}  U^{\ast }(x,y)\frac {\partial \varphi }{\partial \vect {n}_y} = \frac {1}{4\pi \lvert x-y \rvert } \nabla \varphi \cdot \frac {x-y}{\lvert x-y \rvert } = \frac {1}{4\pi \varepsilon } \nabla \varphi \cdot \frac {x-y}{\lvert x-y \rvert }.  \end{equation}<a id='x1-3005r21'></a> When \(\varphi \in C^2(\Omega )\), \(\nabla \varphi \) is a continuous vector field. When \(\varepsilon \) is small enough, \(\nabla \varphi \) is nearly a constant
vector field through \(B_{\varepsilon }(x)\). Therefore, its flux at antipodal points on \(\partial B_{\varepsilon }(x)\) cancel out.
</p>
<div class='center'>
<!-- l. 234 --><p class='noindent'>
</p><!-- l. 235 --><p class='noindent'><img alt='PIC' src='/figures/2024-08-02-16-21-representation-formula-gradient-integral.png' /></p></div>
<!-- l. 238 --><p class='indent'>   Therefore, when \(\varepsilon \rightarrow 0\), \begin{equation}  \int _{\partial B_{\varepsilon }(x)} U^{\ast }(x,y)\frac {\partial \varphi }{\partial \vect {n}_y} \intd s_y = \frac {1}{4\pi \varepsilon } \int _{\partial B_{\varepsilon }(x)} \nabla \varphi \cdot \frac {x-y}{\lvert x-y \rvert } \intd s_y = 0.  \end{equation}<a id='x1-3006r22'></a>
</p><!-- l. 245 --><p class='indent'>   The normal derivative of the fundamental solution is \begin{equation}  \frac {\partial U^{\ast }(x,y)}{\partial \vect {n}_y} = \nabla _y U^{\ast }(x,y) \cdot \vect {n}_y = \nabla _y U^{\ast }(x,y) \cdot \frac {x-y}{\lvert x-y \rvert } = \frac {x-y}{4\pi \lvert x-y \rvert ^3} \frac {x-y}{\lvert x-y \rvert } = \frac {1}{4\pi \lvert x-y \rvert ^2}.  \end{equation}<a id='x1-3007r23'></a> Because \(\lvert x-y \rvert = \varepsilon \), \begin{equation}  \int _{\partial B_{\varepsilon }(x)} - \varphi (y) \frac {1}{4\pi \varepsilon ^2} \intd s_y.  \end{equation}<a id='x1-3008r24'></a> When \(\varepsilon \rightarrow 0\), \(\varphi (y) \rightarrow \varphi (x)\) and the above integral is just
\(-\varphi (x)\).
</p><!-- l. 258 --><p class='indent'>   Summarizing above results, we have the representation formula \begin{equation}  \varphi (x) = \int _{\partial \Omega } \left [ U^{\ast }(x,y) \frac {\pdiff \varphi }{\pdiff \vect {n}_y} - \varphi \frac {\pdiff U^{\ast }(x,y)}{\pdiff \vect {n}_y} \right ] \intd s_y \quad x\in \Omega ,  \end{equation}<a id='x1-3009r25'></a> which is a combination of single layer
potential and double layer potential. Such a representation formula is also called the third Green’s
identity.
</p><!-- l. 266 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>3    </span> <a id='x1-40003'></a>General formulation of Green’s identities</h3>
<!-- l. 268 --><p class='noindent'>Let \(X\) be a Hilbert space with the inner product \(\left \langle \cdot ,\cdot \right \rangle _X\) and the induced norm \(\sqrt {\left \langle \cdot ,\cdot \right \rangle _X}\). Let \(X'\) be the dual space of \(X\) with respect to the duality
pairing \(\left \langle \cdot ,\cdot \right \rangle \) <span class='footnote-mark'><a href='#fn1x0' id='fn1x0-bk'><sup class='textsuperscript'>1</sup></a></span><a id='x1-4001f1'></a>.
</p><!-- l. 270 --><p class='indent'>   Now we take the general self-adjoint second order differential operator \(L: X \rightarrow X'\) as an example. \begin{equation}  \label {eq:diff-operator-2nd-order} (Lu)(x):= -\sum _{i,j=1}^d \frac {\pdiff }{\pdiff x_j} \left [ a_{ji}(x)\frac {\pdiff u}{\pdiff x_{i}} \right ] + a_0(x)u(x).  \end{equation}<a id='x1-4003r26'></a> Because \(L\) is
self-adjoint, \(a_{ji}(x) = a_{ij}(x)\). Test \((Lu)(x)\) with \(v(x)\) and apply integration by parts in Theorem <a href='#x1-2019r3'>3<!-- tex4ht:ref: theo:int-by-parts  --></a>, we have \begin{equation}  \begin {aligned} \int _{\Omega }(Lu)(x)v(x) \intd x &amp;= \sum _{i,j=1}^d \int _{\Omega } a_{ji}(x)\frac {\pdiff u}{\pdiff x_{i}}\frac {\pdiff v}{\pdiff x_{j}} \intd x + \int _{\Omega } a_0(x)u(x)v(x) \intd x \\ &amp; \quad - \sum _{i,j=1}^d\int _{\pdiff \Omega } \gamma _0^{\rm int}\left ( a_{ji}(x)\frac {\pdiff u}{\pdiff x_{i}} \right ) \gamma _0^{\rm int}v(x) n_j \intd s \\ &amp;=\sum _{i,j=1}^d \int _{\Omega } a_{ji}(x)\frac {\pdiff u}{\pdiff x_{i}}\frac {\pdiff v}{\pdiff x_{j}} \intd x + \int _{\Omega } a_0(x)u(x)v(x) \intd x \\ &amp; \quad -\int _{\pdiff \Omega } \gamma _0^{\rm int} \left [ \sum _{i,j=1}^d n_j a_{ji}(x)\frac {\pdiff u}{\pdiff x_{i}} \right ] \gamma _0^{\rm int}v(x) \intd s. \end {aligned}  \end{equation}<a id='x1-4004r27'></a> Define the interior
conormal trace: \begin{equation}  \label {eq:interior-conormal-trace} \gamma _1^{\rm int}u(x):= \lim _{\Omega \ni \tilde {x} \rightarrow x \in \pdiff \Omega } \left [ \sum _{i,j=1}^d n_j(x)a_{ji}(\tilde {x})\frac {\pdiff u(\tilde {x})}{\pdiff \tilde {x}_i} \right ].  \end{equation}<a id='x1-4005r28'></a> Define the bilinear form \(a(u,v)\): \begin{equation}  \label {eq:bilinear-form-2nd-order-pde} a(u,v):= \sum _{i,j=1}^d\int _{\Omega }a_{ji}(x)\frac {\pdiff u}{\pdiff x_{i}}\frac {\pdiff v}{\pdiff x_{j}}\intd x + \int _{\Omega }a_0uv \intd x.  \end{equation}<a id='x1-4006r29'></a> Because the original operator \(L\) is self-adjoint, \(a_{ji}(x) = a_{ij}(x)\), the bilinear
                                                                                               
                                                                                               
form \(a(u,v)\) is symmetric. Then we have the general form of the Green’s first identity: \begin{equation}  \label {eq:green-1st-identity-general} a(u,v) = \int _{\Omega } (Lu)(x)v(x) \intd x + \int _{\pdiff \Omega } \gamma _1^{\rm int}u(x) \gamma _0^{\rm int}v(x) \intd s.  \end{equation}<a id='x1-4007r30'></a> Because \(a(u,v) = a(v,u)\), we also
have \begin{equation}  a(v,u) = \int _{\Omega } (Lv)(x)u(x) \intd x + \int _{\pdiff \Omega } \gamma _1^{\rm int}v(x) \gamma _0^{\rm int}u(x) \intd s.  \end{equation}<a id='x1-4008r31'></a> Subtract it from Equation (<a href='#x1-4007r30'>30<!-- tex4ht:ref: eq:green-1st-identity-general  --></a>), we obtain the general form of the Green’s second identity: \begin{equation}  \label {eq:green-2nd-identity-general} \int _{\Omega } (Lu)(x)v(x) \intd x - \int _{\Omega }(Lv)(x)u(x) \intd x = \int _{\pdiff \Omega }\gamma _1^{\rm int}v(x)\gamma _0^{\rm int}u(x) \intd s - \int _{\pdiff \Omega } \gamma _1^{\rm int}u(x)\gamma _0^{\rm int}v(x) \intd s.  \end{equation}<a id='x1-4009r32'></a>
With duality pairing, these two Green’s identities can be written in a compact form as below. \begin{equation}  \label {eq:green-1st-identity-compact} a(u,v) = \left \langle Lu, v \right \rangle _{\Omega } + \left \langle \gamma _1^{\rm int}u, \gamma _0^{\rm int}v \right \rangle _{\Gamma }.  \end{equation}<a id='x1-4010r33'></a> \begin{equation}  \label {eq:green-2nd-identity-compact} \left \langle Lu, v \right \rangle _{\Omega } - \left \langle u, Lv \right \rangle _{\Omega } = \left \langle \gamma _1^{\rm int}v, \gamma _0^{\rm int}u \right \rangle _{\Gamma } - \left \langle \gamma _1^{\rm int}u, \gamma _0^{\rm int}v \right \rangle _{\Gamma }.  \end{equation}<a id='x1-4011r34'></a>
Here we use the subscript \(\Omega \) to indicate volume integral and use the subscript \(\Gamma \) to indicate surface
integral.
</p><!-- l. 333 --><p class='indent'>   Specifically, when the coefficient \(a_{ji}\) is the Kronecker delta \(\delta _{ji}\) and \(a_0 = 0\), we obtain the <span class='p1xb-x-x-109'>negative </span>Laplace operator \(-\Delta \).
Hence the bilinear form is \begin{equation}  a(u,v) = \int _{\Omega } \sum _{i=1}^d \frac {\partial u}{\partial x_i} \frac {\partial v}{\partial x_j} \intd x = \int _{\Omega } \grad u \cdot \grad v \intd x.  \end{equation}<a id='x1-4012r35'></a> The general Green’s first identity becomes \begin{equation}  \label {eq:green-1st-identity-general-laplace} \int _{\Omega }\grad u\cdot \grad v \intd x = \int _{\Omega }-v\Delta u \intd x + \int _{\Omega } v \frac {\pdiff u}{\pdiff \vect {n}} \intd s,  \end{equation}<a id='x1-4013r36'></a> which is consistent with Equation
(<a href='#x1-2029r15'>15<!-- tex4ht:ref: eq:green-1st-identity  --></a>).
</p><!-- l. 346 --><p class='indent'>   The general Green’s second identity becomes \begin{equation}  \left \langle \Delta u, v \right \rangle _{\Omega } - \left \langle u, \Delta v \right \rangle _{\Omega } = \left \langle \gamma _1^{\rm int}u, \gamma _0^{\rm int}v \right \rangle _{\Gamma } - \left \langle \gamma _1^{\rm int}v, \gamma _0^{\rm int}u \right \rangle _{\Gamma },  \end{equation}<a id='x1-4014r37'></a> which is consistent with Equation (<a href='#x1-2034r16'>16<!-- tex4ht:ref: eq:green-2nd-identity  --></a>).
</p><!-- l. 1 --><p class='noindent'>
</p>
   <h3 class='likesectionHead'><a id='x1-5000'></a>References</h3>
<!-- l. 1 --><p class='noindent'>
  </p><div class='thebibliography'>
  <p class='bibitem'><span class='biblabel'>
<a id='Xevans-pde-2010'></a><span class='bibsp'>   </span></span>Lawrence C.   Evans.     <span class='p1xi-x-x-109'>Partial   Differential   Equations</span>.     &amp;nbsp;Graduate   Studies   in   Mathematics
  ISBN:    9780821849743.    American    Mathematical    Society.         ISBN    978-0-8218-4974-3.         URL
  <a class='url' href='https://book.douban.com/subject/4767375/'><span class='t1xtt-x-x-109'>https://book.douban.com/subject/4767375/</span></a>.
  </p>
  <p class='bibitem'><span class='biblabel'>
<a id='XSteinbachNumerical2007'></a><span class='bibsp'>   </span></span>Olaf  Steinbach.    <span class='p1xi-x-x-109'>Numerical  Approximation  Methods  for  Elliptic  Boundary  Value  Problems:  Finite  and
  </span><span class='p1xi-x-x-109'>Boundary Elements</span>. Springer Science &amp; Business Media. ISBN 978-0-387-31312-2.
</p>
  </div>
   <div class='footnotes'><a id='x1-4002x3'></a>
<!-- l. 268 --><p class='indent'>      <span class='footnote-mark'><a href='#fn1x0-bk' id='fn1x0'><sup class='textsuperscript'>1</sup></a></span><span class='p1xr-x-x-90'>Note the difference between inner product and duality pairing. The two operands in the former operation are both in </span>\(X\)<span class='p1xr-x-x-90'>, while in
</span><span class='p1xr-x-x-90'>the latter operation, one is in </span>\(X\) <span class='p1xr-x-x-90'>and the other is in </span>\(X'\)<span class='p1xr-x-x-90'>. That’s why </span>\(\left \langle \cdot ,\cdot \right \rangle \) <span class='p1xr-x-x-90'>is called duality pairing.</span></p>                                                                      </div>

<p></p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="PDE" /><category term="BEM" /><summary type="html"><![CDATA[Contents  1 Classical formulation of Green’s identities  2 Representation formula for the Laplace equation: the third Green’s identity  3 General formulation of Green’s identities Green’s identities are indispensable for solving partial differential equations. For example, consider the Poisson’s equation \begin{equation} -\Delta u = f \quad \text {in \(\Omega \)}, \end{equation} with a Dirichlet boundary condition \(\gamma _0 u=g_{\mathrm {D}}\) on \(\Gamma _{\mathrm {D}}\) and Neumann condition \(\gamma _1u = g_{\mathrm {N}}\) on \(\Gamma _{\mathrm {N}}\). We can apply a test function \(v\in H_0^1(\Omega )\) to both sides of the equation: \begin{equation} \int _{\Omega } -v\Delta u \intd x = \int _{\Omega } fv \intd x. \end{equation} Then use the Green’s first identity \begin{equation} \int _{\Omega }v\Delta u \intd x =-\int _{\Omega } (\nabla u)\cdot (\nabla v) \intd x + \int _{\pdiff \Omega } v \frac {\pdiff u}{\pdiff \vect {n}} \intd s, \end{equation} and consider the fact that \(v\) vanishes on \(\Gamma _{\mathrm {D}}\), we have the weak form or variational form of the original equation as \begin{equation} \int _{\Omega } (\nabla u)\cdot (\nabla v) \intd x = -\int _{\Omega }v\Delta u \intd x + \int _{\pdiff \Omega } v \frac {\pdiff u}{\pdiff \vect {n}} \intd s = -\int _{\Omega } fv \intd x + \int _{\Gamma _{\mathrm {N}}} v \frac {\pdiff u}{\pdiff \vect {n}} \intd s. \end{equation} This equation can be directly solved using the FEM. If the BEM is used, the procedure is to first represent the solution \(u\) in the domain \(\Omega \) using an integral form, then take the limit \(x \rightarrow \Gamma \) , i.e. let the target point approach the domain boundary, and a boundary integral equation (BIE) can be obtained using the limiting properties of integral operators. The derivation of the representation formula also involves the second Green’s identity. Therefore, in this essay, I will summarize various forms of Green’s identities. 1 Classical formulation of Green’s identities Theorem 1 (Gauss-Ostrogradski Theorem) Let \(u\in C^1(\bar \Omega )\). Then \begin{equation} \int _{\Omega } \frac {\pdiff u}{\pdiff x_{i}} \intd x = \int _{\pdiff \Omega } \gamma _0^{\rm int}u n_i \intd s \quad i=1,\cdots ,d, \end{equation} where \(n_i\) is the \(i\)-th component of the outward unit normal vector field at \(x\), \(\gamma _0^{\rm int}\) is the Dirichlet trace operator. Proof Assume the \(d\) dimensional domain \(\Omega \) is convex. Then the value range, i.e. lower bound and upper bound, for the \(i\)-th coordinate \(x_i\) can be represented by the other coordinate components as \begin{equation} x_i \in [f_{\mathrm {L}}(x_1,\cdots ,\hat {x_i},\cdots ,x_d), f_{\mathrm {H}}(x_1,\cdots ,\hat {x_i},\cdots ,x_d)]. \end{equation} The two equations \begin{equation} \label {eq:lower-bound-xi-domain-boundary} x_i = f_{\mathrm {L}}(x_1,\cdots ,\hat {x_i},\cdots ,x_d) \end{equation} and \begin{equation} \label {eq:upper-bound-xi-domain-boundary} x_i = f_{\mathrm {H}}(x_1,\cdots ,\hat {x_i},\cdots ,x_d) \end{equation} determine two subsets of the boundary \(\partial \Omega \), which are \(d-1\) dimensional manifolds in \(\mathbb {R}^d\). Using the Fubini’s theorem, the left hand side integration on \(\Omega \) can be split into to two successive parts, one is with respect to the coordinate \(x_i\), the other is with respect to remaining coordinates: \begin{equation} \int _{\Omega } \frac {\pdiff u}{\pdiff x_i} \intd x = \int _U \intd x_1\cdots \intd \hat {x_i}\cdots \intd x_d \int _{f_{\mathrm {L}}(x_1,\cdots ,\hat {x_i},\cdots ,x_d)}^{f_{\mathrm {H}}(x_1,\cdots ,\hat {x_i},\cdots ,x_d)} \frac {\pdiff u}{\pdiff x_i} \intd x_i, \end{equation} where \(U\) is the integration domain with respect to the remaining coordinate variables. Therefore, the left hand side of the Gauss-Ostrogradski identity is equal to \begin{equation} \int _U \left [ u(x_1,\cdots ,f_{\mathrm {H}},\cdots ,x_d) - u(x_1,\cdots ,f_{\mathrm {L}},\cdots ,x_d) \right ] \intd x_1\cdots \intd \hat {x_i}\cdots \intd x_d. \end{equation} For \(n^i\) in the boundary integral on the right hand side, it is the \(i\)-th component of the outward normal vector of \(\partial \Omega \). Because Equation (7) describes the lower domain boundary with respect to the \(i\)-th coordinate (written as \(\partial \Omega _{\mathrm {L}_i}\)) and Equation (8) describes the upper domain boundary \(\partial \Omega _{\mathrm {H}_i}\), \(n^i &lt; 0\) on \(\partial \Omega _{\mathrm {L}_i}\) and \(n^i &gt; 0\) on \(\partial \Omega _{\mathrm {H}_i}\). For remaining parts of \(\partial \Omega \), \(n^i=0\). This is illustrated in the following figure. Furthermore, because the geometric meaning of \(n^{i}\) is the cosine of the angle between the outward normal vector and the \(i\)-th coordinate axis, \(n^i \intd s\) is actually a signed projection of the surface integral element \(\intd s\) to the multi-dimensional coordinate “plane” spanned by \((x_1,\cdots ,\hat {x_i},\cdots ,x_d)\). Therefore, the right hand side is \begin{equation} \begin {aligned} \rhs &amp;= \int _{\partial \Omega _{\mathrm {H}_i}} u(x_1,\cdots ,f_{\mathrm {H}},\cdots ,x_d) n^i \intd s + \int _{\partial \Omega _{\mathrm {L}_i}} u(x_1,\cdots ,f_{\mathrm {L}},\cdots ,x_d) n^i \intd s \\ &amp;= \int _U \left [ u(x_1,\cdots ,f_{\mathrm {H}},\cdots ,x_d) - u(x_1,\cdots ,f_{\mathrm {L}},\cdots ,x_d) \right ] \intd x_1\cdots \intd \hat {x_i}\cdots \intd x_d \end {aligned}. \end{equation} This is just the same as the left hand side. When the domain is not convex, we can split it into several convex subdomains \(\left \{\Omega \right \}_{i=1}^n\) and apply the above conclusion to each \(\Omega _i\). Then we add the identities for all subdomains. Because the boundary integrals cancel on interior interfaces, the sum of these identities is the Gauss-Ostrogradski identity for the global domain \(\Omega \). Theorem 2 (Gauss divergence Theorem) Let \(u\in \left [ C^1(\bar \Omega ) \right ]^d\). Then \begin{equation} \int _{\Omega } \divergence \vect {u} \intd x = \int _{\pdiff \Omega } \gamma _0^{\rm int} \vect {u}\cdot \vect {n} \intd s. \end{equation} Proof Apply the Gauss-Ostrogradski Theorem 1 to each term in \(\divergence \vect {u}\). Theorem 3 (Integration by parts) Let \(u,v\in C^1(\bar \Omega )\). Then \begin{equation} \int _{\Omega }\frac {\pdiff u}{\pdiff x_i} v \intd x + \int _{\Omega } u \frac {\pdiff v}{\pdiff x_i} \intd x = \int _{\pdiff \Omega } \left ( \gamma _0^{\rm int}u \right ) \left ( \gamma _0^{\rm int}v \right ) n_i \intd s. \end{equation} Proof Apply the Gauss-Ostrogradski Theorem 1 to \(uv\). Proposition 1 Let \(u\in C^2(\bar \Omega )\). \begin{equation} \int _{\Omega } \Delta u \intd x = \int _{\pdiff \Omega } \frac {\pdiff u}{\pdiff \vect {n}} \intd s. \end{equation} Proof Apply Gauss divergence Theorem 2 to \(\nabla u\) or replace \(u\) with \(\frac {\pdiff u}{\pdiff x_{i}}\) and \(v\) with \(1\) in Theorem 3. Theorem 4 (Green’s first identity) Let \(u,v\in C^2(\bar \Omega )\). \begin{equation} \label {eq:green-1st-identity} \begin {aligned} \int _{\Omega }v\Delta u \intd x &amp;=-\int _{\Omega } (\nabla u)\cdot (\nabla v) \intd x + \int _{\pdiff \Omega } v \frac {\pdiff u}{\pdiff \vect {n}} \intd s \\ \int _{\Omega }u\Delta v \intd x &amp;=-\int _{\Omega } (\nabla u)\cdot (\nabla v) \intd x + \int _{\pdiff \Omega } u \frac {\pdiff v}{\pdiff \vect {n}} \intd s \end {aligned}. \end{equation} Proof To prove the first equation, replace \(u\) with \(\frac {\pdiff u}{\pdiff x_{i}}\) in Theorem 3, we have \[ \int _{\Omega }\frac {\pdiff ^2 u}{\pdiff x_i^2} v \intd x=\int _{\pdiff \Omega } \frac {\pdiff u}{\pdiff x_i} v n_i \intd s - \int _{\Omega }\frac {\pdiff u}{\pdiff x_i}\frac {\pdiff v}{\pdiff x_{i}} \intd x. \] Then sum up this relation for \(i=1,\cdots ,d\) and the first equation is obtained. The proof of the second equation is similar. Theorem 5 (Green’s second identity) Let \(u,v\in C^2(\bar \Omega )\). \begin{equation} \label {eq:green-2nd-identity} \int _{\Omega } \left ( v\Delta u - u\Delta v \right ) \intd x = \int _{\pdiff \Omega } \left [ v \frac {\pdiff u}{\pdiff \vect {n}} - u \frac {\pdiff v}{\pdiff \vect {n}} \right ] \intd s. \end{equation} Proof Subtract the two equations in Theorem 4, we get the Green’s second identity. 2 Representation formula for the Laplace equation: the third Green’s identity Consider the Green’s second identity (16), we replace \(v\) with the fundamental solution \(U^{\ast }(x,y)=\frac {1}{\lvert x-y \rvert }\) of the Laplace equation and \(u\) with a harmonic function \(\varphi (y)\): \begin{equation} \int _{\Omega } \left ( U^{\ast }(x,y)\Delta _y \varphi - \varphi \Delta _y U^{\ast }(x,y) \right ) \intd y = \int _{\pdiff \Omega } \left [ U^{\ast }(x,y) \frac {\pdiff \varphi }{\pdiff \vect {n}_y} - \varphi \frac {\pdiff U^{\ast }(x,y)}{\pdiff \vect {n}_y} \right ] \intd s_y, \end{equation} where \(x\in \Omega \). Because \(\varphi \) is harmonic, the first term on the left hand side is zero. The singularity in the second integral on the left hand side should be evaluated in the sense of Cauchy principal value, i.e. we remove a ball \(B_{\varepsilon }(x)\) with a radius \(\varepsilon \) around the singularity \(x\) from \(\Omega \), then apply the second Green’s identity in \(\tilde {\Omega }:= \Omega \backslash B_{\varepsilon }(x)\), and finally take the limit \(\varepsilon \rightarrow 0\). The left hand side is \begin{equation} -\int _{\tilde {\Omega }} \varphi \Delta _y U^{\ast }(x,y) \intd y. \end{equation} The fundamental solution \(U^{\ast }(x-y)\) is singular at \(x\) and harmonic elsewhere, so it is always zero in \(\tilde {\Omega }\). When \(\varepsilon \rightarrow 0\), the left hand side is zero. Because the boundary of \(\tilde {\Omega }\) comprises two parts \(\partial \Omega \) and \(\partial B_{\varepsilon }(x)\), the right hand side is \begin{equation} \int _{\partial \Omega } \left [ U^{\ast }(x,y) \frac {\pdiff \varphi }{\pdiff \vect {n}_y} - \varphi \frac {\pdiff U^{\ast }(x,y)}{\pdiff \vect {n}_y} \right ] \intd s_y + \int _{\partial B_{\varepsilon }(x)} \left [ U^{\ast }(x,y) \frac {\pdiff \varphi }{\pdiff \vect {n}_y} - \varphi \frac {\pdiff U^{\ast }(x,y)}{\pdiff \vect {n}_y} \right ] \intd s_y. \end{equation} Because \(x\in \Omega \) and \(y\in \partial \Omega \), the first term has no singularity and we just keep it there. In the second term, the normal vector \(\vect {n}_y\) at \(\partial B_{\varepsilon }(x)\) points inward and is equal to \(\frac {x-y}{\lvert x-y \rvert }\). The normal derivative of the harmonic function is \begin{equation} \frac {\partial \varphi }{\partial \vect {n}_y} = \nabla \varphi \cdot \frac {x-y}{\lvert x-y \rvert }. \end{equation} Then \begin{equation} U^{\ast }(x,y)\frac {\partial \varphi }{\partial \vect {n}_y} = \frac {1}{4\pi \lvert x-y \rvert } \nabla \varphi \cdot \frac {x-y}{\lvert x-y \rvert } = \frac {1}{4\pi \varepsilon } \nabla \varphi \cdot \frac {x-y}{\lvert x-y \rvert }. \end{equation} When \(\varphi \in C^2(\Omega )\), \(\nabla \varphi \) is a continuous vector field. When \(\varepsilon \) is small enough, \(\nabla \varphi \) is nearly a constant vector field through \(B_{\varepsilon }(x)\). Therefore, its flux at antipodal points on \(\partial B_{\varepsilon }(x)\) cancel out. Therefore, when \(\varepsilon \rightarrow 0\), \begin{equation} \int _{\partial B_{\varepsilon }(x)} U^{\ast }(x,y)\frac {\partial \varphi }{\partial \vect {n}_y} \intd s_y = \frac {1}{4\pi \varepsilon } \int _{\partial B_{\varepsilon }(x)} \nabla \varphi \cdot \frac {x-y}{\lvert x-y \rvert } \intd s_y = 0. \end{equation} The normal derivative of the fundamental solution is \begin{equation} \frac {\partial U^{\ast }(x,y)}{\partial \vect {n}_y} = \nabla _y U^{\ast }(x,y) \cdot \vect {n}_y = \nabla _y U^{\ast }(x,y) \cdot \frac {x-y}{\lvert x-y \rvert } = \frac {x-y}{4\pi \lvert x-y \rvert ^3} \frac {x-y}{\lvert x-y \rvert } = \frac {1}{4\pi \lvert x-y \rvert ^2}. \end{equation} Because \(\lvert x-y \rvert = \varepsilon \), \begin{equation} \int _{\partial B_{\varepsilon }(x)} - \varphi (y) \frac {1}{4\pi \varepsilon ^2} \intd s_y. \end{equation} When \(\varepsilon \rightarrow 0\), \(\varphi (y) \rightarrow \varphi (x)\) and the above integral is just \(-\varphi (x)\). Summarizing above results, we have the representation formula \begin{equation} \varphi (x) = \int _{\partial \Omega } \left [ U^{\ast }(x,y) \frac {\pdiff \varphi }{\pdiff \vect {n}_y} - \varphi \frac {\pdiff U^{\ast }(x,y)}{\pdiff \vect {n}_y} \right ] \intd s_y \quad x\in \Omega , \end{equation} which is a combination of single layer potential and double layer potential. Such a representation formula is also called the third Green’s identity. 3 General formulation of Green’s identities Let \(X\) be a Hilbert space with the inner product \(\left \langle \cdot ,\cdot \right \rangle _X\) and the induced norm \(\sqrt {\left \langle \cdot ,\cdot \right \rangle _X}\). Let \(X'\) be the dual space of \(X\) with respect to the duality pairing \(\left \langle \cdot ,\cdot \right \rangle \) 1. Now we take the general self-adjoint second order differential operator \(L: X \rightarrow X'\) as an example. \begin{equation} \label {eq:diff-operator-2nd-order} (Lu)(x):= -\sum _{i,j=1}^d \frac {\pdiff }{\pdiff x_j} \left [ a_{ji}(x)\frac {\pdiff u}{\pdiff x_{i}} \right ] + a_0(x)u(x). \end{equation} Because \(L\) is self-adjoint, \(a_{ji}(x) = a_{ij}(x)\). Test \((Lu)(x)\) with \(v(x)\) and apply integration by parts in Theorem 3, we have \begin{equation} \begin {aligned} \int _{\Omega }(Lu)(x)v(x) \intd x &amp;= \sum _{i,j=1}^d \int _{\Omega } a_{ji}(x)\frac {\pdiff u}{\pdiff x_{i}}\frac {\pdiff v}{\pdiff x_{j}} \intd x + \int _{\Omega } a_0(x)u(x)v(x) \intd x \\ &amp; \quad - \sum _{i,j=1}^d\int _{\pdiff \Omega } \gamma _0^{\rm int}\left ( a_{ji}(x)\frac {\pdiff u}{\pdiff x_{i}} \right ) \gamma _0^{\rm int}v(x) n_j \intd s \\ &amp;=\sum _{i,j=1}^d \int _{\Omega } a_{ji}(x)\frac {\pdiff u}{\pdiff x_{i}}\frac {\pdiff v}{\pdiff x_{j}} \intd x + \int _{\Omega } a_0(x)u(x)v(x) \intd x \\ &amp; \quad -\int _{\pdiff \Omega } \gamma _0^{\rm int} \left [ \sum _{i,j=1}^d n_j a_{ji}(x)\frac {\pdiff u}{\pdiff x_{i}} \right ] \gamma _0^{\rm int}v(x) \intd s. \end {aligned} \end{equation} Define the interior conormal trace: \begin{equation} \label {eq:interior-conormal-trace} \gamma _1^{\rm int}u(x):= \lim _{\Omega \ni \tilde {x} \rightarrow x \in \pdiff \Omega } \left [ \sum _{i,j=1}^d n_j(x)a_{ji}(\tilde {x})\frac {\pdiff u(\tilde {x})}{\pdiff \tilde {x}_i} \right ]. \end{equation} Define the bilinear form \(a(u,v)\): \begin{equation} \label {eq:bilinear-form-2nd-order-pde} a(u,v):= \sum _{i,j=1}^d\int _{\Omega }a_{ji}(x)\frac {\pdiff u}{\pdiff x_{i}}\frac {\pdiff v}{\pdiff x_{j}}\intd x + \int _{\Omega }a_0uv \intd x. \end{equation} Because the original operator \(L\) is self-adjoint, \(a_{ji}(x) = a_{ij}(x)\), the bilinear form \(a(u,v)\) is symmetric. Then we have the general form of the Green’s first identity: \begin{equation} \label {eq:green-1st-identity-general} a(u,v) = \int _{\Omega } (Lu)(x)v(x) \intd x + \int _{\pdiff \Omega } \gamma _1^{\rm int}u(x) \gamma _0^{\rm int}v(x) \intd s. \end{equation} Because \(a(u,v) = a(v,u)\), we also have \begin{equation} a(v,u) = \int _{\Omega } (Lv)(x)u(x) \intd x + \int _{\pdiff \Omega } \gamma _1^{\rm int}v(x) \gamma _0^{\rm int}u(x) \intd s. \end{equation} Subtract it from Equation (30), we obtain the general form of the Green’s second identity: \begin{equation} \label {eq:green-2nd-identity-general} \int _{\Omega } (Lu)(x)v(x) \intd x - \int _{\Omega }(Lv)(x)u(x) \intd x = \int _{\pdiff \Omega }\gamma _1^{\rm int}v(x)\gamma _0^{\rm int}u(x) \intd s - \int _{\pdiff \Omega } \gamma _1^{\rm int}u(x)\gamma _0^{\rm int}v(x) \intd s. \end{equation} With duality pairing, these two Green’s identities can be written in a compact form as below. \begin{equation} \label {eq:green-1st-identity-compact} a(u,v) = \left \langle Lu, v \right \rangle _{\Omega } + \left \langle \gamma _1^{\rm int}u, \gamma _0^{\rm int}v \right \rangle _{\Gamma }. \end{equation} \begin{equation} \label {eq:green-2nd-identity-compact} \left \langle Lu, v \right \rangle _{\Omega } - \left \langle u, Lv \right \rangle _{\Omega } = \left \langle \gamma _1^{\rm int}v, \gamma _0^{\rm int}u \right \rangle _{\Gamma } - \left \langle \gamma _1^{\rm int}u, \gamma _0^{\rm int}v \right \rangle _{\Gamma }. \end{equation} Here we use the subscript \(\Omega \) to indicate volume integral and use the subscript \(\Gamma \) to indicate surface integral. Specifically, when the coefficient \(a_{ji}\) is the Kronecker delta \(\delta _{ji}\) and \(a_0 = 0\), we obtain the negative Laplace operator \(-\Delta \). Hence the bilinear form is \begin{equation} a(u,v) = \int _{\Omega } \sum _{i=1}^d \frac {\partial u}{\partial x_i} \frac {\partial v}{\partial x_j} \intd x = \int _{\Omega } \grad u \cdot \grad v \intd x. \end{equation} The general Green’s first identity becomes \begin{equation} \label {eq:green-1st-identity-general-laplace} \int _{\Omega }\grad u\cdot \grad v \intd x = \int _{\Omega }-v\Delta u \intd x + \int _{\Omega } v \frac {\pdiff u}{\pdiff \vect {n}} \intd s, \end{equation} which is consistent with Equation (15). The general Green’s second identity becomes \begin{equation} \left \langle \Delta u, v \right \rangle _{\Omega } - \left \langle u, \Delta v \right \rangle _{\Omega } = \left \langle \gamma _1^{\rm int}u, \gamma _0^{\rm int}v \right \rangle _{\Gamma } - \left \langle \gamma _1^{\rm int}v, \gamma _0^{\rm int}u \right \rangle _{\Gamma }, \end{equation} which is consistent with Equation (16). References    Lawrence C. Evans. Partial Differential Equations. &amp;nbsp;Graduate Studies in Mathematics ISBN: 9780821849743. American Mathematical Society. ISBN 978-0-8218-4974-3. URL https://book.douban.com/subject/4767375/.    Olaf Steinbach. Numerical Approximation Methods for Elliptic Boundary Value Problems: Finite and Boundary Elements. Springer Science &amp; Business Media. ISBN 978-0-387-31312-2. 1Note the difference between inner product and duality pairing. The two operands in the former operation are both in \(X\), while in the latter operation, one is in \(X\) and the other is in \(X'\). That’s why \(\left \langle \cdot ,\cdot \right \rangle \) is called duality pairing.]]></summary></entry><entry><title type="html">Rumination on first order electrical circuit</title><link href="https://jihuan-tian.github.io/math/2024/08/01/first-order-circuit.html" rel="alternate" type="text/html" title="Rumination on first order electrical circuit" /><published>2024-08-01T00:00:00+08:00</published><updated>2024-08-01T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/08/01/first-order-circuit</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/08/01/first-order-circuit.html"><![CDATA[<h3 class='likesectionHead'><a id='x1-1000'></a>Contents</h3>
   <div class='tableofcontents'>
    <span class='sectionToc'>1 <a href='#x1-20001' id='QQ2-1-2'>General theory about linear operator</a></span>
<br />     <span class='subsectionToc'>1.1 <a href='#x1-30001.1' id='QQ2-1-3'>Definition of linear operator</a></span>
<br />     <span class='subsectionToc'>1.2 <a href='#x1-40001.2' id='QQ2-1-4'>Relationship between kernel and image</a></span>
<br />     <span class='subsectionToc'>1.3 <a href='#x1-50001.3' id='QQ2-1-5'>Structure of the solution set of a linear equation</a></span>
<br />    <span class='sectionToc'>2 <a href='#x1-60002' id='QQ2-1-6'>Solution of a first order linear time invariant (LTI) circuit</a></span>
<br />     <span class='subsectionToc'>2.1 <a href='#x1-70002.1' id='QQ2-1-7'>Definition of steady state</a></span>
<br />     <span class='subsectionToc'>2.2 <a href='#x1-80002.2' id='QQ2-1-8'>Meaning of general solution and particular solution</a></span>
<br />     <span class='subsectionToc'>2.3 <a href='#x1-90002.3' id='QQ2-1-9'>Transient process and switching theorem</a></span>
   </div>
<!-- l. 24 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>1    </span> <a id='x1-20001'></a>General theory about linear operator</h3>
<!-- l. 26 --><p class='noindent'>
</p>
   <h4 class='subsectionHead'><span class='titlemark'>1.1    </span> <a id='x1-30001.1'></a>Definition of linear operator</h4>
<!-- l. 28 --><p class='noindent'>Let \(L\) be a linear operator from a vector space \(V\) to \(W\), i.e. \(L\in \mathcal {L}(V,W)\). For all \(x,y\) in \(V\) and \(\alpha \in \mathbb {R}\), \begin{equation}  \begin {aligned} L(x+y)&amp;=L(x)+L(y) \\ L(\alpha x)&amp;=\alpha L(x) \end {aligned}.  \end{equation}<a id='x1-3001r1'></a> Examples of linear operators:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-3003x1'>
     <!-- l. 37 --><p class='noindent'>Linear operator between Cartesian spaces: \(A: \mathbb {R}^n \rightarrow \mathbb {R}^{m}\), hence \(A\) is a matrix in \(\mathbb {R}^{m\times n}\).
                                                                                               
                                                                                               
     </p></li>
<li class='enumerate' id='x1-3005x2'>
     <!-- l. 38 --><p class='noindent'>Linear operator in a first order ordinary differential equation (ODE): \(D = a_1\frac {\diff }{\diff t}+a_0\).</p></li></ol>
<!-- l. 40 --><p class='noindent'>
</p>
   <h4 class='subsectionHead'><span class='titlemark'>1.2    </span> <a id='x1-40001.2'></a>Relationship between kernel and image</h4>
   <div class='definition'><div class='newtheorem'>
<!-- l. 42 --><p class='noindent'><span class='head'>
<span class='p1xb-x-x-109'>Definition 1 (Kernel and image of a linear operator)</span> </span><a id='x1-4002'></a>Let \(L\in \mathcal {L}(V,W)\). Its kernel is the subspace of \(V\) \begin{equation}  \kernel (L) = \left \{ v\in V \vert Lv = 0 \right \}.  \end{equation}<a id='x1-4003r2'></a> Its image is the
subspace of \(W\) \begin{equation}  \image (L) = \left \{ w\in W \vert \exists v\in V, \suchthat Lv = w \right \}.  \end{equation}<a id='x1-4004r3'></a>
</p>
   </div>
<!-- l. 52 --><p class='indent'>   </p></div> The kernel can be understood as the set of solutions for the homogeneous linear equation \(Lv = 0\).
   <div class='theorem'><div class='newtheorem'>
<!-- l. 55 --><p class='noindent'><span class='head'>
<span class='p1xb-x-x-109'>Theorem 1 (Rank plus nullity theorem)</span> </span><a id='x1-4006'></a><span class='p1xi-x-x-109'>Let </span>\(L\in \mathcal {L}(V,W)\)<span class='p1xi-x-x-109'>. </span>\begin{equation}  \mathrm {dim}(\kernel (L)) + \mathrm {dim}(\image (L)) = \mathrm {dim}(V)  \end{equation}<a id='x1-4007r4'></a>
</p>
   </div>
<!-- l. 61 --><p class='indent'>   </p></div>
   <div class='proof'><div class='newtheorem'>
<!-- l. 63 --><p class='noindent'><span class='head'>
<span class='p1xsc-x-x-109'>P<span class='small-caps'>roof</span></span> </span><a id='x1-4009'></a>The domain of the linear operator \(L\) can be decomposed as a direct sum of the kernel and its orthogonal
complement \begin{equation}  V = \kernel (L) \oplus \kernel (L)^{\perp }.  \end{equation}<a id='x1-4010r5'></a> Therefore \begin{equation}  \mathrm {dim}(V) = \mathrm {dim}(\kernel (L)) + \mathrm {dim}(\kernel (L)^{\perp }).  \end{equation}<a id='x1-4011r6'></a> If we restrict the domain of \(L\) from \(V\) to \(\kernel (L)^{\perp }\) and let \(B = \kernel (L)^{\perp }\), we get a new operator \(\tilde {L}: B \rightarrow \image (L)\). We will show that
this map is bijective.
</p><!-- l. 74 --><p class='indent'>   Because \(\tilde {L}\) is a restriction of \(L\), \(\kernel (\tilde {L})\) is also a restriction of \(\kernel (L)\) to \(B\), i.e. \begin{equation}  \kernel (\tilde {L}) = \kernel (L)\cap B = \kernel (L)\cap \kernel (L)^{\perp } = \left \{ 0 \right \}.  \end{equation}<a id='x1-4012r7'></a> Therefore, \(\tilde {L}\) is injective.
</p><!-- l. 80 --><p class='indent'>   For any \(w\in \image (L)\), there exists \(v\in V\) such that \(Lv = w\), where \(v\) can be decomposed as \(v = v_0 + v_1\) with \(v_0\in \kernel (L)\) and \(v_1\in \kernel (L)^{\perp }\). Hence \(Lv = L(v_0+v_1) = L(v_0) + L(v_1) = L(v_1) = \tilde {L}(v_1) = w\). Therefore, \(v_1\) is the
pre-image of \(w\) under \(\tilde {L}\). \(\tilde {L}\) is surjective. Because it is also injective, \(\tilde {L}\) is bijective and \(\mathrm {dim}(\kernel (L)^{\perp }) = \mathrm {dim}(\image (L))\). Then the theorem is
proved.
</p>
   </div>
<!-- l. 83 --><p class='indent'>   </p></div>
                                                                                               
                                                                                               
   <h4 class='subsectionHead'><span class='titlemark'>1.3    </span> <a id='x1-50001.3'></a>Structure of the solution set of a linear equation</h4>
<!-- l. 86 --><p class='noindent'>The inhomogeneous linear equation is \(Lx = b\), where \(b\in W\) is the given data. To find the set of solutions \(x\in X \subset V\), it
is equivalent to find the domain space of the restricted operator \(\tilde {L}: X \rightarrow \left \{ b \right \}\). According to Theorem <a href='#x1-4007r1'>1<!-- tex4ht:ref: theo:rank-plus-nullity  --></a>, the
dimension of the solution set \(X\) is equal to the dimension of the kernel of \(\tilde {L}\). Because \(\tilde {L}\) represents the same
operation as \(L\), it is also the kernel of \(L\). This kernel is just the solution set of the homogeneous equation
\(Lx = 0\).
</p><!-- l. 88 --><p class='indent'>   Let \(x_\mathrm {p}\) be a particular solution of \(Lx = b\), i.e. \(Lx_\mathrm {p} = b\). Then the solution space is \begin{equation}  X = \kernel (L) + x_\mathrm {p}.  \end{equation}<a id='x1-5001r8'></a> For any \(x_\mathrm {h}\in \kernel (L)\), it is called a general solution of the
homogeneous equation \(Lx = 0\). Therefore, a specific solution of the inhomogeneous linear equation is the sum of a
general solution and a particular solution. Due to the linearity of the operator \(L\), it is obvious that such a
decomposed formulation is really the solution of \(Lx=b\): \begin{equation}  L(x_{\mathrm {h}} + x_{\mathrm {p}}) = Lx_{\mathrm {h}} + Lx_{\mathrm {p}} = 0 + b = b.  \end{equation}<a id='x1-5002r9'></a> Which one of \(x_{\mathrm {h}}\) should be selected from \(\kernel (L)\) depends on additional
constraints.
</p>
   <h3 class='sectionHead'><span class='titlemark'>2    </span> <a id='x1-60002'></a>Solution of a first order linear time invariant (LTI) circuit</h3>
<!-- l. 99 --><p class='noindent'>The equation for a first order LTI circuit is just a specific example of the above general linear equation \(Lx=b\), where \(L= D = \frac {\diff }{\diff t} - \lambda \).
Here we write the original differential operator \(a_1\frac {\diff }{\diff t} + a_0\) as this equivalent form \(\frac {\diff }{\diff t}-\lambda \) is because we want to cast the
homogeneous equation into an eigen equation form.
</p>
   <h4 class='subsectionHead'><span class='titlemark'>2.1    </span> <a id='x1-70002.1'></a>Definition of steady state</h4>
<!-- l. 102 --><p class='noindent'>If a device or a circuit is operating under a steady state, according to the semantics of <span class='p1xb-x-x-109'>steady</span>, the state of the
device or circuit is either time invariant or periodic. So we have two situations:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-7002x1'>
     <!-- l. 104 --><p class='noindent'>The state of the circuit is a constant function, which can be handled by the switching theorem and
     three-factor theorem.
     </p></li>
<li class='enumerate' id='x1-7004x2'>
     <!-- l. 105 --><p class='noindent'>The state of the circuit is a periodic function.
          </p><ol class='enumerate2'>
<li class='enumerate' id='x1-7006x1'>
          <!-- l. 107 --><p class='noindent'>If the excitation is a special periodic function, i.e. sinusoidal, phasor method will be used.
          </p></li>
<li class='enumerate' id='x1-7008x2'>
                                                                                               
                                                                                               
          <!-- l. 108 --><p class='noindent'>If the excitation is an arbitrary periodic function, Fourier series expansion will be applied to
          the excitation and phasor method will be adopted for each independent frequency. Based on
          the principle of superposition for LTI circuit, the total response is a linear combination of the
          responses at all frequencies with the same expansion coefficients in the former Fourier series.
          Therefore, the result usually takes the form of an infinite sum.</p></li></ol>
     </li></ol>
<!-- l. 112 --><p class='indent'>   In the following, we only consider the first case of steady state.
</p>
   <h4 class='subsectionHead'><span class='titlemark'>2.2    </span> <a id='x1-80002.2'></a>Meaning of general solution and particular solution</h4>
<!-- l. 115 --><p class='noindent'>According to the linear operator theory, the solution set \(X\) of a first order LTI circuit equation \(Dx=b\) should have the
form \begin{equation}  X = \kernel (D) + x_{\mathrm {p}}(t),  \end{equation}<a id='x1-8001r10'></a> where the particular solution \(x_{\mathrm {p}}(t)\) represents the steady state of the circuit. In reality, the circuit has fixed
parameters and a certain initial state \(x(0)\). It is this initial state \(x(0)\) that the solution \(x(t)\) can be uniquely found in the set
\(X\).
</p><!-- l. 121 --><p class='indent'>   The kernel of \(D\) is the just the solution set of the homogeneous equation \begin{equation}  \frac {\diff x_{\mathrm {h}}(t)}{\diff t} - \lambda x_{\mathrm {h}}(t) = 0.  \end{equation}<a id='x1-8002r11'></a> This is
just the eigen equation for the differential operator \(\frac {\diff }{\diff t}\): \begin{equation}  \frac {\diff }{\diff t} x_{\mathrm {h}}(t) = \lambda x_h(t).  \end{equation}<a id='x1-8003r12'></a> Because the eigen function
<span class='footnote-mark'><a href='#fn1x0' id='fn1x0-bk'><sup class='textsuperscript'>1</sup></a></span><a id='x1-8004f1'></a> of \(\frac {\diff }{\diff t}\) is \(\exp (\lambda t)\), the 1-dimensional
eigen space <span class='footnote-mark'><a href='#fn2x0' id='fn2x0-bk'><sup class='textsuperscript'>2</sup></a></span><a id='x1-8006f2'></a>
associated with the eigen value \(\lambda \) is \begin{equation}  C \exp (\lambda t),  \end{equation}<a id='x1-8008r13'></a> where \(C\) is an unknown constant. This eigen space is just \(\kernel (D)\).
</p><!-- l. 135 --><p class='indent'>   Therefore, the general form of the solution for the inhomogeneous equation \(Dx = b\) is \begin{equation}  x(t) = x_{\mathrm {h}}(t) + x_{\mathrm {p}}(t) = C\exp (\lambda t) + x_{\mathrm {p}}(t).  \end{equation}<a id='x1-8009r14'></a> The constant \(C\) can be
determined from the initial condition \(x(0)\) of the circuit by solving the equation below \begin{equation}  x(0) = C\exp (\lambda 0) + x_{\mathrm {p}}(0) = C + x_{\mathrm {p}}(0).  \end{equation}<a id='x1-8010r15'></a> Since we only consider the
first case of steady state, \(x_{\mathrm {p}}(t)\) is a constant \(x_{\mathrm {p}}\). Therefore \(C = x(0) - x_{\mathrm {p}}\). In classical electrical circuit book, \(x_{\mathrm {p}}\) is written as
\(x_{\infty }\).
</p>
   <h4 class='subsectionHead'><span class='titlemark'>2.3    </span> <a id='x1-90002.3'></a>Transient process and switching theorem</h4>
<!-- l. 146 --><p class='noindent'>We still consider the first case of steady state. Therefore, circuit switching means when the topology of the
circuit changes, its state changes from the first steady state \(x_{\mathrm {p}_1}\) to the second steady state \(x_{\mathrm {p}_2}\). The transient
process in-between is described by the solution \(x(t)\) under the initial condition \(x(0)\). And the switching
theorem is a method to compute this initial condition. The basic idea is energy cannot suddenly
change.
</p>
   <div class='footnotes'><a id='x1-8005x2.2'></a>
<!-- l. 129 --><p class='indent'>      <span class='footnote-mark'><a href='#fn1x0-bk' id='fn1x0'><sup class='textsuperscript'>1</sup></a></span><span class='p1xr-x-x-90'>The said eigen function corresponds to eigen vector in linear algebra.</span></p><a id='x1-8007x2.2'></a>
<!-- l. 129 --><p class='indent'>        <span class='footnote-mark'><a href='#fn2x0-bk' id='fn2x0'><sup class='textsuperscript'>2</sup></a></span><span class='p1xr-x-x-90'>This is a space of function. Compare with the eigen space of vectors in linear algebra.</span></p>                                                        </div>

<p></p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="linear-algebra" /><category term="electrical-circuit" /><summary type="html"><![CDATA[Contents  1 General theory about linear operator   1.1 Definition of linear operator   1.2 Relationship between kernel and image   1.3 Structure of the solution set of a linear equation  2 Solution of a first order linear time invariant (LTI) circuit   2.1 Definition of steady state   2.2 Meaning of general solution and particular solution   2.3 Transient process and switching theorem 1 General theory about linear operator 1.1 Definition of linear operator Let \(L\) be a linear operator from a vector space \(V\) to \(W\), i.e. \(L\in \mathcal {L}(V,W)\). For all \(x,y\) in \(V\) and \(\alpha \in \mathbb {R}\), \begin{equation} \begin {aligned} L(x+y)&amp;=L(x)+L(y) \\ L(\alpha x)&amp;=\alpha L(x) \end {aligned}. \end{equation} Examples of linear operators: Linear operator between Cartesian spaces: \(A: \mathbb {R}^n \rightarrow \mathbb {R}^{m}\), hence \(A\) is a matrix in \(\mathbb {R}^{m\times n}\). Linear operator in a first order ordinary differential equation (ODE): \(D = a_1\frac {\diff }{\diff t}+a_0\). 1.2 Relationship between kernel and image Definition 1 (Kernel and image of a linear operator) Let \(L\in \mathcal {L}(V,W)\). Its kernel is the subspace of \(V\) \begin{equation} \kernel (L) = \left \{ v\in V \vert Lv = 0 \right \}. \end{equation} Its image is the subspace of \(W\) \begin{equation} \image (L) = \left \{ w\in W \vert \exists v\in V, \suchthat Lv = w \right \}. \end{equation} The kernel can be understood as the set of solutions for the homogeneous linear equation \(Lv = 0\). Theorem 1 (Rank plus nullity theorem) Let \(L\in \mathcal {L}(V,W)\). \begin{equation} \mathrm {dim}(\kernel (L)) + \mathrm {dim}(\image (L)) = \mathrm {dim}(V) \end{equation} Proof The domain of the linear operator \(L\) can be decomposed as a direct sum of the kernel and its orthogonal complement \begin{equation} V = \kernel (L) \oplus \kernel (L)^{\perp }. \end{equation} Therefore \begin{equation} \mathrm {dim}(V) = \mathrm {dim}(\kernel (L)) + \mathrm {dim}(\kernel (L)^{\perp }). \end{equation} If we restrict the domain of \(L\) from \(V\) to \(\kernel (L)^{\perp }\) and let \(B = \kernel (L)^{\perp }\), we get a new operator \(\tilde {L}: B \rightarrow \image (L)\). We will show that this map is bijective. Because \(\tilde {L}\) is a restriction of \(L\), \(\kernel (\tilde {L})\) is also a restriction of \(\kernel (L)\) to \(B\), i.e. \begin{equation} \kernel (\tilde {L}) = \kernel (L)\cap B = \kernel (L)\cap \kernel (L)^{\perp } = \left \{ 0 \right \}. \end{equation} Therefore, \(\tilde {L}\) is injective. For any \(w\in \image (L)\), there exists \(v\in V\) such that \(Lv = w\), where \(v\) can be decomposed as \(v = v_0 + v_1\) with \(v_0\in \kernel (L)\) and \(v_1\in \kernel (L)^{\perp }\). Hence \(Lv = L(v_0+v_1) = L(v_0) + L(v_1) = L(v_1) = \tilde {L}(v_1) = w\). Therefore, \(v_1\) is the pre-image of \(w\) under \(\tilde {L}\). \(\tilde {L}\) is surjective. Because it is also injective, \(\tilde {L}\) is bijective and \(\mathrm {dim}(\kernel (L)^{\perp }) = \mathrm {dim}(\image (L))\). Then the theorem is proved. 1.3 Structure of the solution set of a linear equation The inhomogeneous linear equation is \(Lx = b\), where \(b\in W\) is the given data. To find the set of solutions \(x\in X \subset V\), it is equivalent to find the domain space of the restricted operator \(\tilde {L}: X \rightarrow \left \{ b \right \}\). According to Theorem 1, the dimension of the solution set \(X\) is equal to the dimension of the kernel of \(\tilde {L}\). Because \(\tilde {L}\) represents the same operation as \(L\), it is also the kernel of \(L\). This kernel is just the solution set of the homogeneous equation \(Lx = 0\). Let \(x_\mathrm {p}\) be a particular solution of \(Lx = b\), i.e. \(Lx_\mathrm {p} = b\). Then the solution space is \begin{equation} X = \kernel (L) + x_\mathrm {p}. \end{equation} For any \(x_\mathrm {h}\in \kernel (L)\), it is called a general solution of the homogeneous equation \(Lx = 0\). Therefore, a specific solution of the inhomogeneous linear equation is the sum of a general solution and a particular solution. Due to the linearity of the operator \(L\), it is obvious that such a decomposed formulation is really the solution of \(Lx=b\): \begin{equation} L(x_{\mathrm {h}} + x_{\mathrm {p}}) = Lx_{\mathrm {h}} + Lx_{\mathrm {p}} = 0 + b = b. \end{equation} Which one of \(x_{\mathrm {h}}\) should be selected from \(\kernel (L)\) depends on additional constraints. 2 Solution of a first order linear time invariant (LTI) circuit The equation for a first order LTI circuit is just a specific example of the above general linear equation \(Lx=b\), where \(L= D = \frac {\diff }{\diff t} - \lambda \). Here we write the original differential operator \(a_1\frac {\diff }{\diff t} + a_0\) as this equivalent form \(\frac {\diff }{\diff t}-\lambda \) is because we want to cast the homogeneous equation into an eigen equation form. 2.1 Definition of steady state If a device or a circuit is operating under a steady state, according to the semantics of steady, the state of the device or circuit is either time invariant or periodic. So we have two situations: The state of the circuit is a constant function, which can be handled by the switching theorem and three-factor theorem. The state of the circuit is a periodic function. If the excitation is a special periodic function, i.e. sinusoidal, phasor method will be used. If the excitation is an arbitrary periodic function, Fourier series expansion will be applied to the excitation and phasor method will be adopted for each independent frequency. Based on the principle of superposition for LTI circuit, the total response is a linear combination of the responses at all frequencies with the same expansion coefficients in the former Fourier series. Therefore, the result usually takes the form of an infinite sum. In the following, we only consider the first case of steady state. 2.2 Meaning of general solution and particular solution According to the linear operator theory, the solution set \(X\) of a first order LTI circuit equation \(Dx=b\) should have the form \begin{equation} X = \kernel (D) + x_{\mathrm {p}}(t), \end{equation} where the particular solution \(x_{\mathrm {p}}(t)\) represents the steady state of the circuit. In reality, the circuit has fixed parameters and a certain initial state \(x(0)\). It is this initial state \(x(0)\) that the solution \(x(t)\) can be uniquely found in the set \(X\). The kernel of \(D\) is the just the solution set of the homogeneous equation \begin{equation} \frac {\diff x_{\mathrm {h}}(t)}{\diff t} - \lambda x_{\mathrm {h}}(t) = 0. \end{equation} This is just the eigen equation for the differential operator \(\frac {\diff }{\diff t}\): \begin{equation} \frac {\diff }{\diff t} x_{\mathrm {h}}(t) = \lambda x_h(t). \end{equation} Because the eigen function 1 of \(\frac {\diff }{\diff t}\) is \(\exp (\lambda t)\), the 1-dimensional eigen space 2 associated with the eigen value \(\lambda \) is \begin{equation} C \exp (\lambda t), \end{equation} where \(C\) is an unknown constant. This eigen space is just \(\kernel (D)\). Therefore, the general form of the solution for the inhomogeneous equation \(Dx = b\) is \begin{equation} x(t) = x_{\mathrm {h}}(t) + x_{\mathrm {p}}(t) = C\exp (\lambda t) + x_{\mathrm {p}}(t). \end{equation} The constant \(C\) can be determined from the initial condition \(x(0)\) of the circuit by solving the equation below \begin{equation} x(0) = C\exp (\lambda 0) + x_{\mathrm {p}}(0) = C + x_{\mathrm {p}}(0). \end{equation} Since we only consider the first case of steady state, \(x_{\mathrm {p}}(t)\) is a constant \(x_{\mathrm {p}}\). Therefore \(C = x(0) - x_{\mathrm {p}}\). In classical electrical circuit book, \(x_{\mathrm {p}}\) is written as \(x_{\infty }\). 2.3 Transient process and switching theorem We still consider the first case of steady state. Therefore, circuit switching means when the topology of the circuit changes, its state changes from the first steady state \(x_{\mathrm {p}_1}\) to the second steady state \(x_{\mathrm {p}_2}\). The transient process in-between is described by the solution \(x(t)\) under the initial condition \(x(0)\). And the switching theorem is a method to compute this initial condition. The basic idea is energy cannot suddenly change. 1The said eigen function corresponds to eigen vector in linear algebra. 2This is a space of function. Compare with the eigen space of vectors in linear algebra.]]></summary></entry><entry><title type="html">Understanding about operator discretization</title><link href="https://jihuan-tian.github.io/math/2024/07/24/understanding-about-operator-discretization.html" rel="alternate" type="text/html" title="Understanding about operator discretization" /><published>2024-07-24T00:00:00+08:00</published><updated>2024-07-24T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/07/24/understanding-about-operator-discretization</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/07/24/understanding-about-operator-discretization.html"><![CDATA[<h3 class='likesectionHead'><a id='x1-1000'></a>Contents</h3>
   <div class='tableofcontents'>
    <span class='sectionToc'>1 <a href='#x1-20001' id='QQ2-1-2'>Vectors in strong form and weak form</a></span>
<br />    <span class='sectionToc'>2 <a href='#x1-30002' id='QQ2-1-3'>Discretization of the inverse of an operator</a></span>
<br />    <span class='sectionToc'>3 <a href='#x1-40003' id='QQ2-1-4'>Discretization of a preconditioner</a></span>
<br />    <span class='sectionToc'>4 <a href='#x1-50004' id='QQ2-1-5'>Summary</a></span>
   </div>
<!-- l. 24 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>1    </span> <a id='x1-20001'></a>Vectors in strong form and weak form</h3>
<!-- l. 26 --><p class='noindent'>Let \(V\) and \(W\) be two Hilbert spaces. Let \(V_h\) be the finite dimensional subspace of \(V\) with the basis \(\left \{ \varphi _{1},\cdots ,\varphi _{n} \right \}\) and \(W_h\) be the finite
dimensional subspace of \(W\) with the basis \(\left \{ \zeta _{1},\cdots ,\zeta _{p} \right \}\). Let \(W'\) be the dual space of \(W\) and \(W_h'\) be its finite dimensional
subspace with the basis \(\left \{ \psi _{1},\cdots ,\psi _{m} \right \}\). Let \(A: V \rightarrow W\) be a bounded linear operator from \(V\) to \(W\) and \(A_h: V_h \rightarrow W_h\) be its finite dimensional
approximation.
</p><!-- l. 28 --><p class='indent'>   A function \(u_h\in V_h\) can be expanded as \(u_h=\sum _{i=1}^n u_i \varphi _i\). Its expansion coefficients make up the <span class='p1xb-x-x-109'>strong </span>form vector \(\underline {u} = (u_1,\cdots ,u_n)^{\mathrm {T}}\) of \(u_h\). Here we
explicitly write this vector as \(\underline {u}^{\mathrm {S}}\) to be differentiated from the weak form.
</p><!-- l. 30 --><p class='indent'>   To discretize the operator \(A_h\), we need two steps:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-2002x1'>
     <!-- l. 32 --><p class='noindent'>Apply \(A_h\) to \(u_h\), which produces a function \(w_h\) in \(W_h\).
     </p></li>
<li class='enumerate' id='x1-2004x2'>
     <!-- l. 33 --><p class='noindent'>Project \(w_h\) to each basis function \(\psi _i\) of \(W_h'\).</p></li></ol>
                                                                                               
                                                                                               
<!-- l. 36 --><p class='indent'>   The  said  projection  is  formed  by  duality  pairing  \(\left \langle \cdot ,\cdot \right \rangle _{W_h',W_h}\)
<span class='footnote-mark'><a href='#fn1x0' id='fn1x0-bk'><sup class='textsuperscript'>1</sup></a></span><a id='x1-2005f1'></a> and
will produce a vector \(\underline {w}^{\mathrm {W}}\), which is the <span class='p1xb-x-x-109'>weak </span>form of \(w_h\). This vector is <span class='p1xb-x-x-109'>not </span>related to the expansion coefficients \(w_k\) of \(w_h\) in \(W_h\),
i.e. \(w_h = \sum _{k=1}^p w_k\zeta _k\), but is the projection coefficients of \(w_h\) in the dual space \(W_h'\).
</p><!-- l. 38 --><p class='indent'>   The expression for \(\underline {w}^{\mathrm {W}}\) can be expanded as \begin{equation}  \underline {w}^{\mathrm {W}} = \begin {pmatrix} \left \langle \psi _1, w_h \right \rangle \\ \vdots \\ \left \langle \psi _m, w_h \right \rangle \end {pmatrix} = \begin {pmatrix} \left \langle \psi _1,A_hu_h \right \rangle \\ \vdots \\ \left \langle \psi _m,A_hu_h \right \rangle \end {pmatrix} = \begin {pmatrix} \left \langle \psi _1,A_h\sum _{j=1}^n u_j\varphi _j \right \rangle \\ \vdots \\ \left \langle \psi _m,A_h\sum _{j=1}^n u_j\varphi _j \right \rangle \end {pmatrix} = \begin {pmatrix} \sum _{j=1}^n \left \langle \psi _1, A_h \varphi _j \right \rangle u_j \\ \vdots \\ \sum _{j=1}^n \left \langle \psi _m, A_h \varphi _j \right \rangle u_j \end {pmatrix}.  \end{equation}<a id='x1-2007r1'></a> Its matrix form is \begin{equation}  \underline {w}^{\mathrm {W}} = \mathcal {A} \underline {u}^{\mathrm {S}},  \end{equation}<a id='x1-2008r2'></a> where \(\mathcal {A}_{ij} = \left \langle \psi _i, A_h\varphi _j \right \rangle \). If we substitute \(w_h = \sum _{k=1}^p w_k\zeta _k\) into the left hand
side of this equation, we have \begin{equation}  \underline {w}^{\mathrm {W}} = \begin {pmatrix} \left \langle \psi _1, \sum _{k=1}^p w_i\zeta _k \right \rangle \\ \vdots \\ \left \langle \psi _m, \sum _{k=1}^p w_i\zeta _k \right \rangle \end {pmatrix} = \begin {pmatrix} \sum _{k=1}^p \left \langle \psi _1, \zeta _k \right \rangle w_k \\ \vdots \\ \sum _{k=1}^p \left \langle \psi _m, \zeta _k \right \rangle w_k \\ \end {pmatrix} = \begin {pmatrix} \sum _{j=1}^n \left \langle \psi _1, A_h \varphi _j \right \rangle u_j \\ \vdots \\ \sum _{j=1}^n \left \langle \psi _m, A_h \varphi _j \right \rangle u_j \end {pmatrix}.  \end{equation}<a id='x1-2009r3'></a> Its matrix form is \begin{equation}  \underline {w}^{\mathrm {W}} = \mathcal {M}_{W_h',W_h} \underline {w}^{\mathrm {S}} = \mathcal {A} \underline {u}^{\mathrm {S}}.  \end{equation}<a id='x1-2010r4'></a> In this equation, \(\mathcal {M}_{W_h',W_h}\) is the mass matrix associated
with the duality pairing between the dual space \(W_h'\) and the range space \(W_h\) with respect to the operator \(A\)
<span class='footnote-mark'><a href='#fn2x0' id='fn2x0-bk'><sup class='textsuperscript'>2</sup></a></span><a id='x1-2011f2'></a>. For
simplicity, this matrix is written as \(\mathcal {M}_A\).
</p><!-- l. 90 --><p class='indent'>   The vector \(\underline {w}^{\mathrm {S}}\) contains the actual expansion coefficients \((w_{1},\cdots ,w_{p})^{\mathrm {T}}\) of \(w_h\) in \(W_h\), therefore it is the <span class='p1xb-x-x-109'>strong form </span>of \(w_h\), which can be
derived from the weak form of \(w_h\) as below: \begin{equation}  \underline {w}^{\mathrm {S}} = \mathcal {M}_A^{-1} \underline {w}^{\mathrm {W}} = \mathcal {M}_A^{-1}\mathcal {A}\underline {u}^{\mathrm {S}}.  \end{equation}<a id='x1-2013r5'></a> This means the mass matrix \(\mathcal {M}_A\) should be a square matrix and invertible,
which requires two conditions:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-2015x1'>
     <!-- l. 96 --><p class='noindent'>The number of basis elements adopted for the range space \(W_h\) of \(A\) should be the same as that for the
     dual space \(W_h'\).
     </p><!-- l. 98 --><p class='noindent'>Take the single layer potential (SLP) operator \(V: H^{-1/2}(\Gamma ) \rightarrow H^{1/2}(\Gamma )\) in BEM as an example. \(W\) is \(H^{1/2}(\Gamma )\) and \(W'\) is \(H^{-1/2}(\Gamma )\). \(W_h\) is usually the
     piecewise linear continuous function space and \(W_h'\) is the piecewise constant function space. Basis
     functions of the former space are configured at nodes in the mesh, while basis functions for the
     latter  space  are  configured  in  cells.  Therefore,  the  number  of  basis  functions  of  the  two  spaces
     usually mismatch. The remedy is to construct the space \(W_h'\) on the dual mesh, then there is a one-to-one
     correspondence between each cell in the dual mesh and each node in the primal mesh.
     </p></li>
<li class='enumerate' id='x1-2017x2'>
     <!-- l. 100 --><p class='noindent'>The duality pairing between \(W_h'\) and \(W_h\) should be \(\inf \sup \) stable in the sense that the following condition should be
     satisfied (<a href='#XBetckeProduct2017'>Betcke et al.</a>) \begin{equation}  \inf _{u\in W_h} \sup _{v\in W_h'} \frac {\lvert \left \langle u,v \right \rangle \rvert }{\lVert u \rVert _{W_h} \lVert v \rVert _{W_h'}} \geq c &gt; 0.  \end{equation}<a id='x1-2018r6'></a></p></li></ol>
<!-- l. 105 --><p class='noindent'>Then the mass matrix \(\mathcal {M}_A\) is invertible.
</p><!-- l. 107 --><p class='indent'>   This \(\inf \sup \) stability is just the unique solvability condition or well-posedness condition of the following variational
formulation: given \(f\in V'\), find \(u\in U\), such that \begin{equation}  a(u,v) = \left \langle f, v \right \rangle \quad \forall v\in V.  \end{equation}<a id='x1-2019r7'></a> Such a variational formulation involves the generalized bilinear form \(a(\cdot ,\cdot ): U\times V \rightarrow \mathbb {R}\),
where the two component spaces \(U\) and \(V\) are different. Therefore, this condition is a generalization of the ellipticity
condition involved in the Lax-Milgram Lemma, where the bilinear form requires two identical
spaces.
</p><!-- l. 113 --><p class='indent'>   For the duality pairing \(\left \langle \cdot ,\cdot \right \rangle _{W_h', W_h}\), it is actually a bilinear form \(a(\cdot ,\cdot ): W_h\times W_h' \rightarrow \mathbb {R}\) defined as \(a(u,v) := \left \langle v, Iu \right \rangle \), where \(I: W_h \rightarrow W_h\) is the identity operator. If the \(\inf \sup \)
condition is satisfied, the mass matrix, as a discretization of the operator \(I\), is invertible.
</p><!-- l. 115 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>2    </span> <a id='x1-30002'></a>Discretization of the inverse of an operator</h3>
<!-- l. 117 --><p class='noindent'>In BEM, when there is source distribution \(f\in \tilde {H}^{-1}(\Omega )\) in the volume domain \(\Omega \), the Newton potential \(N_0 f\) will also contribute to
                                                                                               
                                                                                               
the Neumann trace at the boundary \(\Gamma \) as \(V^{-1}(N_0 f)\), where \(N_0: \tilde {H}^{-1}(\Omega ) \rightarrow H^{1/2}(\Gamma )\) and \(V^{-1}: H^{1/2}(\Gamma ) \rightarrow H^{-1/2}(\Gamma )\). It is obvious that if somehow we can directly discretize the
inverse SLP operator \(V^{-1}\) into a matrix \(\mathcal {V}^{\dagger }\), its row and column function spaces are \(^{\mathrm {W}}H_h^{1/2}(\Gamma )\times ^{\mathrm {S}}H_h^{1/2}(\Gamma )\). On the other hand, if the matrix for \(V\)
is \(\mathcal {V}\) and \(\mathcal {V}^{-1}\) is its inverse matrix, the row and column function spaces of \(\mathcal {V}^{-1}\) are \(^{\mathrm {S}}H_h^{-1/2}(\Gamma )\times ^{\mathrm {W}}H_h^{-1/2}(\Gamma )\). Therefore, \(\mathcal {V}^{\dagger }\) and \(\mathcal {V}^{-1}\) are two
different matrices. <span class='p1xb-x-x-109'>N.B. Even though the function spaces related to matrices </span>\(\mathcal {V}\) <span class='p1xb-x-x-109'>and </span>\(\mathcal {V}^{-1}\) <span class='p1xb-x-x-109'>are the same </span>\(H_h^{-1/2}(\Gamma )\times H_h^{-1/2}(\Gamma )\)<span class='p1xb-x-x-109'>, the
</span><span class='p1xb-x-x-109'>input or output vectors of the two matrices have different identities in the sense of strong or
</span><span class='p1xb-x-x-109'>weak.</span>
</p><!-- l. 119 --><p class='indent'>   In practice, an inverse operator like \(V^{-1}\) cannot be directly evaluated or discretized. We can only compute an
approximation \(\tilde {\mathcal {V}^{\dagger }}\) of \(\mathcal {V}^{\dagger }\) by starting from the inverse of the matrix for the original operator, which is \(\mathcal {V}^{-1}\). In \(V^{-1}(N_0f)\), \(N_0f\) directly
returns a strong form vector in \(H_h^{1/2}(\Gamma )\), which is the column space of the matrix \(\tilde {\mathcal {V}^{\dagger }}\). Hence we need to project this
strong form vector from \(H_h^{1/2}(\Gamma )\) to the <span class='p1xb-x-x-109'>weak </span>column space \(H_h^{-1/2}(\Gamma )\) of \(\mathcal {V}^{-1}\), and we can call this operation as <span class='p1xb-x-x-109'>space
</span><span class='p1xb-x-x-109'>adaptation</span>. This can be directly achieved by multiplying the mass matrix \(\mathcal {M}_{V}\) induced by the duality pairing
between \(H_h^{-1/2}(\Gamma )\) and \(H_h^{1/2}(\Gamma )\). The output from \(\mathcal {V}^{-1}\) is a strong form vector in \(H_h^{-1/2}(\Gamma )\), which should be further mapped to
the <span class='p1xb-x-x-109'>weak </span>row space \(H_h^{1/2}(\Gamma )\) of \(\mathcal {V}^{\dagger }\). Because this is also a direct projection of a strong form vector to a weak
form vector, we need to multiply the mass matrix \(\mathcal {M}_{V^{-1}}\) induced by the duality pairing between \(H_h^{1/2}(\Gamma )\) and
\(H_h^{-1/2}(\Gamma )\).
</p><!-- l. 121 --><p class='indent'>   Let the basis of \(H_h^{1/2}(\Gamma )\) be \(\left \{ \varphi _{1},\cdots ,\varphi _{n} \right \}\) and the basis of \(H_h^{-1/2}(\Gamma )\) be \(\left \{ \psi _{1},\cdots ,\psi _{m} \right \}\). The coefficients of the above two mass matrices are \begin{equation}  \mathcal {M}_V[i,j] = \left \langle \psi _i,\varphi _j \right \rangle \quad i=1,\cdots ,m, j=1,\cdots ,n  \end{equation}<a id='x1-3001r8'></a> and \begin{equation}  \mathcal {M}_{V^{-1}}[i,j] = \left \langle \varphi _i,\psi _j \right \rangle \quad i=1,\cdots ,n, j=1,\cdots ,m.  \end{equation}<a id='x1-3002r9'></a> Therefore, \(\mathcal {M}_V = \mathcal {M}_{V^{-1}}^{\mathrm {T}}\).
The matrix \(\tilde {\mathcal {V}}^{\dagger }\) as an approximate discretization of the inverse operator \(V^{-1}\) is \begin{equation}  \tilde {\mathcal {V}^{\dagger }} = \mathcal {M}_V^{\mathrm {T}} \mathcal {V}^{-1} \mathcal {M}_V.  \end{equation}<a id='x1-3003r10'></a> The commutative diagram for the above
operators and matrices is shown below, where the red route indicates the computation of \(\tilde {\mathcal {V}^{\dagger }}\) as the approximation
for \(\mathcal {V}^{\dagger }\).
</p>
   <figure class='figure'> 

                                                                                               
                                                                                               
                                                                                               
                                                                                               
<!-- l. 137 --><p class='noindent'><img alt='PIC' src='/figures/2024-07-24_12-30-43-commutative-diagram-for-inverse-operator.png'/>
                                                                                               
                                                                                               
</p>
   </figure>
<!-- l. 140 --><p class='indent'>   Another example is the discretization of the Steklov-Poincaré operator in the symmetric form for the
interior problem: \begin{equation}  S = D + \left ( \frac {1}{2}I_1 + K' \right ) V^{-1} \left ( \frac {1}{2}I_2 + K \right ),  \end{equation}<a id='x1-3004r11'></a> where the mapping properties of involved operators are \begin{equation}  \begin {aligned} S, D &amp;: H^{1/2}(\Gamma ) \rightarrow H^{-1/2}(\Gamma ) \\ I_1, K' &amp;: H^{-1/2}(\Gamma ) \rightarrow H^{-1/2}(\Gamma ) \\ V^{-1} &amp;: H^{1/2}(\Gamma ) \rightarrow H^{-1/2}(\Gamma ) \\ I_2, K &amp;: H^{1/2}(\Gamma ) \rightarrow H^{1/2}(\Gamma ) \end {aligned}.  \end{equation}<a id='x1-3005r12'></a> Let \(\mathcal {M}\) be the mass matrix between \(H^{-1/2}(\Gamma )\)
and \(H^{1/2}(\Gamma )\), we have the matrix form for \(S\) as \begin{equation}  \mathcal {S} = \mathcal {D} + \left ( \frac {1}{2}\mathcal {M}^{\mathrm {T}} + \mathcal {K}^{\mathrm {T}} \right ) \mathcal {M}^{-\mathrm {T}} \mathcal {M}^{\mathrm {T}} \mathcal {V}^{-1} \mathcal {M} \mathcal {M}^{-1} \left ( \frac {1}{2}\mathcal {M} + \mathcal {K} \right ) = \mathcal{D} + \left ( \frac {1}{2}\mathcal {M}^{\mathrm {T}} + \mathcal {K}^{\mathrm {T}} \right ) \mathcal {V}^{-1} \left ( \frac {1}{2}\mathcal {M} + \mathcal {K} \right ),  \end{equation}<a id='x1-3006r13'></a> which follows the red route in the following commutative
diagram.
</p>
   <figure class='figure'> 

                                                                                               
                                                                                               
                                                                                               
                                                                                               
<!-- l. 166 --><p class='noindent'><img alt='PIC' src='/figures/2024-07-24_12-31-33-commutative-diagram-for-steklov-poincare.png'/>
                                                                                               
                                                                                               
</p>
   </figure>
   <h3 class='sectionHead'><span class='titlemark'>3    </span> <a id='x1-40003'></a>Discretization of a preconditioner</h3>
<!-- l. 172 --><p class='noindent'>When an iterative solver is used in BEM, only matrix/vector multiplication is needed. Therefore, the stiffness
matrix \(\mathcal {S}\) associated with the Steklov-Poincaré operator \(S\) is not assembled explicitly, while each of its component
matrix that constitutes \(S\) except the inverse matrix \(\mathcal {V}^{-1}\) will be assembled, i.e. \(\mathcal {D}\) and \(\frac {1}{2}\mathcal {M} + \mathcal {K}\). Let the input vector of \(\mathcal {S}\) be \(\underline {u}\) and the
result of \(\left ( \frac {1}{2}\mathcal {M} + \mathcal {K} \right ) \underline {u}\) be \(\underline {b}\). Then computing \(\mathcal {V}^{-1} \underline {b}\) is equivalent to solving \(\mathcal {V} \underline {x} = \underline {b}\) for \(\underline {x}\). During the solution of this equation, \(\mathcal {V}\) should be
preconditioned. Using the concept of operator preconditioning (<a href='#XMardalPreconditioning2011'>Mardal and Winther</a>), the preconditioning
operator for \(V\) is \(I + D\), due to the fact that \((I+D)^{-1}\) is spectrally equivalent to \(V\) (<a href='#XHsiaoDomain2000'>Hsiao et al.</a>). Then the matrix
corresponding to \((I+D)^{-1}\) is the preconditioning matrix, whose inverse will be applied to the two sides of \(\mathcal {V}\underline {x} = \underline {b}\).
Compared to multigrid and multilevel preconditioning techniques, operator preconditioning has two
advantages:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-4002x1'>
     <!-- l. 174 --><p class='noindent'>It does not rely on a hierarchical mesh, which is much easier to implement.
     </p></li>
<li class='enumerate' id='x1-4004x2'>
     <!-- l. 175 --><p class='noindent'>There is a rigorous proof about spectral equivalence, so the condition number of the preconditioned
     system can be controlled uniformly independent of the mesh size.</p></li></ol>
<!-- l. 178 --><p class='indent'>   <span class='p1xb-x-x-109'>N.B. The preconditioning operator corresponds to the inverse of the preconditioning matrix.</span>
</p><!-- l. 180 --><p class='indent'>   With the conclusion for the discretization of an inverse operator, the approximate matrix for \((I+D)^{-1}\) is \begin{equation}  \mathcal {M}_D^{\mathrm {T}} (\tilde {\mathcal {M}} + \mathcal {D})^{-1} \mathcal {M}_D,  \end{equation}<a id='x1-4005r14'></a> where \(\mathcal {M}_D\)
is the mass matrix related to the duality pairing \(\left \langle \cdot ,\cdot \right \rangle _{H_h^{1/2}(\Gamma ), H_h^{-1/2}(\Gamma )}\) between the dual space and range space of the
operator \(D\), \(\tilde {\mathcal {M}}\) is the mass matrix related to the inner product in \(H^{1/2}(\Gamma )\), since its row space and column space
are the same space \(H^{1/2}(\Gamma )\). The row space of \(\mathcal {M}_D^{\mathrm {T}} (\tilde {\mathcal {M}} + \mathcal {D})^{-1} \mathcal {M}_D\) is the dual space of the domain space of the operator \(I+D\), i.e.
\(^{\mathrm {W}}H^{-1/2}(\Gamma )\). Its column space is the range space of \(I+D\), i.e. \(^{\mathrm {S}}H^{-1/2}(\Gamma )\). Therefore, the preconditioning matrix is a square
matrix.
</p><!-- l. 186 --><p class='indent'>   The inverse of the preconditioning matrix is \begin{equation}  \mathcal {M}_D^{-1} (\tilde {\mathcal {M}} + \mathcal {D}) \mathcal {M}_D^{-\mathrm {T}},  \end{equation}<a id='x1-4006r15'></a> which is to be multiplied to \(\mathcal {V}\) from left. Its row and column
spaces are \(^{\mathrm {S}}H^{-1/2}(\Gamma )\times ^{\mathrm {W}}H^{-1/2}(\Gamma )\). We also notice that the function spaces for the preconditioned matrix \(\mathcal{M}_D^{-1} (\tilde{\mathcal{M}} + \mathcal{D}) \mathcal{M}_D^{-\mathrm{T}} \mathcal {V}\) are \(^{\mathrm {S}}H^{-1/2}(\Gamma )\times ^{\mathrm {S}}H^{-1/2}(\Gamma )\), i.e. its row and column
spaces are both in the strong form.
</p><!-- l. 192 --><p class='indent'>   The above example is special in that the preconditioning operator \(I+D\) is explicitly known and can be evaluated.
Therefore, there is no need to build the matrix for \((I+D)^{-1}\) first, then compute its inverse. Instead, we can directly build
the inverse of the preconditioning matrix using the rule of product algebra (<a href='#XBetckeProduct2017'>Betcke et al.</a>) for discretizing a
composition of operators \((I+D)V\). Its matrix form is \begin{equation}  (\tilde {\mathcal {M}} + \mathcal {D}) \mathcal {M}_V^{-1} \mathcal {V},  \end{equation}<a id='x1-4007r16'></a> where \(\mathcal {M}_V\) is the duality pairing between the dual and
range space of the operator \(V\), which is just the transpose of \(\mathcal {M}_D\). The row space of \(\tilde {\mathcal {M}} + \mathcal {D}\) is \(^{\mathrm {W}}H_h^{1/2}(\Gamma )\), which is the
dual space of the range space of the operator \(I+D\), i.e. \(^{\mathrm {S}}H^{-1/2}(\Gamma )\). However, as we mentioned above, the row
and column spaces of a preconditioned matrix should be in the strong form, and the row space
should be the range space of the operator \(I+D\). Therefore, we need to perform another weak form to
strong form transformation via the inverse of the matrix \(\mathcal {M}_D\). Finally, we obtain \begin{equation}  \mathcal {M}_D^{-1} (\tilde {\mathcal {M}} + \mathcal {D}) \mathcal {M}_D^{-\mathrm {T}} \mathcal {V}  \end{equation}<a id='x1-4008r17'></a> and the inverse of the
preconditioning matrix is \(\mathcal {M}_D^{-1} (\tilde {\mathcal {M}} + \mathcal {D}) \mathcal {M}_D^{-\mathrm {T}}\). This result is the same as the previous method. The commutative diagram is as
                                                                                               
                                                                                               
below.
</p>
   <figure class='figure'> 

                                                                                               
                                                                                               
                                                                                               
                                                                                               
<!-- l. 204 --><p class='noindent'><img alt='PIC' src='/figures/2024-07-24_12-32-05-commutative-diagram-for-preconditioner.png'/>
                                                                                               
                                                                                               
</p>
   </figure>
   <h3 class='sectionHead'><span class='titlemark'>4    </span> <a id='x1-50004'></a>Summary</h3>
<!-- l. 209 --><p class='noindent'>In BEM, the Galerkin discretization of an operator and its input and output functions involve the concept of strong and
weak form. The transformation between such strong form and weak form is realized via the duality pairing
between the range space and dual space of the operator. Even though, the matrix obtained from operator
discretization hides the range space, when the inverse of an operator is to be discretized, such range space
emerges to the surface. If we want to build a preconditioner using operator preconditioning technique, such
problem will also arise.
</p><!-- l. 211 --><p class='indent'>   Following is a list of key points to be kept in mind:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-5002x1'>
     <!-- l. 213 --><p class='noindent'>The Galerkin discretization of an operator produces a matrix which involves the domain space and dual
     space of the operator, while <span class='p1xb-x-x-109'>the range space is implicit</span>.
     </p></li>
<li class='enumerate' id='x1-5004x2'>
     <!-- l. 214 --><p class='noindent'>The input vector of the matrix is a <span class='p1xb-x-x-109'>strong form </span>vector in the domain space, which is associated with
     the column space of the matrix.
     </p></li>
<li class='enumerate' id='x1-5006x3'>
     <!-- l. 215 --><p class='noindent'>The output vector of the matrix is a <span class='p1xb-x-x-109'>weak form </span>vector in the dual space of the operator’s range
     space, which is associated with the row space of the matrix. The weak form vector can be considered
     as a direct projection from the implicit range space to its dual space via duality paring.
     </p></li>
<li class='enumerate' id='x1-5008x4'>
     <!-- l. 216 --><p class='noindent'>To get the <span class='p1xb-x-x-109'>strong form </span>vector related to the output function of the operator, we need to further
     multiply the inverse of the mass matrix associated with the duality pairing to the <span class='p1xb-x-x-109'>weak form </span>vector.
     This is inverse mass matrix is the discretization of the Riesz map mentioned in (<a href='#XBetckeProduct2017'>Betcke et al.</a>).
     </p></li>
<li class='enumerate' id='x1-5010x5'>
     <!-- l. 217 --><p class='noindent'>The Galerkin matrix for the inverse of an operator maps a strong form vector in the range space of
     the original operator to a weak form vector in the dual space of the domain space.
                                                                                               
                                                                                               
     </p></li>
<li class='enumerate' id='x1-5012x6'>
     <!-- l. 218 --><p class='noindent'>The inverse of the Galerkin matrix for an operator maps a weak form vector in the dual space of
     the range space to a strong form vector in the domain space.
     </p></li>
<li class='enumerate' id='x1-5014x7'>
     <!-- l. 219 --><p class='noindent'>The row and column spaces of a preconditioned matrix are both in the strong form.</p></li></ol>
<!-- l. 1 --><p class='noindent'>
</p>
   <h3 class='likesectionHead'><a id='x1-6000'></a>References</h3>
<!-- l. 1 --><p class='noindent'>
  </p><div class='thebibliography'>
  <p class='bibitem'><span class='biblabel'>
<a id='XBetckeProduct2017'></a><span class='bibsp'>   </span></span>Timo Betcke, Matthew Scroggs, and Wojciech Smigaj. Product algebras for galerkin discretisations of
  boundary integral operators and their applications. URL <a class='url' href='http://arxiv.org/abs/1711.10607'><span class='t1xtt-x-x-109'>http://arxiv.org/abs/1711.10607</span></a>.
  </p>
  <p class='bibitem'><span class='biblabel'>
<a id='XHsiaoDomain2000'></a><span class='bibsp'>   </span></span>G. C. Hsiao, O. Steinbach, and W. L. Wendland.   Domain decomposition methods via boundary
  integral equations. 125(1):521–537. ISSN 0377-0427. doi: 10.1016/S0377-0427(00)00488-X.
  </p>
  <p class='bibitem'><span class='biblabel'>
<a id='XMardalPreconditioning2011'></a><span class='bibsp'>   </span></span>Kent-Andre  Mardal  and  Ragnar  Winther.    Preconditioning  discretizations  of  systems  of  partial
  differential equations. 18(1):1–40. ISSN 1099-1506. doi: 10.1002/nla.716.
</p>
  </div>
   <div class='footnotes'><a id='x1-2006x1'></a>
<!-- l. 36 --><p class='indent'>      <span class='footnote-mark'><a href='#fn1x0-bk' id='fn1x0'><sup class='textsuperscript'>1</sup></a></span><span class='p1xr-x-x-90'>If there is no ambiguity from the context, we can simply write the duality pairing as </span>\(\left \langle \cdot ,\cdot \right \rangle \) <span class='p1xr-x-x-90'>without indicating the primal and dual
</span><span class='p1xr-x-x-90'>space.</span></p><a id='x1-2012x1'></a>
<!-- l. 88 --><p class='indent'>        <span class='footnote-mark'><a href='#fn2x0-bk' id='fn2x0'><sup class='textsuperscript'>2</sup></a></span><span class='p1xr-x-x-90'>When specifying the function spaces related to a matrix, we put the row space in front of the column space.</span></p>                 </div>

<p>Backlinks: <a href="/math/2024/11/06/spectral-equivalence.html">《Spectral equivalence》</a></p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="BEM" /><category term="PDE" /><summary type="html"><![CDATA[Contents  1 Vectors in strong form and weak form  2 Discretization of the inverse of an operator  3 Discretization of a preconditioner  4 Summary 1 Vectors in strong form and weak form Let \(V\) and \(W\) be two Hilbert spaces. Let \(V_h\) be the finite dimensional subspace of \(V\) with the basis \(\left \{ \varphi _{1},\cdots ,\varphi _{n} \right \}\) and \(W_h\) be the finite dimensional subspace of \(W\) with the basis \(\left \{ \zeta _{1},\cdots ,\zeta _{p} \right \}\). Let \(W'\) be the dual space of \(W\) and \(W_h'\) be its finite dimensional subspace with the basis \(\left \{ \psi _{1},\cdots ,\psi _{m} \right \}\). Let \(A: V \rightarrow W\) be a bounded linear operator from \(V\) to \(W\) and \(A_h: V_h \rightarrow W_h\) be its finite dimensional approximation. A function \(u_h\in V_h\) can be expanded as \(u_h=\sum _{i=1}^n u_i \varphi _i\). Its expansion coefficients make up the strong form vector \(\underline {u} = (u_1,\cdots ,u_n)^{\mathrm {T}}\) of \(u_h\). Here we explicitly write this vector as \(\underline {u}^{\mathrm {S}}\) to be differentiated from the weak form. To discretize the operator \(A_h\), we need two steps: Apply \(A_h\) to \(u_h\), which produces a function \(w_h\) in \(W_h\). Project \(w_h\) to each basis function \(\psi _i\) of \(W_h'\). The said projection is formed by duality pairing \(\left \langle \cdot ,\cdot \right \rangle _{W_h',W_h}\) 1 and will produce a vector \(\underline {w}^{\mathrm {W}}\), which is the weak form of \(w_h\). This vector is not related to the expansion coefficients \(w_k\) of \(w_h\) in \(W_h\), i.e. \(w_h = \sum _{k=1}^p w_k\zeta _k\), but is the projection coefficients of \(w_h\) in the dual space \(W_h'\). The expression for \(\underline {w}^{\mathrm {W}}\) can be expanded as \begin{equation} \underline {w}^{\mathrm {W}} = \begin {pmatrix} \left \langle \psi _1, w_h \right \rangle \\ \vdots \\ \left \langle \psi _m, w_h \right \rangle \end {pmatrix} = \begin {pmatrix} \left \langle \psi _1,A_hu_h \right \rangle \\ \vdots \\ \left \langle \psi _m,A_hu_h \right \rangle \end {pmatrix} = \begin {pmatrix} \left \langle \psi _1,A_h\sum _{j=1}^n u_j\varphi _j \right \rangle \\ \vdots \\ \left \langle \psi _m,A_h\sum _{j=1}^n u_j\varphi _j \right \rangle \end {pmatrix} = \begin {pmatrix} \sum _{j=1}^n \left \langle \psi _1, A_h \varphi _j \right \rangle u_j \\ \vdots \\ \sum _{j=1}^n \left \langle \psi _m, A_h \varphi _j \right \rangle u_j \end {pmatrix}. \end{equation} Its matrix form is \begin{equation} \underline {w}^{\mathrm {W}} = \mathcal {A} \underline {u}^{\mathrm {S}}, \end{equation} where \(\mathcal {A}_{ij} = \left \langle \psi _i, A_h\varphi _j \right \rangle \). If we substitute \(w_h = \sum _{k=1}^p w_k\zeta _k\) into the left hand side of this equation, we have \begin{equation} \underline {w}^{\mathrm {W}} = \begin {pmatrix} \left \langle \psi _1, \sum _{k=1}^p w_i\zeta _k \right \rangle \\ \vdots \\ \left \langle \psi _m, \sum _{k=1}^p w_i\zeta _k \right \rangle \end {pmatrix} = \begin {pmatrix} \sum _{k=1}^p \left \langle \psi _1, \zeta _k \right \rangle w_k \\ \vdots \\ \sum _{k=1}^p \left \langle \psi _m, \zeta _k \right \rangle w_k \\ \end {pmatrix} = \begin {pmatrix} \sum _{j=1}^n \left \langle \psi _1, A_h \varphi _j \right \rangle u_j \\ \vdots \\ \sum _{j=1}^n \left \langle \psi _m, A_h \varphi _j \right \rangle u_j \end {pmatrix}. \end{equation} Its matrix form is \begin{equation} \underline {w}^{\mathrm {W}} = \mathcal {M}_{W_h',W_h} \underline {w}^{\mathrm {S}} = \mathcal {A} \underline {u}^{\mathrm {S}}. \end{equation} In this equation, \(\mathcal {M}_{W_h',W_h}\) is the mass matrix associated with the duality pairing between the dual space \(W_h'\) and the range space \(W_h\) with respect to the operator \(A\) 2. For simplicity, this matrix is written as \(\mathcal {M}_A\). The vector \(\underline {w}^{\mathrm {S}}\) contains the actual expansion coefficients \((w_{1},\cdots ,w_{p})^{\mathrm {T}}\) of \(w_h\) in \(W_h\), therefore it is the strong form of \(w_h\), which can be derived from the weak form of \(w_h\) as below: \begin{equation} \underline {w}^{\mathrm {S}} = \mathcal {M}_A^{-1} \underline {w}^{\mathrm {W}} = \mathcal {M}_A^{-1}\mathcal {A}\underline {u}^{\mathrm {S}}. \end{equation} This means the mass matrix \(\mathcal {M}_A\) should be a square matrix and invertible, which requires two conditions: The number of basis elements adopted for the range space \(W_h\) of \(A\) should be the same as that for the dual space \(W_h'\). Take the single layer potential (SLP) operator \(V: H^{-1/2}(\Gamma ) \rightarrow H^{1/2}(\Gamma )\) in BEM as an example. \(W\) is \(H^{1/2}(\Gamma )\) and \(W'\) is \(H^{-1/2}(\Gamma )\). \(W_h\) is usually the piecewise linear continuous function space and \(W_h'\) is the piecewise constant function space. Basis functions of the former space are configured at nodes in the mesh, while basis functions for the latter space are configured in cells. Therefore, the number of basis functions of the two spaces usually mismatch. The remedy is to construct the space \(W_h'\) on the dual mesh, then there is a one-to-one correspondence between each cell in the dual mesh and each node in the primal mesh. The duality pairing between \(W_h'\) and \(W_h\) should be \(\inf \sup \) stable in the sense that the following condition should be satisfied (Betcke et al.) \begin{equation} \inf _{u\in W_h} \sup _{v\in W_h'} \frac {\lvert \left \langle u,v \right \rangle \rvert }{\lVert u \rVert _{W_h} \lVert v \rVert _{W_h'}} \geq c &gt; 0. \end{equation} Then the mass matrix \(\mathcal {M}_A\) is invertible. This \(\inf \sup \) stability is just the unique solvability condition or well-posedness condition of the following variational formulation: given \(f\in V'\), find \(u\in U\), such that \begin{equation} a(u,v) = \left \langle f, v \right \rangle \quad \forall v\in V. \end{equation} Such a variational formulation involves the generalized bilinear form \(a(\cdot ,\cdot ): U\times V \rightarrow \mathbb {R}\), where the two component spaces \(U\) and \(V\) are different. Therefore, this condition is a generalization of the ellipticity condition involved in the Lax-Milgram Lemma, where the bilinear form requires two identical spaces. For the duality pairing \(\left \langle \cdot ,\cdot \right \rangle _{W_h', W_h}\), it is actually a bilinear form \(a(\cdot ,\cdot ): W_h\times W_h' \rightarrow \mathbb {R}\) defined as \(a(u,v) := \left \langle v, Iu \right \rangle \), where \(I: W_h \rightarrow W_h\) is the identity operator. If the \(\inf \sup \) condition is satisfied, the mass matrix, as a discretization of the operator \(I\), is invertible. 2 Discretization of the inverse of an operator In BEM, when there is source distribution \(f\in \tilde {H}^{-1}(\Omega )\) in the volume domain \(\Omega \), the Newton potential \(N_0 f\) will also contribute to the Neumann trace at the boundary \(\Gamma \) as \(V^{-1}(N_0 f)\), where \(N_0: \tilde {H}^{-1}(\Omega ) \rightarrow H^{1/2}(\Gamma )\) and \(V^{-1}: H^{1/2}(\Gamma ) \rightarrow H^{-1/2}(\Gamma )\). It is obvious that if somehow we can directly discretize the inverse SLP operator \(V^{-1}\) into a matrix \(\mathcal {V}^{\dagger }\), its row and column function spaces are \(^{\mathrm {W}}H_h^{1/2}(\Gamma )\times ^{\mathrm {S}}H_h^{1/2}(\Gamma )\). On the other hand, if the matrix for \(V\) is \(\mathcal {V}\) and \(\mathcal {V}^{-1}\) is its inverse matrix, the row and column function spaces of \(\mathcal {V}^{-1}\) are \(^{\mathrm {S}}H_h^{-1/2}(\Gamma )\times ^{\mathrm {W}}H_h^{-1/2}(\Gamma )\). Therefore, \(\mathcal {V}^{\dagger }\) and \(\mathcal {V}^{-1}\) are two different matrices. N.B. Even though the function spaces related to matrices \(\mathcal {V}\) and \(\mathcal {V}^{-1}\) are the same \(H_h^{-1/2}(\Gamma )\times H_h^{-1/2}(\Gamma )\), the input or output vectors of the two matrices have different identities in the sense of strong or weak. In practice, an inverse operator like \(V^{-1}\) cannot be directly evaluated or discretized. We can only compute an approximation \(\tilde {\mathcal {V}^{\dagger }}\) of \(\mathcal {V}^{\dagger }\) by starting from the inverse of the matrix for the original operator, which is \(\mathcal {V}^{-1}\). In \(V^{-1}(N_0f)\), \(N_0f\) directly returns a strong form vector in \(H_h^{1/2}(\Gamma )\), which is the column space of the matrix \(\tilde {\mathcal {V}^{\dagger }}\). Hence we need to project this strong form vector from \(H_h^{1/2}(\Gamma )\) to the weak column space \(H_h^{-1/2}(\Gamma )\) of \(\mathcal {V}^{-1}\), and we can call this operation as space adaptation. This can be directly achieved by multiplying the mass matrix \(\mathcal {M}_{V}\) induced by the duality pairing between \(H_h^{-1/2}(\Gamma )\) and \(H_h^{1/2}(\Gamma )\). The output from \(\mathcal {V}^{-1}\) is a strong form vector in \(H_h^{-1/2}(\Gamma )\), which should be further mapped to the weak row space \(H_h^{1/2}(\Gamma )\) of \(\mathcal {V}^{\dagger }\). Because this is also a direct projection of a strong form vector to a weak form vector, we need to multiply the mass matrix \(\mathcal {M}_{V^{-1}}\) induced by the duality pairing between \(H_h^{1/2}(\Gamma )\) and \(H_h^{-1/2}(\Gamma )\). Let the basis of \(H_h^{1/2}(\Gamma )\) be \(\left \{ \varphi _{1},\cdots ,\varphi _{n} \right \}\) and the basis of \(H_h^{-1/2}(\Gamma )\) be \(\left \{ \psi _{1},\cdots ,\psi _{m} \right \}\). The coefficients of the above two mass matrices are \begin{equation} \mathcal {M}_V[i,j] = \left \langle \psi _i,\varphi _j \right \rangle \quad i=1,\cdots ,m, j=1,\cdots ,n \end{equation} and \begin{equation} \mathcal {M}_{V^{-1}}[i,j] = \left \langle \varphi _i,\psi _j \right \rangle \quad i=1,\cdots ,n, j=1,\cdots ,m. \end{equation} Therefore, \(\mathcal {M}_V = \mathcal {M}_{V^{-1}}^{\mathrm {T}}\). The matrix \(\tilde {\mathcal {V}}^{\dagger }\) as an approximate discretization of the inverse operator \(V^{-1}\) is \begin{equation} \tilde {\mathcal {V}^{\dagger }} = \mathcal {M}_V^{\mathrm {T}} \mathcal {V}^{-1} \mathcal {M}_V. \end{equation} The commutative diagram for the above operators and matrices is shown below, where the red route indicates the computation of \(\tilde {\mathcal {V}^{\dagger }}\) as the approximation for \(\mathcal {V}^{\dagger }\).]]></summary></entry><entry><title type="html">From duality pairing to operator preconditioning</title><link href="https://jihuan-tian.github.io/math/2024/07/13/from-duality-pairing-to-operator-preconditioning.html" rel="alternate" type="text/html" title="From duality pairing to operator preconditioning" /><published>2024-07-13T00:00:00+08:00</published><updated>2024-07-13T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/07/13/from-duality-pairing-to-operator-preconditioning</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/07/13/from-duality-pairing-to-operator-preconditioning.html"><![CDATA[<h3 class='likesectionHead'><a id='x1-1000'></a>Contents</h3>
   <div class='tableofcontents'>
    <span class='sectionToc'>1 <a href='#x1-20001' id='QQ2-1-2'>Duality pairing</a></span>
<br />    <span class='sectionToc'>2 <a href='#x1-30002' id='QQ2-1-3'>Gelfand triple</a></span>
<br />    <span class='sectionToc'>3 <a href='#x1-40003' id='QQ2-1-4'>Operator preconditioning</a></span>
   </div>
<!-- l. 24 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>1    </span> <a id='x1-20001'></a>Duality pairing</h3>
<!-- l. 26 --><p class='noindent'>In  Galerkin  BEM,  the  mass  matrix  \(\mathcal {M}\)  for  the  identity  operator  \(I: H^{1/2}(\Gamma ) \rightarrow H^{1/2}(\Gamma )\)
<span class='footnote-mark'><a href='#fn1x0' id='fn1x0-bk'><sup class='textsuperscript'>1</sup></a></span><a id='x1-2001f1'></a> is the
duality pairing between trial basis functions \(\{ \varphi _j \}_{j=1}^n\) in \(V_h \subset H^{1/2}(\Gamma )\) and test basis functions \(\{ \psi _i \}_{i=1}^m\) in \(W_h \subset H^{-1/2}(\Gamma )\). To compute the matrix element \(\mathcal {M}_{ij}\), we
directly compute the \(L_2(\Gamma )\) inner product of \(\varphi _j\) and \(\psi _i\).
</p><!-- l. 28 --><p class='indent'>   The reason why we can do this is: \(H^{1/2}(\Gamma )\) is a subspace of \(L_2(\Gamma )\), so the trial basis function \(\varphi _j\) in the finite dimensional
subspace \(V_h\) is also an \(L_2\) function. Meanwhile, we are using a finite dimensional polynomial space \(W_h\) to approximate \(H^{-1/2}(\Gamma )\),
such as the subspace spanned by piecewise constant functions. Therefore, the test basis function \(\psi _i\) is also an \(L_2\)
function and computing \(\langle \varphi _j,\psi _i \rangle = \int _{\Gamma } \varphi _j\psi _i \intd s_x\) is meaningful. From this we can see that even though the original Sobolev spaces
adopted in the weak form are relatively large, in practice, we are building matrices and searching the solution in
smaller spaces.
</p><!-- l. 30 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>2    </span> <a id='x1-30002'></a>Gelfand triple</h3>
<!-- l. 32 --><p class='noindent'>Because the duality pairing between \(H^{1/2}\) and \(H^{-1/2}\) inherits the duality pairing defined in \(L_2\), and \(H^{1/2}\) is dense in \(L_2\), according to
(<a href='#XBrezisFunctional2011'>Brezis</a>, page 136), \(L_2\) is self-dual or identified with itself and \(H^{-1/2}\) is a space larger than \(H^{1/2}\). There is the chain
                                                                                               
                                                                                               
of embedded function spaces on the boundary \(\Gamma \): \begin{equation}  H^{1/2}(\Gamma ) \subset L_2(\Gamma ) \simeq L_2^{*}(\Gamma ) \subset H^{-1/2}(\Gamma ).  \end{equation}<a id='x1-3001r1'></a> Similarly, in Galerkin FEM, function spaces are
defined in the domain \(\Omega \) and we also have \begin{equation}  H_0^1(\Omega ) \subset H^1(\Omega ) \subset L_2(\Omega ) \simeq L_2^{*}(\Omega ) \subset H^{-1}(\Omega ).  \end{equation}<a id='x1-3002r2'></a> The central space \(L_2\) in the above two chains is called the
pivot space, which is <span class='p1xb-x-x-109'>wrapped </span>by a dense subspace of \(L_2\) and its dual space. Such a “sandwich”
structure like \(( H^{1/2}(\Gamma ), L_2(\Gamma ), H^{-1/2}(\Gamma ) )\) or \(( H_0^1(\Omega ), L_2(\Omega ), H^{-1}(\Omega ) )\) is called the <span class='p1xb-x-x-109'>Gelfand triple</span>. In general, this triple is written as \begin{equation}  \label {eq:gelfand-triple} V\subset H \simeq H^{*} \subset V^{*}.  \end{equation}<a id='x1-3003r3'></a> Even though the
Riesz-Fréchet representation theorem tells us that for a Hilbert space we can identify it with its dual space,
we cannot <span class='p1xb-x-x-109'>simultaneously </span>identify \(V\) with \(V^{\ast }\) and \(H\) with \(H^{\ast }\). This is because when we define bounded
linear functionals on the dense subspace \(V\) by restricting the domain of the linear functionals on \(H\),
the duality pairing \(\langle \cdot ,\cdot \rangle _{V^{\ast },V}\) inherits the original duality pairing \(\langle \cdot ,\cdot \rangle _{H^{\ast },H}\), the latter of which further depends on
the inner product in \(H\). So the inner product in \(V\) is not used, and \(V\) cannot be identified with \(V^{\ast }\) when \(H\)
has been identified with \(H^{\ast }\). On the other hand, if we want to identify \(V\) with \(V^{\ast }\), the inner product in \(V\)
should be used. Then the inner product in \(H\) as well as the identification between \(H\) and \(H^{\ast }\) should be
abandoned.
</p><!-- l. 47 --><p class='indent'>   Here is the important point: <span class='p1xb-x-x-109'>if we want to identify a Hilbert space </span>\(H\) <span class='p1xb-x-x-109'>with its dual space </span>\(H^{\ast }\)<span class='p1xb-x-x-109'>, according to the
</span><span class='p1xb-x-x-109'>Riesz-Fréchet representation theorem, duality pairing describes the way of applying a bounded linear
</span><span class='p1xb-x-x-109'>functional in the dual space onto an element in the Hilbert space. This duality pairing is actually determined
</span><span class='p1xb-x-x-109'>by the inner product assigned to </span>\(H\)<span class='p1xb-x-x-109'>.</span>
</p><!-- l. 49 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>3    </span> <a id='x1-40003'></a>Operator preconditioning</h3>
<!-- l. 51 --><p class='noindent'>In BEM, we also have the above “sandwich” with \(L_2(\Gamma )\) as the pivot. Taking the Laplace problem as example. There
are two possible choices of key integral operators to be discretized as stiff matrices:
</p><!-- l. 53 --><p class='indent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-4002x1'>
     <!-- l. 54 --><p class='noindent'>The single layer potential integral operator \(V: H^{-1/2}(\Gamma ) \rightarrow H^{1/2}(\Gamma )\).
     </p></li>
<li class='enumerate' id='x1-4004x2'>
     <!-- l. 55 --><p class='noindent'>The hyper singular integral operator \(D: H^{1/2}(\Gamma ) \rightarrow H^{-1/2}(\Gamma )\).</p></li></ol>
<!-- l. 58 --><p class='indent'>   The “sandwich” is \(H^{1/2}(\Gamma ) \rightarrow L_2(\Gamma ) \rightarrow H^{-1/2}(\Gamma )\).
</p><!-- l. 60 --><p class='indent'>   If FEM is used, the key operator is the Laplace operator \(-\Delta : H_0^1(\Omega ) \rightarrow H^{-1}(\Omega )\) and the “sandwich” is \(H_0^1(\Omega ) \rightarrow L_2(\Omega ) \rightarrow H^{-1}(\Omega )\).
</p><!-- l. 62 --><p class='indent'>   According to (<a href='#XMardalPreconditioning2011'>Mardal and Winther</a>), the condition for Krylov space methods to be applicable, such as CG,
GMRES, etc., is that the operator should be symmetric and belong to the space of bounded linear operators from
a Hilbert space \(X\) to <span class='p1xb-x-x-109'>itself</span>, i.e. \(\mathcal {L}(X,X)\). In other words, the domain space and the range space of the operator should be the
same. Obviously, neither \(V\) nor \(D\) satisfies this condition. They belong to a space \(\mathcal {L}(X,Y)\) where \(X \neq Y\) and usually \(X\) and \(Y\) are a pair
of dual spaces.
</p><!-- l. 64 --><p class='indent'>   Generally, let a partial differential equation (PDE) or boundary integral equation (BIE) be \(Au = f\), where the
symmetric operator \(A: X \rightarrow Y\) is an isomorphism from \(X\) to \(Y\) and \(X \neq Y\). If \(A=V\), \(Y\) is smaller space than \(X\). If \(A=D\) or \(A=-\Delta \), \(Y\) is a larger space. To solve
this equation using Krylov space methods, we can apply a <span class='p1xb-x-x-109'>left preconditioner </span>\(B: Y \rightarrow X\) to both sides of the equation,
                                                                                               
                                                                                               
which is another isomorphism from \(Y\) to \(X\). Then the composition \(BA\) is an isomorphism from \(X\) to itself. The
preconditioned operator equation (strong form) is \begin{equation}  BA u = Bf.  \end{equation}<a id='x1-4005r4'></a> Its weak form is \begin{equation}  \left \langle BA u, v \right \rangle _X = \left \langle Bf, v \right \rangle _X \quad \forall v\in X.  \end{equation}<a id='x1-4006r5'></a> Because both \(BAu\) and the test function \(v\) are in \(X\), \(\left \langle \cdot ,\cdot \right \rangle _X\)
is an inner product on \(X\) instead of duality pairing as in the weak form without the preconditioner \(\left \langle Au,v \right \rangle = \left \langle f, v \right \rangle \). To ensure
that the new operator \(BA: X \rightarrow X\) is symmetric so that Krylov methods can be applied, the inner product \(\left \langle \cdot ,\cdot \right \rangle _X\)
should not be arbitrarily given but be induced by the inverse operator \(B^{-1}\) as \(\langle B^{-1}\cdot ,\cdot \rangle _X\). This can be proved by
simply checking the equality between \begin{equation}  \left \langle BAu, v \right \rangle _X = \left \langle B^{-1}BAu, v \right \rangle = \left \langle Au, v \right \rangle  \end{equation}<a id='x1-4007r6'></a> and \begin{equation}  \left \langle u, BAv \right \rangle _X = \left \langle u, B^{-1}BAv \right \rangle = \left \langle u, Av \right \rangle .  \end{equation}<a id='x1-4008r7'></a> Because \(A\) is symmetric, we have \(\left \langle Au,v \right \rangle = \left \langle u,Av \right \rangle \), hence \(\left \langle BAu,v \right \rangle _X = \left \langle u, BAv \right \rangle _{X}\). Therefore, \(BA\) is
symmetric.
</p><!-- l. 82 --><p class='indent'>   Now, a Krylov space method can be selected to solve the equation \(BAu = Bf\). The whole method is called the
preconditioned Krylov space method. <span class='p1xb-x-x-109'>It can be understood that applying a preconditioner to the equation is
</span><span class='p1xb-x-x-109'>equivalent to defining a norm </span>\(\langle B^{-1}\cdot ,\cdot \rangle \) <span class='p1xb-x-x-109'>on </span>\(X\)<span class='p1xb-x-x-109'>. The error estimate of the preconditioner Krylov space method is also given
</span><span class='p1xb-x-x-109'>in this norm.</span>
</p><!-- l. 1 --><p class='noindent'>
</p>
   <h3 class='likesectionHead'><a id='x1-5000'></a>References</h3>
<!-- l. 1 --><p class='noindent'>
  </p><div class='thebibliography'>
  <p class='bibitem'><span class='biblabel'>
<a id='XBrezisFunctional2011'></a><span class='bibsp'>   </span></span>Haim  Brezis.   <span class='p1xi-x-x-109'>Functional  Analysis,  Sobolev  Spaces  and  Partial  Differential  Equations</span>.   Springer.   ISBN
  978-0-387-70913-0 978-0-387-70914-7. doi: 10.1007/978-0-387-70914-7.
  </p>
  <p class='bibitem'><span class='biblabel'>
<a id='XMardalPreconditioning2011'></a><span class='bibsp'>   </span></span>Kent-Andre  Mardal  and  Ragnar  Winther.    Preconditioning  discretizations  of  systems  of  partial
  differential equations. 18(1):1–40. ISSN 1099-1506. doi: 10.1002/nla.716.
</p>
  </div>
   <div class='footnotes'><a id='x1-2002x1'></a>
<!-- l. 26 --><p class='indent'>      <span class='footnote-mark'><a href='#fn1x0-bk' id='fn1x0'><sup class='textsuperscript'>1</sup></a></span><span class='p1xr-x-x-90'>N.B. The bilinear form associated with this identity operator is </span>\(b_I: H^{1/2}(\Gamma )\times H^{-1/2}(\Gamma ) \rightarrow \mathbb {R}\)<span class='p1xr-x-x-90'>.</span></p>                                                                                              </div>

<p></p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="FEM" /><category term="BEM" /><category term="PDE" /><category term="functional-analysis" /><summary type="html"><![CDATA[Contents  1 Duality pairing  2 Gelfand triple  3 Operator preconditioning 1 Duality pairing In Galerkin BEM, the mass matrix \(\mathcal {M}\) for the identity operator \(I: H^{1/2}(\Gamma ) \rightarrow H^{1/2}(\Gamma )\) 1 is the duality pairing between trial basis functions \(\{ \varphi _j \}_{j=1}^n\) in \(V_h \subset H^{1/2}(\Gamma )\) and test basis functions \(\{ \psi _i \}_{i=1}^m\) in \(W_h \subset H^{-1/2}(\Gamma )\). To compute the matrix element \(\mathcal {M}_{ij}\), we directly compute the \(L_2(\Gamma )\) inner product of \(\varphi _j\) and \(\psi _i\). The reason why we can do this is: \(H^{1/2}(\Gamma )\) is a subspace of \(L_2(\Gamma )\), so the trial basis function \(\varphi _j\) in the finite dimensional subspace \(V_h\) is also an \(L_2\) function. Meanwhile, we are using a finite dimensional polynomial space \(W_h\) to approximate \(H^{-1/2}(\Gamma )\), such as the subspace spanned by piecewise constant functions. Therefore, the test basis function \(\psi _i\) is also an \(L_2\) function and computing \(\langle \varphi _j,\psi _i \rangle = \int _{\Gamma } \varphi _j\psi _i \intd s_x\) is meaningful. From this we can see that even though the original Sobolev spaces adopted in the weak form are relatively large, in practice, we are building matrices and searching the solution in smaller spaces. 2 Gelfand triple Because the duality pairing between \(H^{1/2}\) and \(H^{-1/2}\) inherits the duality pairing defined in \(L_2\), and \(H^{1/2}\) is dense in \(L_2\), according to (Brezis, page 136), \(L_2\) is self-dual or identified with itself and \(H^{-1/2}\) is a space larger than \(H^{1/2}\). There is the chain of embedded function spaces on the boundary \(\Gamma \): \begin{equation} H^{1/2}(\Gamma ) \subset L_2(\Gamma ) \simeq L_2^{*}(\Gamma ) \subset H^{-1/2}(\Gamma ). \end{equation} Similarly, in Galerkin FEM, function spaces are defined in the domain \(\Omega \) and we also have \begin{equation} H_0^1(\Omega ) \subset H^1(\Omega ) \subset L_2(\Omega ) \simeq L_2^{*}(\Omega ) \subset H^{-1}(\Omega ). \end{equation} The central space \(L_2\) in the above two chains is called the pivot space, which is wrapped by a dense subspace of \(L_2\) and its dual space. Such a “sandwich” structure like \(( H^{1/2}(\Gamma ), L_2(\Gamma ), H^{-1/2}(\Gamma ) )\) or \(( H_0^1(\Omega ), L_2(\Omega ), H^{-1}(\Omega ) )\) is called the Gelfand triple. In general, this triple is written as \begin{equation} \label {eq:gelfand-triple} V\subset H \simeq H^{*} \subset V^{*}. \end{equation} Even though the Riesz-Fréchet representation theorem tells us that for a Hilbert space we can identify it with its dual space, we cannot simultaneously identify \(V\) with \(V^{\ast }\) and \(H\) with \(H^{\ast }\). This is because when we define bounded linear functionals on the dense subspace \(V\) by restricting the domain of the linear functionals on \(H\), the duality pairing \(\langle \cdot ,\cdot \rangle _{V^{\ast },V}\) inherits the original duality pairing \(\langle \cdot ,\cdot \rangle _{H^{\ast },H}\), the latter of which further depends on the inner product in \(H\). So the inner product in \(V\) is not used, and \(V\) cannot be identified with \(V^{\ast }\) when \(H\) has been identified with \(H^{\ast }\). On the other hand, if we want to identify \(V\) with \(V^{\ast }\), the inner product in \(V\) should be used. Then the inner product in \(H\) as well as the identification between \(H\) and \(H^{\ast }\) should be abandoned. Here is the important point: if we want to identify a Hilbert space \(H\) with its dual space \(H^{\ast }\), according to the Riesz-Fréchet representation theorem, duality pairing describes the way of applying a bounded linear functional in the dual space onto an element in the Hilbert space. This duality pairing is actually determined by the inner product assigned to \(H\). 3 Operator preconditioning In BEM, we also have the above “sandwich” with \(L_2(\Gamma )\) as the pivot. Taking the Laplace problem as example. There are two possible choices of key integral operators to be discretized as stiff matrices: The single layer potential integral operator \(V: H^{-1/2}(\Gamma ) \rightarrow H^{1/2}(\Gamma )\). The hyper singular integral operator \(D: H^{1/2}(\Gamma ) \rightarrow H^{-1/2}(\Gamma )\). The “sandwich” is \(H^{1/2}(\Gamma ) \rightarrow L_2(\Gamma ) \rightarrow H^{-1/2}(\Gamma )\). If FEM is used, the key operator is the Laplace operator \(-\Delta : H_0^1(\Omega ) \rightarrow H^{-1}(\Omega )\) and the “sandwich” is \(H_0^1(\Omega ) \rightarrow L_2(\Omega ) \rightarrow H^{-1}(\Omega )\). According to (Mardal and Winther), the condition for Krylov space methods to be applicable, such as CG, GMRES, etc., is that the operator should be symmetric and belong to the space of bounded linear operators from a Hilbert space \(X\) to itself, i.e. \(\mathcal {L}(X,X)\). In other words, the domain space and the range space of the operator should be the same. Obviously, neither \(V\) nor \(D\) satisfies this condition. They belong to a space \(\mathcal {L}(X,Y)\) where \(X \neq Y\) and usually \(X\) and \(Y\) are a pair of dual spaces. Generally, let a partial differential equation (PDE) or boundary integral equation (BIE) be \(Au = f\), where the symmetric operator \(A: X \rightarrow Y\) is an isomorphism from \(X\) to \(Y\) and \(X \neq Y\). If \(A=V\), \(Y\) is smaller space than \(X\). If \(A=D\) or \(A=-\Delta \), \(Y\) is a larger space. To solve this equation using Krylov space methods, we can apply a left preconditioner \(B: Y \rightarrow X\) to both sides of the equation, which is another isomorphism from \(Y\) to \(X\). Then the composition \(BA\) is an isomorphism from \(X\) to itself. The preconditioned operator equation (strong form) is \begin{equation} BA u = Bf. \end{equation} Its weak form is \begin{equation} \left \langle BA u, v \right \rangle _X = \left \langle Bf, v \right \rangle _X \quad \forall v\in X. \end{equation} Because both \(BAu\) and the test function \(v\) are in \(X\), \(\left \langle \cdot ,\cdot \right \rangle _X\) is an inner product on \(X\) instead of duality pairing as in the weak form without the preconditioner \(\left \langle Au,v \right \rangle = \left \langle f, v \right \rangle \). To ensure that the new operator \(BA: X \rightarrow X\) is symmetric so that Krylov methods can be applied, the inner product \(\left \langle \cdot ,\cdot \right \rangle _X\) should not be arbitrarily given but be induced by the inverse operator \(B^{-1}\) as \(\langle B^{-1}\cdot ,\cdot \rangle _X\). This can be proved by simply checking the equality between \begin{equation} \left \langle BAu, v \right \rangle _X = \left \langle B^{-1}BAu, v \right \rangle = \left \langle Au, v \right \rangle \end{equation} and \begin{equation} \left \langle u, BAv \right \rangle _X = \left \langle u, B^{-1}BAv \right \rangle = \left \langle u, Av \right \rangle . \end{equation} Because \(A\) is symmetric, we have \(\left \langle Au,v \right \rangle = \left \langle u,Av \right \rangle \), hence \(\left \langle BAu,v \right \rangle _X = \left \langle u, BAv \right \rangle _{X}\). Therefore, \(BA\) is symmetric. Now, a Krylov space method can be selected to solve the equation \(BAu = Bf\). The whole method is called the preconditioned Krylov space method. It can be understood that applying a preconditioner to the equation is equivalent to defining a norm \(\langle B^{-1}\cdot ,\cdot \rangle \) on \(X\). The error estimate of the preconditioner Krylov space method is also given in this norm. References    Haim Brezis. Functional Analysis, Sobolev Spaces and Partial Differential Equations. Springer. ISBN 978-0-387-70913-0 978-0-387-70914-7. doi: 10.1007/978-0-387-70914-7.    Kent-Andre Mardal and Ragnar Winther. Preconditioning discretizations of systems of partial differential equations. 18(1):1–40. ISSN 1099-1506. doi: 10.1002/nla.716. 1N.B. The bilinear form associated with this identity operator is \(b_I: H^{1/2}(\Gamma )\times H^{-1/2}(\Gamma ) \rightarrow \mathbb {R}\).]]></summary></entry><entry><title type="html">Understanding about Riesz map</title><link href="https://jihuan-tian.github.io/math/2024/07/11/understanding-about-riesz-map.html" rel="alternate" type="text/html" title="Understanding about Riesz map" /><published>2024-07-11T00:00:00+08:00</published><updated>2024-07-11T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/07/11/understanding-about-riesz-map</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/07/11/understanding-about-riesz-map.html"><![CDATA[<h3 class='likesectionHead'><a id='x1-1000'></a>Contents</h3>
   <div class='tableofcontents'>
    <span class='sectionToc'>1 <a href='#x1-20001' id='QQ2-1-2'>Riesz representation theorem (Brezis)</a></span>
<br />    <span class='sectionToc'>2 <a href='#x1-30002' id='QQ2-1-3'>Construction of the Riesz map as a preconditioner</a></span>
<br />     <span class='subsectionToc'>2.1 <a href='#x1-40002.1' id='QQ2-1-4'>Riesz map to be used as a preconditioner depends on the
definition of inner product</a></span>
<br />     <span class='subsectionToc'>2.2 <a href='#x1-50002.2' id='QQ2-1-5'>Riesz map as a baseline preconditioner</a></span>
<br />    <span class='sectionToc'>3 <a href='#x1-60003' id='QQ2-1-6'>Construction of the Riesz map as a space adaptor</a></span>
<br />     <span class='subsectionToc'>3.1 <a href='#x1-70003.1' id='QQ2-1-7'>Issue of implicit range spaces in BEM</a></span>
<br />     <span class='subsectionToc'>3.2 <a href='#x1-80003.2' id='QQ2-1-8'>Mass matrix as space projection</a></span>
<br />     <span class='subsectionToc'>3.3 <a href='#x1-90003.3' id='QQ2-1-9'>Riesz map for transforming a function from dual space to range space</a></span>
   </div>

<h3 class='sectionHead'><span class='titlemark'>1    </span> <a id='x1-20001'></a>Riesz representation theorem (<a href='#XBrezisFunctional2011'>Brezis</a>)</h3>
   <div class='theorem'><div class='newtheorem'>
<!-- l. 27 --><p class='noindent'><span class='head'>
<span class='p1xb-x-x-109'>Theorem 1 (Riesz-Fréchet representation theorem)</span> </span><a id='x1-2002'></a><span class='p1xi-x-x-109'>Let </span>\(H\) <span class='p1xi-x-x-109'>be a Hilbert space and </span>\(H'\) <span class='p1xi-x-x-109'>be its dual space. </span>\(\left ( \cdot ,\cdot \right )\) <span class='p1xi-x-x-109'>is the
</span><span class='p1xi-x-x-109'>inner product on </span>\(H\)<span class='p1xi-x-x-109'>. </span>\(\left \langle \cdot ,\cdot \right \rangle \) <span class='p1xi-x-x-109'>is the duality pairing between </span>\(H\) <span class='p1xi-x-x-109'>and </span>\(H'\)<span class='p1xi-x-x-109'>. Given any </span>\(f \in H'\)<span class='p1xi-x-x-109'>, there exits a unique </span>\(u \in H\) <span class='p1xi-x-x-109'>such that </span>\begin{equation}  \label {eq:riesz-representation} \left \langle f,v \right \rangle = \left ( u,v \right ) \quad \forall v\in H  \end{equation}<a id='x1-2003r1'></a> <span class='p1xi-x-x-109'>and </span>\(\lVert f \rVert _{H'} = \lVert u \rVert _H\)<span class='p1xi-x-x-109'>.</span>
</p>
   </div>
<!-- l. 35 --><p class='indent'>   </p></div>
                                                                                               
                                                                                               
<!-- l. 37 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>2    </span> <a id='x1-30002'></a>Construction of the Riesz map as a preconditioner</h3>
<!-- l. 39 --><p class='noindent'>
</p>
   <h4 class='subsectionHead'><span class='titlemark'>2.1    </span> <a id='x1-40002.1'></a>Riesz map to be used as a preconditioner depends on the definition of inner product</h4>
<!-- l. 41 --><p class='noindent'>The two functions in the duality pairing on the left hand side of Equation (<a href='#x1-2003r1'>1<!-- tex4ht:ref: eq:riesz-representation  --></a>) have different roles: \(f\) is the bounded
linear functional and \(v\) is the object to be operated by it. The two functions \(u\)and \(v\) in the inner product on the right
hand side have the same identity. This theorem states that there is an isometric isomorphism between a Hilbert
space and its dual space.
</p><!-- l. 43 --><p class='indent'>   Riesz map is a map from the dual space \(H'\) to the primal space \(H\), i.e. \(\tau : H' \rightarrow H\). Hence, the Riesz-Fréchet theorem can be
written as \begin{equation}  \left ( \tau f, v \right ) = \left \langle f, v \right \rangle \quad \forall v \in H  \end{equation}<a id='x1-4001r2'></a> and \(\lVert \tau f \rVert _H = \lVert f \rVert _{H'}\). The Riesz map depends on the definition of the inner product on \(H\). Here are three
examples.
</p><!-- l. 49 --><p class='indent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-4003x1'>
     <!-- l. 50 --><p class='noindent'>Use the canonical \(L_2\) inner product on \(H\). Then \begin{equation}  \left ( \tau f, v \right ) = \int _{\Omega } (\tau f) v \intd x.  \end{equation}<a id='x1-4004r3'></a> Equate it to the duality pairing \(\left \langle f, v \right \rangle \): \begin{equation}  \int _{\Omega } (\tau f) v \intd x = \left \langle f, v \right \rangle \quad \forall v\in H.  \end{equation}<a id='x1-4005r4'></a> If we treat \(\tau f\) (as a whole) as the
     solution to be sought, this equation is actually in the weak form or variational form. By introducing the
     identity operator \(I: H \rightarrow H'\), the integral on the left hand side can be written as a bilinear form, \begin{equation}  \left \langle I (\tau f), v \right \rangle = \left \langle f, v \right \rangle \quad \forall v\in H.  \end{equation}<a id='x1-4006r5'></a> The strong
     form or operator form of the equation is \begin{equation}  I (\tau f) = f.  \end{equation}<a id='x1-4007r6'></a> Then the Riesz map \(\tau \) is the inverse operator of \(I\), i.e.
     \(I^{-1}\).
     </p><!-- l. 68 --><p class='noindent'>Now we want to see the discretized matrix form of the operators \(I\) and \(\tau \). The discretization is carried out
     using the Galerkin method. Let \(H_h\) be a finite dimensional subspace of the primal space \(H\) with the basis \(\left \{\varphi _1,\cdots ,\varphi _n \right \}\), and \(H_h'\)
     be a finite dimensional subspace of the dual space \(H'\) with the basis \(\left ( \psi _1,\cdots ,\psi _m \right )\). The matrix for \(I\) is \begin{equation}  \mathcal {M}_{ij} = \left \langle I\varphi _j, \varphi _i \right \rangle = \left ( \varphi _j, \varphi _i \right ),  \end{equation}<a id='x1-4008r7'></a> and the matrix for \(\tau \) is
     \(\mathcal {M}^{-1}\).
</p>
     <div class='newtheorem'>
     <!-- l. 74 --><p class='noindent'><span class='head'>
     <span class='p1xb-x-x-109'>Comment 1</span> </span><a id='x1-4010'></a><span class='p1xi-x-x-109'>Here we directly use the matrix coefficient </span>\(\mathcal {M}_{ij}\) <span class='p1xi-x-x-109'>as a representation of the whole matrix </span>\(\mathcal {M}\)<span class='p1xi-x-x-109'>. Such
     </span><span class='p1xi-x-x-109'>a representation is unambiguous and is consistent with the convention for writing tensors in differential
     </span><span class='p1xi-x-x-109'>geometry. Because </span>\(I\) <span class='p1xi-x-x-109'>maps </span>\(\varphi _j\) <span class='p1xi-x-x-109'>in </span>\(H\) <span class='p1xi-x-x-109'>to a same function in </span>\(H'\)<span class='p1xi-x-x-109'>, the duality pairing of </span>\(I\varphi _j\) <span class='p1xi-x-x-109'>and </span>\(\varphi _i\) <span class='p1xi-x-x-109'>is actually the </span>\(L_2\) <span class='p1xi-x-x-109'>inner product
     </span><span class='p1xi-x-x-109'>of </span>\(\varphi _j\) <span class='p1xi-x-x-109'>and </span>\(\varphi _i\)<span class='p1xi-x-x-109'>.</span>
</p>
     </div>
     <!-- l. 76 --><p class='noindent'>
</p>
     <div class='definition'><div class='newtheorem'>
     <!-- l. 78 --><p class='noindent'><span class='head'>
                                                                                               
                                                                                               
     <span class='p1xb-x-x-109'>Definition 1 (Mass matrix)</span> </span><a id='x1-4012'></a>The matrix \(\mathcal {M}\) with its elements being the inner product between each
     pair of basis functions of \(H\) is called the mass matrix with respect to \(H\). <a id='x1-40111'></a>
</p>
     </div>
     <!-- l. 81 --><p class='noindent'></p></div>
     </li>
<li class='enumerate' id='x1-4014x2'>
     <!-- l. 83 --><p class='noindent'>If \(H\) is the Sobolev space \(H_0^1(\Omega )\), we can use the inner product inherited from \(H^1(\Omega )\): \begin{equation}  \left ( u,v \right )_{H^1(\Omega )} = \int _{\Omega } \left ( \nabla u \cdot \nabla v + uv \right ) \intd x.  \end{equation}<a id='x1-4015r8'></a> Then the left hand side of Equation
     (<a href='#x1-2003r1'>1<!-- tex4ht:ref: eq:riesz-representation  --></a>) is \begin{equation}  \left ( \tau f, v \right ) = \int _{\Omega } \left ( \nabla (\tau f)\cdot (\nabla v) + (\tau f) v \right ) \intd x.  \end{equation}<a id='x1-4016r9'></a> The corresponding weak form is \begin{equation}  \left ( \tau f, v \right ) = \int _{\Omega } \left ( \nabla (\tau f)\cdot (\nabla v) + (\tau f) v \right ) \intd x = \left \langle f, v \right \rangle \quad \forall v\in H.  \end{equation}<a id='x1-4017r10'></a> Its strong form is \begin{equation}  -\Delta (\tau f) + \tau f = f,  \end{equation}<a id='x1-4018r11'></a> i.e. \begin{equation}  \left ( -\Delta + I \right )(\tau f) = f.  \end{equation}<a id='x1-4019r12'></a> This is just the Helmholtz equation and \(\tau f\) is its
     solution. Solving \(\tau f\) for this equation with the given data \(f\) has the same effect as the Riesz map
     \(\tau \). Therefore, using the above inner product, the Riesz map is the inverse of the Helmholtz
     operator.
     </p><!-- l. 105 --><p class='noindent'>The matrix derived from the weak form of the Helmholtz operator is \begin{equation}  \mathcal {H}_{ij} = \int _{\Omega } \left ( \nabla \varphi _j \cdot \nabla \varphi _i + \varphi _j \varphi _i \right ) \intd x.  \end{equation}<a id='x1-4020r13'></a> The matrix for \(\tau \) is \(\mathcal {H}^{-1}\). \(\mathcal {H}\) can also be
     understood as a new <span class='p1xb-x-x-109'>mass matrix </span>with respect to the above inner product.
     </p></li>
<li class='enumerate' id='x1-4022x3'>
     <!-- l. 111 --><p class='noindent'>In this case, \(H\) is still \(H_0^1(\Omega )\), but the inner product becomes \begin{equation}  \left ( u,v \right )_{H^1(\Omega )} = \int _{\Omega } \nabla u \cdot \nabla v \intd x.  \end{equation}<a id='x1-4023r14'></a> This inner product induces the semi-norm on \(H^1(\Omega )\), which is
     also a norm on \(H_0^1(\Omega )\).
     </p><!-- l. 117 --><p class='noindent'>The weak form equation is \begin{equation}  \int _{\Omega } \nabla (\tau f) \cdot (\nabla v) \intd x = \left \langle f, v \right \rangle \quad \forall v\in H.  \end{equation}<a id='x1-4024r15'></a> The strong form is \begin{equation}  -\Delta (\tau f) = f.  \end{equation}<a id='x1-4025r16'></a> This is just the Poisson equation. Riesz map \(\tau \) is
     the inverse of the Laplace operator. The matrix for the Laplace operator is \begin{equation}  \mathcal {L}_{ij} = \int _{\Omega } \nabla \varphi _j \cdot \nabla \varphi _i \intd x.  \end{equation}<a id='x1-4026r17'></a> Similarly, \(\mathcal {L}\) can
     be understood as a <span class='p1xb-x-x-109'>mass matrix </span>with respect to the above inner product. The matrix for \(\tau \) is
     \(\mathcal {L}^{-1}\).</p></li></ol>
<!-- l. 132 --><p class='noindent'>
</p>
   <h4 class='subsectionHead'><span class='titlemark'>2.2    </span> <a id='x1-50002.2'></a>Riesz map as a baseline preconditioner</h4>
<!-- l. 134 --><p class='noindent'>From above we can see that the Riesz map from \(H'\) to \(H\) is actually the inverse of a differential operator \(\tilde {A}: H \rightarrow H'\), or it
can also be understood as a kind of mass matrix which depends on the inner product adopted for
\(H\). According to (<a href='#XMardalPreconditioning2011'>Mardal and Winther</a>), a preconditioner \(B\) is an isomorphism from \(H'\) to \(H\). Therefore,
the Riesz map can be considered as a preconditioner, even though it is not optimal according to
(<a href='#XKirbyFunctional2010'>Kirby</a>).
</p><!-- l. 136 --><p class='indent'>   Since a Riesz map is the inverse of \(\tilde {A}\), its matrix form usually cannot be directly assembled. We can first
assemble the matrix \(\widetilde {\mathcal {A}}\) for the operator \(\tilde {A}\), then compute the approximate inverse of \(\widetilde {\mathcal {A}}\). In practice, when we use a
preconditioner in an iterative solver, only matrix/vector multiplication is needed and a direct computation of
matrix inverse is avoided. Instead, the linear system \(\widetilde {\mathcal {A}}x = y\) is to be solved, which has the same effect as
\(\tilde {\mathcal {A}}^{-1}y\).
</p>
                                                                                               
                                                                                               
   <div class='newtheorem'>
<!-- l. 138 --><p class='noindent'><span class='head'>
<span class='p1xb-x-x-109'>Comment 2</span> </span><a id='x1-5002'></a><span class='p1xi-x-x-109'>As        an        operator,        the        Riesz        map        </span>\(\tau \)        <span class='p1xi-x-x-109'>maps        from        </span>\(H'\)        <span class='p1xi-x-x-109'>to        </span>\(H\)<span class='p1xi-x-x-109'>.        As
</span><span class='p1xi-x-x-109'>an       inverse       matrix       </span>\(\tilde {\mathcal {A}}^{-1}\)<span class='p1xi-x-x-109'>,       the       matrix       </span>\(\tau \)       <span class='p1xi-x-x-109'>have       row       and       column       spaces       as       </span>\((H_h, H_h)\)
<span class='footnote-mark'><a href='#fn1x0' id='fn1x0-bk'><sup class='textsuperscript'>1</sup></a></span><a id='x1-5003f1'></a> <span class='p1xi-x-x-109'>.
</span><span class='p1xi-x-x-109'>At the first glance, it seems that the matrix spaces </span>\(\left ( H_h,H_h \right )\) <span class='p1xi-x-x-109'>do not match the operator spaces </span>\(H' \rightarrow H\)<span class='p1xi-x-x-109'>. However, this is actually
</span><span class='p1xi-x-x-109'>correct, since </span>\(\tilde {\mathcal {A}}\) <span class='p1xi-x-x-109'>is derived from the weak form of the differential operator </span>\(\tilde {A}\)<span class='p1xi-x-x-109'>, whose row and column spaces are </span>\(\left ( H_h,H_h \right )\)<span class='p1xi-x-x-109'>. These
</span><span class='p1xi-x-x-109'>spaces are just the same as those of the stiff matrix of the PDE. Therefore, the matrix for the Riesz map can be
</span><span class='p1xi-x-x-109'>directly applied to the stiff matrix as a preconditioner.</span>
</p>
   </div>
<!-- l. 140 --><p class='indent'>
</p><!-- l. 142 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>3    </span> <a id='x1-60003'></a>Construction of the Riesz map as a space adaptor</h3>
<!-- l. 144 --><p class='noindent'>
</p>
   <h4 class='subsectionHead'><span class='titlemark'>3.1    </span> <a id='x1-70003.1'></a>Issue of implicit range spaces in BEM</h4>
<!-- l. 146 --><p class='noindent'>In (<a href='#XBetckeProduct2017'>Betcke et al.</a>), product algebra is proposed for handling the discretization of the composition of
two operators used in BEM. Being different from FEM, BEM has the issue of implicit range spaces
(<a href='/math/2024/07/04/domain-range-and-dual-spaces-in-bem.html'>Domain, range and dual spaces in BEM</a>). When there is a composition of two
operators \(BA\), \(C=BA\) cannot be discretized as a whole usually. Then it is natural for us to build the two matrices
for \(A\) and \(B\) first. Then we use \(\mathcal {A}\) and \(\mathcal {B}\) to construct the matrix \(\mathcal {C}\) for \(C\). However, \(\mathcal {C}\) is usually not a direct
product of \(\mathcal {A}\) and \(\mathcal {B}\). Actually, after discretization, the range space of \(A\) is hidden and the test space of
\(A\), which is the row space of \(\mathcal {A}\) does not match the domain space of \(B\), which is the column space of
\(\mathcal {B}\).
</p><!-- l. 148 --><p class='noindent'>
</p>
   <h4 class='subsectionHead'><span class='titlemark'>3.2    </span> <a id='x1-80003.2'></a>Mass matrix as space projection</h4>
<!-- l. 150 --><p class='noindent'>Definitions of the operators and matrices which are related to \(BA\): </p>
     <ul class='itemize1'>
     <li class='itemize'>
     <!-- l. 152 --><p class='noindent'>\(A: \mathcal {H}_A^{\mathrm {dom}} \rightarrow \mathcal {H}_A^{\mathrm {ran}}\);
                                                                                               
                                                                                               
     </p></li>
     <li class='itemize'>
     <!-- l. 153 --><p class='noindent'>\(B: \mathcal {H}_{B}^{\mathrm {dom}} \rightarrow \mathcal {H}_{B}^{\mathrm {ran}}\) with \(\mathcal {H}_{A}^{\mathrm {ran}} \subset \mathcal {H}_{B}^{\mathrm {dom}}\), which makes the composition of \(A\) and \(B\) meaningful;
     </p></li>
     <li class='itemize'>
     <!-- l. 154 --><p class='noindent'>The identity maps \(M_A\) and \(M_B\) from the corresponding range space to dual space;
     </p></li>
     <li class='itemize'>
     <!-- l. 155 --><p class='noindent'>The mass matrices \(\mathcal {M}_A\) and \(\mathcal {M}_B\) associated with the identity maps;
     </p></li>
     <li class='itemize'>
     <!-- l. 156 --><p class='noindent'>The Riesz maps \(M_A^{-1}\) and \(M_B^{-1}\) as the inverse maps of \(M_A\) and \(M_B\) respectively;
     </p></li>
     <li class='itemize'>
     <!-- l. 157 --><p class='noindent'>The matrices for the Riesz maps \(\mathcal {M}_A^{-1}\) and \(\mathcal {M}_B^{-1}\).</p></li></ul>
<!-- l. 160 --><p class='indent'>   The following commutative diagram shows the relationship between function spaces, when we are dealing
with the discretization of \(BA\). </p><figure class='figure'> 

                                                                                               
                                                                                               
                                                                                               
                                                                                               
<!-- l. 164 --><p class='noindent'><img alt='PIC' src='/figures/2024-07-11-cd-for-ba-discretization.png' />
                                                                                               
                                                                                               
</p>
   </figure>
<!-- l. 167 --><p class='indent'>   Taking the operator \(A\) as example. Let \(\left \{ \varphi _1,\cdots ,\varphi _n \right \}\) be the basis of the finite dimensional subspace \(\mathcal {V}_h^{\mathrm {ran}}\) of the range space \(\mathcal {H}_A^{\mathrm {ran}}\). Let \(\left \{ \psi _1,\cdots ,\psi _m \right \}\)
be the basis of the finite dimensional subspace \(\mathcal {V}_h^{\mathrm {dual}}\) of the dual space \(\mathcal {H}_A^{\mathrm {dual}}\). Then the mass matrix \(\mathcal {M}_A\) is \begin{equation}  \left ( \mathcal {M}_A \right )_{ij} = \left \langle \varphi _j, \psi _i \right \rangle .  \end{equation}<a id='x1-8001r18'></a> This is the projection
of each basis function in the range space to each basis function in the dual space. The formulation of this mass
matrix involves two different spaces and the projection of basis function is evaluated via the duality pairing.
However, when the Riesz map is used as a preconditioner as in Section <a href='#x1-30002'>2<!-- tex4ht:ref: sec:org690cf71  --></a>, the mass matrix involves only
the primal space without its dual and the projection of basis function is evaluated via the inner
product.
</p>
   <h4 class='subsectionHead'><span class='titlemark'>3.3    </span> <a id='x1-90003.3'></a>Riesz map for transforming a function from dual space to range space</h4>
<!-- l. 175 --><p class='noindent'>The Riesz map \(M_A^{-1}: \mathcal {H}_A^{\mathrm {dual}} \rightarrow \mathcal {H}_{A}^{\mathrm {ran}}\) is the inverse of the identity operator from the range space to the dual space. For short, we write
it as \(M_A^{-1}: H' \rightarrow H\). The associated matrix \(\mathcal {M}_A^{-1}\) have row and column spaces as \(\left ( H_h, H_h' \right )\), which are compatible with the operator spaces.
This is different from the case when the Riesz map is used as a preconditioner.
</p><!-- l. 177 --><p class='indent'>   By following the red route in the above diagram, the matrix \(\mathcal {C}\) can be obtained as \begin{equation}  \mathcal {C}=\mathcal {B} \mathcal {M}_A^{-1} \mathcal {A}.  \end{equation}<a id='x1-9001r19'></a> Let \(u\) be a function in \(\mathcal {H}_{A}^{\mathrm {dom}}\). In the
finite dimensional subspace, its associated vector is \(\vect {u}\). Then applying \(\mathcal {C}\) to \(\vect {u}\), we obtain a vector in the weak form, i.e.
in the dual space \(\mathcal {H}_{B}^{\mathrm {dual}}\): \begin{equation}  \mathcal {B} \mathcal {M}_A^{-1} \mathcal {A} \vect {u}.  \end{equation}<a id='x1-9002r20'></a> If we further apply the Riesz matrix \(\mathcal {M}_B^{-1}\), we have the vector in the strong form, i.e. in the range
space \(\mathcal {H}_{B}^{\mathrm {ran}}\): \begin{equation}  \mathcal {M}_B^{-1} \mathcal {B} \mathcal {M}_A^{-1} \mathcal {A} \vect {u}.  \end{equation}<a id='x1-9003r21'></a>
</p><!-- l. 1 --><p class='noindent'>
</p>
   <h3 class='likesectionHead'><a id='x1-10000'></a>References</h3>
<!-- l. 1 --><p class='noindent'>
  </p><div class='thebibliography'>
  <p class='bibitem'><span class='biblabel'>
<a id='XBetckeProduct2017'></a><span class='bibsp'>   </span></span>Timo Betcke, Matthew Scroggs, and Wojciech Smigaj. Product algebras for galerkin discretisations of
  boundary integral operators and their applications. URL <a class='url' href='http://arxiv.org/abs/1711.10607'><span class='t1xtt-x-x-109'>http://arxiv.org/abs/1711.10607</span></a>.
  </p>
  <p class='bibitem'><span class='biblabel'>
<a id='XBrezisFunctional2011'></a><span class='bibsp'>   </span></span>Haim  Brezis.   <span class='p1xi-x-x-109'>Functional  Analysis,  Sobolev  Spaces  and  Partial  Differential  Equations</span>.   Springer.   ISBN
  978-0-387-70913-0 978-0-387-70914-7. doi: 10.1007/978-0-387-70914-7.
  </p>
  <p class='bibitem'><span class='biblabel'>
<a id='XKirbyFunctional2010'></a><span class='bibsp'>   </span></span>Robert C. Kirby.  From functional analysis to iterative methods.  52(2):269–293.  ISSN 0036-1445.  doi:
  10.1137/070706914.
  </p>
  <p class='bibitem'><span class='biblabel'>
<a id='XMardalPreconditioning2011'></a><span class='bibsp'>   </span></span>Kent-Andre  Mardal  and  Ragnar  Winther.    Preconditioning  discretizations  of  systems  of  partial
  differential equations. 18(1):1–40. ISSN 1099-1506. doi: 10.1002/nla.716.
</p>
                                                                                               
                                                                                               
  </div>
   <div class='footnotes'><a id='x1-5004x1'></a>
<!-- l. 139 --><p class='indent'>      <span class='footnote-mark'><a href='#fn1x0-bk' id='fn1x0'><sup class='textsuperscript'>1</sup></a></span><span class='p1xr-x-x-90'>Here we write the row space in front of the column space for a matrix as the Galerkin discretization of an operator.</span></p>    </div>

<p>Backlinks: <a href="/math/2024/11/10/adjoint-operators-in-functional-analysis.html">《Adjoint operators in functional analysis》</a></p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="FEM" /><category term="BEM" /><category term="PDE" /><category term="functional-analysis" /><summary type="html"><![CDATA[Contents  1 Riesz representation theorem (Brezis)  2 Construction of the Riesz map as a preconditioner   2.1 Riesz map to be used as a preconditioner depends on the definition of inner product   2.2 Riesz map as a baseline preconditioner  3 Construction of the Riesz map as a space adaptor   3.1 Issue of implicit range spaces in BEM   3.2 Mass matrix as space projection   3.3 Riesz map for transforming a function from dual space to range space]]></summary></entry><entry><title type="html">Domain, range and dual spaces in BEM</title><link href="https://jihuan-tian.github.io/math/2024/07/04/domain-range-and-dual-spaces-in-bem.html" rel="alternate" type="text/html" title="Domain, range and dual spaces in BEM" /><published>2024-07-04T00:00:00+08:00</published><updated>2024-07-04T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/07/04/domain-range-and-dual-spaces-in-bem</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/07/04/domain-range-and-dual-spaces-in-bem.html"><![CDATA[<p>In Galerkin FEM, we are dealing with two function spaces, the trial space \(X\) and the test space \(Y\). The test function \(v\in Y\) is directly applied to the two sides of the operator equation. If we take the Laplace problem as example, we’ll then apply the Green identity to obtain the variational form or weak form. The weak form contains an integration term \(\left( \nabla u,\nabla v \right)\), which is product of the differentials of \(v\) and \(u\). If \(u\) and \(v\) belong to \(H_0^1(\Omega)\), which means both of them as well as their first order partial derivatives are square integrable, the integration term is the \(L_2\)-inner product of the gradient of \(v\) and \(u\) in the Cartesian product space \(\left[ L_2(\Omega) \right]^d\), where \(d\) is the spatial dimension.</p>

<p>In Galerkin BEM, a boundary integral equation will be considered, which is equivalent to the original PDE. An integral operator, such as the single layer potential (SLP) integral operator \(V\), operates on the solution function \(u\) in the PDE</p>

\[\begin{equation}
(Vu)(x) = \int_{\Gamma} U^{*}(x,y) u(y) ds_y,
\end{equation}\]

<p>where \(U^{*}(x, y)\) is the fundamental solution.</p>

<p>The test function \(v\) will not be directly applied to the solution function \(u\) or its differential as in FEM, but to the result function returned from the integral operator. Let the domain space and range space of \(V\) be \(X\) and \(Z\). For example, \(V: H^{-1/2}(\Gamma) \rightarrow H^{1/2}(\Gamma)\). Obviously, the trial space is the domain space of the integral operator. The test space \(Y\) is not always the same as the domain space as in FEM. Actually, the test function should belong to the dual space \(Z'\) of the range space \(Z\) and is applied to a function in the range space via duality pairing between \(Z\) and \(Z'\). Therefore, the test function space for the integral operator \(V\) is \(Y=H^{-1/2}(\Gamma)\), which happens to be the same as the domain space \(X\). This explains why the discretized matrix associated with \(V\) is symmetric.</p>

<p>However, for the double layer integral operator \(K: H^{1/2}(\Gamma) \rightarrow H^{1/2}(\Gamma)\), the trial function space is \(H^{1/2}(\Gamma)\) and the test space is \(H^{-1/2}(\Gamma)\), which are different. Hence, the matrix for \(K\) is generally not a square matrix.</p>

<p>See also <a href="/math/2022/05/24/difference-between-interpolation-and-projection-in-fem.html">Difference between interpolation and projection in FEM</a>.</p>

<p>Backlinks: <a href="/math/2024/07/11/understanding-about-riesz-map.html">《Understanding about Riesz map》</a></p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="BEM" /><summary type="html"><![CDATA[In Galerkin FEM, we are dealing with two function spaces, the trial space \(X\) and the test space \(Y\). The test function \(v\in Y\) is directly applied to the two sides of the operator equation. If we take the Laplace problem as example, we’ll then apply the Green identity to obtain the variational form or weak form. The weak form contains an integration term \(\left( \nabla u,\nabla v \right)\), which is product of the differentials of \(v\) and \(u\). If \(u\) and \(v\) belong to \(H_0^1(\Omega)\), which means both of them as well as their first order partial derivatives are square integrable, the integration term is the \(L_2\)-inner product of the gradient of \(v\) and \(u\) in the Cartesian product space \(\left[ L_2(\Omega) \right]^d\), where \(d\) is the spatial dimension.]]></summary></entry><entry><title type="html">关于数值优化的定性理解</title><link href="https://jihuan-tian.github.io/math/2024/06/23/%E5%85%B3%E4%BA%8E%E6%95%B0%E5%80%BC%E4%BC%98%E5%8C%96%E7%9A%84%E5%AE%9A%E6%80%A7%E7%90%86%E8%A7%A3.html" rel="alternate" type="text/html" title="关于数值优化的定性理解" /><published>2024-06-23T00:00:00+08:00</published><updated>2024-06-23T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/06/23/%E5%85%B3%E4%BA%8E%E6%95%B0%E5%80%BC%E4%BC%98%E5%8C%96%E7%9A%84%E5%AE%9A%E6%80%A7%E7%90%86%E8%A7%A3</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/06/23/%E5%85%B3%E4%BA%8E%E6%95%B0%E5%80%BC%E4%BC%98%E5%8C%96%E7%9A%84%E5%AE%9A%E6%80%A7%E7%90%86%E8%A7%A3.html"><![CDATA[<p>做数值优化的过程中会遇到目标函数的连续性比较差的情况。当然，这并不是说目标函数本身不连续，而是说用来定义函数连续性、且与自变量无关的常数 C 非常小：</p>

\[\vert f(x) - f(y) \vert \leq C \Vert x - y \Vert.\]

<p>这意味着当两个函数值比较接近时，产生它们的两个自变量的值在多维优化变量空间中相距较远，从而使目标函数的优化存在一定困难。</p>

<p>从我近期的项目案例来看，造成上述目标函数连续性较差的原因可能是目标函数本身包含了多个动力学常微分方程的积分环节。动力学方程大体长这个样子：</p>

\[\hat{I}(\tau_0, T) = Cn_0 s(\frac{1}{\tau_0}, T) \cdot \exp \left[ -\frac{1}{\beta}\int_{T_0}^T s(\frac{1}{\tau_0}, T') d T' \right].\]

<p>积分本身是一种能够过滤掉高频数据的算子或操作。这一积分“滤波”过程会导致细节信息的丢失，或者也可定性理解为在从高维优化变量空间映射到目标函数值空间的过程中产生了数据压缩现象，以至于在通过数值优化方法求解反问题的过程中，不能获得充足信息，最终得到的解只会是包含多种可能的解的集合，而不是唯一解。</p>

<p>使用基于遗传算法的全局搜索加上局部基于梯度下降优化算法的组合，一方面能够将目标函数最小化至误差限以下，另一方面可以在可行空间中搜索出多个产生这样高质量拟合精度的模型参数。而若使用局部搜索算法，只能得到一个解，该解虽能保证数值误差达标，但不一定符合物理实际。</p>

<p>现实中物理系统的真实参数应是唯一的。若想从全局搜索算法得到的解集合中将这个唯一的真解找到，单凭纯数学的方法是做不到的。这就需要基于物理理论与物理实验获得的先验知识来做进一步的推断与筛选。</p>

<p>此外需注意的是，必须保证优化变量的维度亦即系统的自由度数量足够高，目标函数值才会较容易地降下来。</p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="optimization" /><summary type="html"><![CDATA[做数值优化的过程中会遇到目标函数的连续性比较差的情况。当然，这并不是说目标函数本身不连续，而是说用来定义函数连续性、且与自变量无关的常数 C 非常小：]]></summary></entry><entry><title type="html">Understanding about energy norm used in Galerkin method</title><link href="https://jihuan-tian.github.io/math/2024/06/21/understanding-about-energy-norm-in-galerkin-method.html" rel="alternate" type="text/html" title="Understanding about energy norm used in Galerkin method" /><published>2024-06-21T00:00:00+08:00</published><updated>2024-06-21T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/06/21/understanding-about-energy-norm-in-galerkin-method</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/06/21/understanding-about-energy-norm-in-galerkin-method.html"><![CDATA[<!-- l. 24 -->
<p class="indent">   In this lecture “<a href="https://youtu.be/tvHRyAu9zLc?list=PL10zZbQGXP3OyzxhnYju2RJR8j_nu9FFB&amp;t=2813">PDEs and applications</a>”, Prof. Hiptmair mentioned that the blow up of Galerkin
matrix condition number is related to the intrinsic instability of the boundary element basis in
the <span class="p1xb-x-x-109">energy norm</span>. On the other hand, the discretization of the operator in the \(V\)-norm is perfectly
stable.
</p>
<!-- l. 26 -->
<p class="indent">   In (<a href="#XBrennerMathematical2009">Brenner</a>, page 4), <span class="p1xb-x-x-109">energy inner product </span>is defined as the symmetric bilinear form \(a(\cdot ,\cdot )\). For a general second
order PDE, the bilinear form is 

$$
\begin{equation}  \label {eq:bilinear-form-2nd-order-pde} a(u,v):= \sum _{i,j=1}^d\int _{\Omega }a_{ji}(x)\frac {\pdiff u}{\pdiff x_{i}}\frac {\pdiff v}{\pdiff x_{j}}\intd x + \int _{\Omega }a_0uv \intd x.  \end{equation}
$$

<a id="x1-1001r1"></a>
</p>
<!-- l. 35 -->
<p class="indent">   The symmetry of the bilinear form is due to the self-adjointness of the differential operator \(L\)


$$
\begin{equation}  \label {eq:diff-operator-2nd-order} (Lu)(x):= -\sum _{i,j=1}^d \frac {\pdiff }{\pdiff x_j} \left [ a_{ji}(x)\frac {\pdiff u}{\pdiff x_{i}} \right ] + a_0(x)u(x).  \end{equation}
$$

<a id="x1-1002r2"></a>
</p>
<!-- l. 43 -->
<p class="indent">   Then in (<a href="#XBrennerMathematical2009">Brenner</a>, page 5), <span class="p1xb-x-x-109">energy norm </span>is defined via the bilinear form: 

$$
\begin{equation}  \label {eq:energy-norm} \lVert v \rVert _E = \sqrt {a(v,v)} \quad \forall v\in X.  \end{equation}
$$

<a id="x1-1003r3"></a> Meanwhile, \(a(\cdot ,\cdot )\) can also be considered
as an inner product on \(X\). Cauchy-Schwartz inequality also holds for such inner product and energy norm: 

$$
\begin{equation}  \lvert a(v,w) \rvert \leq \lVert v \rVert _E \lVert w \rVert _E.  \end{equation}
$$

<a id="x1-1004r4"></a> On the
other hand, the original norm and inner product on the space \(X\) can still be adopted, which has nothing to do with
the differential operator \(L\).
</p>
<!-- l. 54 -->
<p class="indent">   Take the \(H^1(\Omega )\) Sobolev space as example, its standard norm is 

$$
\begin{equation}  \| u \|_{H^1(\Omega )} = \left ( \| u \|_{L^2(\Omega )}^2 + \| \nabla u \|_{(L^2(\Omega ))^n}^2 \right )^{1/2}.  \end{equation}
$$

<a id="x1-1005r5"></a> If the differential operator is \(L=-\Delta + I\), its corresponding
bilinear form is \(a(u,v) = \left \langle \nabla u, \nabla v \right \rangle + \left \langle u,v \right \rangle \). Then the norm \(\lVert u \rVert _{H^1(\Omega )}\) is equivalent to \(\sqrt {a(u,u)}\), so it is an energy norm.
</p>
<!-- l. 60 -->
<p class="indent">   If the differential operator is \(L=-\Delta \), the bilinear form is \(a(u,v) = \left \langle \nabla u, \nabla v \right \rangle \). Then \(a(u,u) = \left \langle \nabla u, \nabla u \right \rangle \) and the energy norm \(\sqrt {a(u,u)}\) is actually the semi-norm
\(\lvert u \rvert _{H^1(\Omega )}\).
</p>
<!-- l. 62 -->
<p class="indent">   In addition, because \(H^1(\Omega )\) is a subspace of \(L_2(\Omega )\), we can still assign the \(L_2\)-norm to \(H^1(\Omega )\). Of course, this norm is not the energy
norm.
</p>
<!-- l. 64 -->
<p class="indent">   According to the definition of the standard \(H^1\)-norm, when the differential operator is \(L=-\Delta + I\), both \(L_2\)-norm and
\(H^1\)-semi-norm is smaller than the energy norm. When the differential operator is \(L=-\Delta \), while the \(H^1\)-semi-norm is
the same as the energy norm, whether \(L_2\)-norm is smaller than the energy norm seems unknown.
However, in (<a href="#XBrennerMathematical2009">Brenner</a>, page 6), it is proved generally that the \(L_2\)-norm is much smaller than the energy
norm.
</p>
<!-- l. 66 -->
<p class="indent">   N.B. The Ritz-Galerkin method ensures that the solution error is optimal in the energy norm. However, if
we use \(L_2\)-norm to measure the error, since for a same error function, its value is smaller than the
energy norm, the method is actually more stable. This explains the statement of Prof. Hiptmair at the
                                                                                               
                                                                                               
beginning.
</p>
<!-- l. 68 -->
<p class="indent">   Finally, we can also see that the usually adopted function space norm and inner product on \(X\) only focus on the
continuity and smoothness of the function itself. The energy norm and inner product defined via
the bilinear form on \(X\) is related to the differential operator, or rather the physical property of the
system.
</p>
<!-- l. 1 -->
<p class="noindent">
</p>
<h3 class="likesectionHead"><a id="x1-2000"></a>References</h3>
<!-- l. 1 -->
<p class="noindent">
  </p>
<div class="thebibliography">
  <p class="bibitem"><span class="biblabel">
<a id="XBrennerMathematical2009"></a><span class="bibsp">   </span></span>Susanne C. Brenner. <span class="p1xi-x-x-109">The Mathematical Theory of Finite Element Methods</span>. Springer New York, 3 edition.
  ISBN 978-1-4419-2611-1. URL <a class="url" href="https://book.douban.com/subject/4219448/#"><span class="t1xtt-x-x-109">https://book.douban.com/subject/4219448/#</span></a>.
</p>
  </div>

<p>Backlinks: <a href="/math/2024/11/06/spectral-equivalence.html">《Spectral equivalence》</a></p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="PDE" /><summary type="html"><![CDATA[In this lecture “PDEs and applications”, Prof. Hiptmair mentioned that the blow up of Galerkin matrix condition number is related to the intrinsic instability of the boundary element basis in the energy norm. On the other hand, the discretization of the operator in the \(V\)-norm is perfectly stable. In (Brenner, page 4), energy inner product is defined as the symmetric bilinear form \(a(\cdot ,\cdot )\). For a general second order PDE, the bilinear form is]]></summary></entry><entry><title type="html">Fundamental theorems in PDE theory</title><link href="https://jihuan-tian.github.io/math/2024/06/19/fundamental-theorems-in-pde-theory.html" rel="alternate" type="text/html" title="Fundamental theorems in PDE theory" /><published>2024-06-19T00:00:00+08:00</published><updated>2024-06-19T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/06/19/fundamental-theorems-in-pde-theory</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/06/19/fundamental-theorems-in-pde-theory.html"><![CDATA[<ul>
  <li><a href="#org60d4829">Riesz representation theorem</a></li>
  <li><a href="#org650a387">Lax-Milgram Lemma</a></li>
  <li><a href="#org96b6675">Cea’s Lemma</a></li>
  <li><a href="#org4dffff6">Strang Lemma</a></li>
</ul>

<p><a id="org60d4829"></a></p>

<h1 id="riesz-representation-theorem">Riesz representation theorem</h1>

<p>Let \(X\) be a Hilbert space and \(X'\) be its dual space with respect to the duality pairing \(\left\langle \cdot,\cdot \right\rangle\). Let b\(\left\langle \cdot,\cdot \right\rangle_X\) be the inner product on \(X\). The dual space \(X'\) is the set of all linear and bounded functionals on \(X\). For any \(f\in X'\), there exists an unique \(u\in X\) such that applying \(f\) to any \(v\in X\), i.e. its duality pairing with \(v\), is equal to the inner product of \(u\) and \(v\), i.e.</p>

\[\begin{equation}
\left\langle f,v \right\rangle = \left\langle u,v \right\rangle_X.
\end{equation}\]

<p>Furthermore, \(\lVert u \rVert_X = \lVert f \rVert_{X'}\), where \(\lVert \cdot \rVert_{X'}\) is the operator norm on \(X'\).</p>

<p>Not only states this theorem the equivalence between the Hilbert space \(X\) and its dual space \(X'\), it can also be understood as an existence and uniqueness theorem for the solution of the above simple variational problem, i.e. find \(u\in X\) such that \(\left\langle u,v \right\rangle_X = \left\langle f,v \right\rangle\) for any \(v\in X\).</p>

<p><a id="org650a387"></a></p>

<h1 id="lax-milgram-lemma">Lax-Milgram Lemma</h1>

<p>This is an existence and uniqueness theorem for the solution of an operator equation as well as its equivalent variational formulation, whose key feature is governed by the operator \(A: X \rightarrow X'\) or the corresponding bilinear form \(a(\cdot,\cdot)\).</p>

<p>Operator equation is</p>

\[\begin{equation}
Au = f
\end{equation}\]

<p>and its variational formulation is</p>

\[\begin{equation}
a(u,v) = \left\langle Au,v \right\rangle = \left\langle f,v \right\rangle \quad \forall v\in X.
\end{equation}\]

<p>The boundedness and \(X\)-ellipticity of \(A\) ensures the existence and uniqueness of the solution \(u\). N.B. Ellipticity of an operator is defined with respect to with its domain.</p>

<p><a id="org96b6675"></a></p>

<h1 id="ceas-lemma">Cea’s Lemma</h1>

<p>It describes the stability estimate and quasi-optimal error estimate for the solution of the <strong>discrete</strong> variational problem obtained from the Galerkin-Bubnov method.</p>

\[\begin{equation}
  \label{eq:variational-problem-finite-dim}
  \left\langle Au_M,v_M \right\rangle = \left\langle f,v_M \right\rangle \quad \forall v_M\in X_M,
\end{equation}\]

<p>where \(X_M\) is the finite dimensional approximation of \(X\). N.B. The said quasi-optimal error estimate means the error measured by \(X\)-norm between the true solution \(u\) and the finite dimensional solution \(u_M\) is controlled by the infimum of the space approximation error \(\inf_{v_M\in X_M} \lVert u - v_M \rVert_X\). This means as long as the finite dimensional space \(X_M\) is selected good enough with respect to the original Sobolev space \(X\), the solution error can be small enough. This ensures it is meaningful to obtain a better result by increasing the mesh density or polynomial order of the finite element.</p>

<p>The adopted finite dimensional space \(X_M\) leads to the discretization error, which can be considered as a perturbation to the original continuous problem. As regards spatial finite dimension, it is achieved by a triangulation of the domain. As regards finite dimension caused by finite element, it is achieved by a set of polynomials which forms a subspace of the Sobolev space (This is the so-called conforming finite element and the wording <strong>conforming</strong> should be differentiated from that used in <strong>conforming mesh</strong>. The latter means there are no hanging nodes in the mesh).</p>

<p><a id="org4dffff6"></a></p>

<h1 id="strang-lemma">Strang Lemma</h1>

<p>Not only the trial and test function spaces (usually Sobolev spaces) can be approximated by finite dimensional spaces via the Galerkin-Bubnov method, the linear form on the right hand side of the variational problem in Equation \ref{eq:variational-problem-finite-dim} can also be approximated. For example, if the linear functional \(f\in X'\) on the right hand side is represented as \(Bg\), where \(B: Y \rightarrow X'\) is a bounded linear operator and \(g\in Y\), the finite dimensional approximation \(g_N\) of \(g\) leads to another source of perturbation to the PDE. Strang Lemma then characterizes the quasi-optimal error estimate in this case, i.e. the new solution error is controlled by a scaled sum of the space approximation error \(\inf_{v_M\in X_M} \lVert u - v_M \rVert_X\) and the right hand side data error \(\lVert g - g_N \rVert_Y\).</p>

<p>Backlinks: <a href="/math/2024/11/10/adjoint-operators-in-functional-analysis.html">《Adjoint operators in functional analysis》</a>, <a href="/math/2024/11/11/ellipticity-of-boundary-integral-operators.html">《Ellipticity of boundary integral operators》</a></p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="PDE" /><summary type="html"><![CDATA[Riesz representation theorem Lax-Milgram Lemma Cea’s Lemma Strang Lemma]]></summary></entry><entry><title type="html">Different types of convergence</title><link href="https://jihuan-tian.github.io/math/2024/06/08/different-types-of-convergence.html" rel="alternate" type="text/html" title="Different types of convergence" /><published>2024-06-08T00:00:00+08:00</published><updated>2024-06-08T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/06/08/different-types-of-convergence</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/06/08/different-types-of-convergence.html"><![CDATA[<p><img src="/figures/2024-06-08-10-24-different-types-of-convergence.png" alt="Summary of different types of convergence in functional analysis" /></p>

<p>In functional analysis, several types of convergence are defined, namely,</p>

<ul>
  <li>strong convergence for elements in normed linear space.</li>
  <li>weak convergence for elements in normed linear space, which is defined via the assistance of the dual space.</li>
  <li>weak-​* convergence for linear functionals in the strong dual space of a normed linear space.</li>
  <li>pointwise convergence for functions</li>
  <li>uniform convergence for functions</li>
</ul>

<p>Their definitions and differences are summarized below.</p>

<ol>
  <li>
    <p><strong>Definition</strong> (Strong convergence) Let \(X\) be a normed linear space and \((x_l)_{l \in \mathbb{N}}\) be a sequence in \(X\). Then \((x_l)_{l \in \mathbb{N}}\) converges (strongly) to \(x \in X\) if</p>

\[\begin{equation}
\lim_{l \rightarrow \infty} \norm{x_l - x}_X = 0.
\end{equation}\]

    <p>It can be seen that the strong convergence is just the convergence with respect to the “distance between points”, or more generally, the so-called “norm” defined for a linear space, which is what we have been familiar with in fundamental calculus.</p>
  </li>
  <li>
    <p><strong>Definition</strong> (Weak convergence) Let \(X\) be a Banach space and \(X'\) be its dual space. The sequence \((x_l)_{l \in \mathbb{N}}\) in \(X\) converges weakly to \(x \in X\) if</p>

\[\begin{equation}
\lim_{l \rightarrow \infty} \abs{f(x_l) - f(x)} = 0 \quad (\forall f \in X').
\end{equation}\]

    <p>We can see that the convergence here is called <strong>weak</strong>, because it is not directly based on point distance in the original space \(X\), but the evaluation of an arbitrary functional in the dual space on the sequence.</p>

    <p>It is easy and natural to see that the strong convergence implies weak convergence because of the continuity of the linear functional \(f \in X'\):</p>

\[\begin{equation}
\abs{f(x_l) - f(x)} = \abs{f(x_l - x)} \leq \norm{f}_{X'} \norm{x_l - x}_X.
\end{equation}\]
  </li>
  <li>
    <p><strong>Definition</strong> (Pointwise convergence) Let \(X\) and \(Y\) be normed spaces. The sequence of bounded linear operators \((T_l)_{l \in \mathbb{N}} \subset L(X, Y)\) converges to \(T \in L(X, Y)\) if</p>

\[\begin{equation}
\lim_{l \rightarrow \infty} \norm{T_l x - T x}_Y = 0 \quad (\forall x \in X).
\end{equation}\]

    <p>The pointwise convergence is used to describe the convergence of operators at each point in \(X\). A more strict convergence for operators is <strong>uniform convergence</strong>, which means the convergence speeds of \((T_l x)_{l \in \mathbb{N}}\) at different points \(x\) in \(X\) are comparable. It is also easy to see that the strong convergence of \((T_l)_{l \in \mathbb{N}}\) implies pointwise convergence:</p>

\[\begin{equation}
\lVert T_lx - Tx \rVert_Y = \lVert (T_l - T)x \rVert_Y \leq \lVert T_l - T \rVert_{L(X,Y)} \lVert x \rVert_X.
\end{equation}\]
  </li>
  <li>
    <p><strong>Definition</strong> (Weak-​* convergence) Let \(X_s'\) be the strong dual space of the normed linear space \(X\). The linear functional sequence \((T_l)_{l \in \mathbb{N}}\) converges to \(T\) in \(X_s'\) if</p>

\[\begin{equation}
\lim_{l \rightarrow \infty} \abs{T_l x - T x} = 0 \quad (\forall x \in X).
\end{equation}\]

    <p>The weak-​* convergence can be considered as a special case of pointwise convergence with the difference that the linear operators become linear functionals and the dual space \(X'\) of \(X\) is assigned with the strong topology.</p>
  </li>
</ol>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="functional-analysis" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Understanding about isomorphism</title><link href="https://jihuan-tian.github.io/math/2024/06/08/understanding-about-isomorphism.html" rel="alternate" type="text/html" title="Understanding about isomorphism" /><published>2024-06-08T00:00:00+08:00</published><updated>2024-06-08T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/06/08/understanding-about-isomorphism</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/06/08/understanding-about-isomorphism.html"><![CDATA[<p>An isomorphism is a bijective mapping between two spaces, which preserves the structure of and allowed operations in the space. Under different scenarios, the said structure can be different, for example:</p>

<ul>
  <li>Metric space: the metric, i.e. distance → isometry</li>
  <li>Group: multiplication → bijective group homomorphism</li>
  <li>Vector space: addition of vectors and product of a scalar value and a vector</li>
  <li>Topological space: openness of the sets → homeomorphism</li>
  <li>In differential geometry
    <ul>
      <li>When differentiability is appended to the forward mapping → differentiable homeomorphism</li>
      <li>When differentiability is appended to both forward and inverse mapping → diffeomorphism</li>
    </ul>
  </li>
</ul>

<p>With this concept, two spaces can be identified in the sense that operations on corresponding points in these two spaces lead to same effects, while we do not care the exact or numerical representation of a point in these spaces. Of course, there is no need to do this: once the features of one space have been studied and clarified, the other space is automatically known to us.</p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="functional-analysis" /><summary type="html"><![CDATA[An isomorphism is a bijective mapping between two spaces, which preserves the structure of and allowed operations in the space. Under different scenarios, the said structure can be different, for example:]]></summary></entry><entry><title type="html">Understanding about ellipticity of operators</title><link href="https://jihuan-tian.github.io/math/2024/06/07/understanding-about-ellipticity-of-operators.html" rel="alternate" type="text/html" title="Understanding about ellipticity of operators" /><published>2024-06-07T00:00:00+08:00</published><updated>2024-06-07T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/06/07/understanding-about-ellipticity-of-operators</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/06/07/understanding-about-ellipticity-of-operators.html"><![CDATA[<p>Let \(X\) be a Hilbert space and \(X'\) be its dual space. For a bounded linear operator \(A: X \rightarrow X'\), its ellipticity condition is defined as</p>

\[\begin{equation}
  \label{eq:ellipticity-condition}
  \left\langle Av, v \right\rangle \geq c_1^A \lVert v \rVert_X^2 \quad \forall v \in X,
\end{equation}\]

<p>where \(\left\langle \cdot,\cdot \right\rangle\) is the duality pairing.</p>

<p>Now let’s compare this formulation with its discrete counterpart. Let \(\mathscr{A}\) be the matrix in \(\mathbb{R}^{n\times n}\) corresponding to \(A\) which is discretized via the Galerkin method. Let \(x\) be the vector in \(\mathbb{R}^n\) related to \(v\). Assume \(\mathscr{A}\) has \(n\) eigenvalues \(\lambda_1,\cdots,\lambda_n\) and \(n\) eigenvectors \(v_1,\cdots,v_n\) (N.B. Even though the multiplicity of some of the eigenvalues may be larger than one, for the same number of eigenvectors spanning its eigenspace, we can still explicitly use different symbols to represent their same eigenvalue. Therefore, we allow duplicated values in the list \(\lambda_1,\cdots,\lambda_n\). Because the sum of the multiplicity of all eigenvalues is \(n\) and all the eigenspaces are orthogonal to each other, we can choose those eigenvectors \(v_1,\cdots,v_n\) to form an orthonormal set. Then any vector \(x\) in \(\mathbb{R}^n\) can be expanded by this basis:</p>

\[\begin{equation}
x = \sum_{i=1}^n c_i v_i.
\end{equation}\]

<p>And the duality pairing can be represented as</p>

\[\begin{equation}
  x^{\mathrm{T}} \mathscr{A} x = \left( \sum_{i=1}^n c_iv_i^{\mathrm{T}} \right)
  \mathscr{A} \left( \sum_{j=1}^n c_jv_j \right) = \sum_{i,j} c_ic_j v_i^{\mathrm{T}}
  \mathscr{A} v_j = \sum_{i,j} c_ic_j \lambda_j (v_i,v_j),
\end{equation}\]

<p>where \((\cdot,\cdot)\) is the inner product in \(\mathbb{R}^n\). Because \(v_1,\cdots,v_n\) are orthonormal, we have \((v_i,v_j) =\delta_{ij}\). Hence,</p>

\[\begin{equation}
x^{\mathrm{T}} \mathscr{A} x = \sum_{i=1}^n \lambda_i c_i^2.
\end{equation}\]

<p>If \(\mathscr{A}\) is positive definite, the solution to the linear system \(Ax = b\) exists and is unique. Since all eigenvalues are larger than zero, we let \(\lambda_{\min}\) be the minimum and then have</p>

\[\begin{equation}
\left( \mathscr{A}x, x \right) = x^{\mathrm{T}} \mathscr{A} x \geq \lambda_{\min} \sum_{i=1}^n c_i^2 = \lambda_{\min} \lVert x \rVert_{\mathbb{R}^n}^2.
\end{equation}\]

<p>We can see that this discrete formulation is consistent with the ellipticity condition. The ellipticity constant \(c_1^A\) of the bounded linear operator \(A\) corresponds to the minimum eigenvalue of the positive definite matrix \(\mathscr{A}\). That’s why the ellipticity condition is the key in the Lax-Milgram Lemma, which governs the existence and uniqueness of the solution for a PDE formed by \(A\).</p>

<p>Backlinks: <a href="/math/2024/11/06/spectral-equivalence.html">《Spectral equivalence》</a>, <a href="/math/2024/11/11/ellipticity-of-boundary-integral-operators.html">《Ellipticity of boundary integral operators》</a></p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="PDE" /><summary type="html"><![CDATA[Let \(X\) be a Hilbert space and \(X'\) be its dual space. For a bounded linear operator \(A: X \rightarrow X'\), its ellipticity condition is defined as]]></summary></entry><entry><title type="html">Schur complement</title><link href="https://jihuan-tian.github.io/math/2024/04/28/schur-complement.html" rel="alternate" type="text/html" title="Schur complement" /><published>2024-04-28T00:00:00+08:00</published><updated>2024-04-28T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/04/28/schur-complement</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/04/28/schur-complement.html"><![CDATA[<p>When there are two variables in a PDE to be solved, the system matrix for the discretized variational formulation or weak form is a \(2\times 2\) block matrix instead of a single block,</p>

\[\begin{equation}
  \begin{pmatrix}
    M_{11} &amp; M_{12} \\
    M_{21} &amp; M_{22}
  \end{pmatrix}
  \begin{pmatrix}
    u \\
    w
  \end{pmatrix} =
  \begin{pmatrix}
    f_1 \\
    f_2
  \end{pmatrix}.
\end{equation}\]

<p>Such a system matrix appears in the boundary integral equation for the Laplace problem with mixed boundary condition:</p>

\[\begin{equation}
\begin{pmatrix}
  \gamma_0^{\rm int} u \\
  \gamma_1^{\rm int} u
\end{pmatrix} =
\begin{pmatrix}
  (1-\sigma) I - K &amp; V \\
  D &amp; \sigma I + K'
\end{pmatrix}
\begin{pmatrix}
  \gamma_0^{\rm int} u \\
  \gamma_1^{\rm int} u
\end{pmatrix} +
\begin{pmatrix}
  N_0 f \\
  N_1 f
\end{pmatrix}.
\end{equation}\]

<p>Schur complement is often used to solve this equation system.</p>

<ol>
  <li>
    <p>Multiply the equation in first row with \(-M_{21}M_{11}^{-1}\) and add the result to the second row.</p>

\[\begin{equation}
  \begin{pmatrix}
    M_{11} &amp; M_{12} \\
    0 &amp; M_{22} - M_{21}M_{11}^{-1}M_{12}
  \end{pmatrix}
  \begin{pmatrix}
    u \\ w
  \end{pmatrix} =
  \begin{pmatrix}
    f_1 \\ f_2 - M_{21}M_{11}^{-1}f_1
  \end{pmatrix}.
\end{equation}\]

    <p>Here, \(M_{22} - M_{21}M_{11}^{-1}M_{12}\) is called the Schur complement of \(M\) with respect to the block \(M_{11}\).</p>
  </li>
  <li>
    <p>Solve the following equation for \(w\).</p>

\[\begin{equation}
  \left( M_{22} - M_{21}M_{11}^{-1}M_{12} \right) w = f_2 - M_{21}M_{11}^{-1} f_1.
\end{equation}\]
  </li>
  <li>
    <p>Solve the following equation for \(u\).</p>

\[\begin{equation}
\label{eq:2}
M_{11} u = f_1 - M_{12}w.
\end{equation}\]
  </li>
</ol>

<p>We can see that the Schur complement method is actually the Gauss-Seidel (Gauss elimination) method for a \(2\times 2\) matrix, or the method for solving linear equations of two variables which we once learnt in middle school. <em>Sometimes a high level mathematical method, such as the Schur complement used in PDE, tallies with an elementary method, both of which bear a same idea or spirit.</em></p>

<p>The benefit of this method is by decomposing the original large system matrix into two smaller problems, less memory is needed and the iterative solver usually converges faster.</p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="BEM" /><summary type="html"><![CDATA[When there are two variables in a PDE to be solved, the system matrix for the discretized variational formulation or weak form is a \(2\times 2\) block matrix instead of a single block,]]></summary></entry><entry><title type="html">关于行列式的理解</title><link href="https://jihuan-tian.github.io/math/2024/03/23/%E5%85%B3%E4%BA%8E%E8%A1%8C%E5%88%97%E5%BC%8F%E7%9A%84%E7%90%86%E8%A7%A3.html" rel="alternate" type="text/html" title="关于行列式的理解" /><published>2024-03-23T00:00:00+08:00</published><updated>2024-03-23T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2024/03/23/%E5%85%B3%E4%BA%8E%E8%A1%8C%E5%88%97%E5%BC%8F%E7%9A%84%E7%90%86%E8%A7%A3</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2024/03/23/%E5%85%B3%E4%BA%8E%E8%A1%8C%E5%88%97%E5%BC%8F%E7%9A%84%E7%90%86%E8%A7%A3.html"><![CDATA[<p>\(k\) 阶行列式可以看作是处于对偶空间中的 \(k\) 阶外微分形式在 \(k\) 重切向量上的投影。其中，\(k\) 阶外微分形式实际上是 \(k\) 重对偶切向量的反对称张量积（张量积的一种特殊形式）。上述投影操作是将这一组 <strong>固定</strong> 顺序的 \(k\) 个对偶切向量分别作用在以不同方式排列（permutation）的这一组 \(k\) 个切向量上。若将以上给定的 \(k\) 个对偶切向量的索引作为行列式中元素的行索引，\(k\) 个切向量的初始索引（排列操作之前）作为行列式中元素的列索引，则 \(k\) 阶外微分形式在 \(k\) 重切向量上的投影与相应行列式值一致。</p>

<p>在 \(k\) 阶外微分形式于流形上的积分中，每一个无穷小的积分微元相当于将此 \(k\) 阶外微分形式投影在由该处切空间的基所构成的 \(k\) 重切向量上（该基并不需要正交或者规一化）。特别地，对于 1 阶外微分形式，例如电场强度，该投影的结果或者行列式的值相当于经典矢量微积分理论中在笛卡尔坐标系下电场强度矢量在路径 \(dl\) 上的投影。对于 2 阶外微分形式，例如磁感应强度，该投影的结果或者行列式的值相当于磁感应强度矢量在面元 \(dS\) 上的通量。对于 3 阶外微分形式，例如电荷密度，该投影的结果或者行列式的值相当于电荷密度在体积元 \(dV\) 上的总量。</p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="differential-geometry" /><summary type="html"><![CDATA[\(k\) 阶行列式可以看作是处于对偶空间中的 \(k\) 阶外微分形式在 \(k\) 重切向量上的投影。其中，\(k\) 阶外微分形式实际上是 \(k\) 重对偶切向量的反对称张量积（张量积的一种特殊形式）。上述投影操作是将这一组 固定 顺序的 \(k\) 个对偶切向量分别作用在以不同方式排列（permutation）的这一组 \(k\) 个切向量上。若将以上给定的 \(k\) 个对偶切向量的索引作为行列式中元素的行索引，\(k\) 个切向量的初始索引（排列操作之前）作为行列式中元素的列索引，则 \(k\) 阶外微分形式在 \(k\) 重切向量上的投影与相应行列式值一致。]]></summary></entry><entry><title type="html">Formulation of complex structure</title><link href="https://jihuan-tian.github.io/math/2023/08/08/formulation-of-complex-structure.html" rel="alternate" type="text/html" title="Formulation of complex structure" /><published>2023-08-08T00:00:00+08:00</published><updated>2023-08-08T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2023/08/08/formulation-of-complex-structure</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2023/08/08/formulation-of-complex-structure.html"><![CDATA[<p>In the <a href="https://youtu.be/FRvhgkGKfSM?list=PL9_jI1bdZmz0hIrNCMQW1YmZysAiIYSSS&amp;t=2762">CMU DDG course</a>, the symbol adopted for the complex structure, i.e. the operation for rotating by \(\frac{\pi}{2}\) should be \(\mathcal{J}\) instead of \(\mathcal{J}_f\). The latter should be the Jacobian matrix, which satisfies \(df(X) = \mathcal{J}_f X\).</p>

<p>Then according to the definition of complex structure:</p>

\[df(\mathcal{J} X) := N \times df(X),\]

<p>the left hand side is equal to \(\mathcal{J}_f \mathcal{J} X\) and the right hand side is equal to \(\hat{N} \mathcal{J}_f X\). Multiply both sides by \(\mathcal{J}_f^T\)</p>

\[\mathcal{J}_f^T \mathcal{J}_f \mathcal{J} X = \mathcal{J}_f^T \hat{N} \mathcal{J}_f X.\]

<p>Since \(X\) is arbitrary, we have</p>

\[\mathcal{J} = \left( \mathcal{J}_f^T \mathcal{J}_f \right)^{-1} \left( \mathcal{J}_f^T \hat{N} \mathcal{J}_f \right).\]]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="differential-geometry" /><category term="cmu-ddg" /><category term="snippet" /><summary type="html"><![CDATA[In the CMU DDG course, the symbol adopted for the complex structure, i.e. the operation for rotating by \(\frac{\pi}{2}\) should be \(\mathcal{J}\) instead of \(\mathcal{J}_f\). The latter should be the Jacobian matrix, which satisfies \(df(X) = \mathcal{J}_f X\).]]></summary></entry><entry><title type="html">The relation between Riemannian metric, area form and complex structure</title><link href="https://jihuan-tian.github.io/math/2023/08/08/the-relation-between-riemannian-metric-area-form-and-complex-structure.html" rel="alternate" type="text/html" title="The relation between Riemannian metric, area form and complex structure" /><published>2023-08-08T00:00:00+08:00</published><updated>2023-08-08T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2023/08/08/the-relation-between-riemannian-metric-area-form-and-complex-structure</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2023/08/08/the-relation-between-riemannian-metric-area-form-and-complex-structure.html"><![CDATA[<p>In the <a href="https://youtu.be/FRvhgkGKfSM?list=PL9_jI1bdZmz0hIrNCMQW1YmZysAiIYSSS&amp;t=3288">CMU DDG course</a>, for an immersed surface defined by the map \(f\) which is a vector-valued 0-form, the relation between Riemannian metric \(g\), area form \(dA\) and complex structure \(\mathcal{J}\) is given as</p>

\[g(X,Y) = dA(X, \mathcal{J}Y).\]

<p>Here \(X\) and \(Y\) are vectors in the local coordinate chart assigned to the surface patch. \(g(\cdot,\cdot)\) is a covariant rank-2 tensor, which returns the inner product of the two pushed forward vectors related to its two operands, i.e.</p>

\[g(X, Y) = \left\langle df(X), df(Y) \right\rangle.\]

<p>\(dA(\cdot,\cdot)\) is the area form, which returns the absolute area of the parallelogram spanned by the two pushed forward vectors related to its two operands.</p>

<p>The complex structure \(\mathcal{J}\) is defined as rotating the pushed forward vector by \(\frac{\pi}{2}\) around the normal vector:</p>

\[df(\mathcal{J}X) := N \times df(X).\]

<p>According the rule for the wedge product of two vector-valued 1-forms, we have</p>

\[\left( df \wedge df \right)(X, Y) = df(X)\times df(Y) - df(Y)\times df(X) = 2 df(X)\times df(Y) = 2 N dA(X, Y),\]

<p>where \(N\) is the unit normal vector.</p>

<p>Take inner product with respect to \(N\) at both sides of the above equation, we have</p>

\[\left\langle N, df(X) \times df(Y) \right\rangle = dA(X, Y).\]

<p>Then the right hand side of the first equation becomes</p>

\[\rhs = \left\langle N, df(X) \times df(\mathcal{J}Y) \right\rangle = \left\langle N, df(X) \times \left( N \times df(Y) \right) \right\rangle.\]

<p>Using the mixed product identity \(a\cdot(b\times c) = b\cdot(c\times a) = -b\cdot(a\times c)\), we have</p>

\[\rhs = -\left\langle df(X), N\times(N\times df(Y)) \right\rangle.\]

<p>Note that applying \(N\times\) two times is equivalent to rotating the original pushed forward vector by \(\pi\), hence</p>

\[\rhs = -\left\langle df(X), -df(Y) \right\rangle = \left\langle df(X), df(Y) \right\rangle,\]

<p>which is equal to the left hand side \(g(X,Y)\) of the first equation.</p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="differential-geometry" /><category term="cmu-ddg" /><category term="snippet" /><summary type="html"><![CDATA[In the CMU DDG course, for an immersed surface defined by the map \(f\) which is a vector-valued 0-form, the relation between Riemannian metric \(g\), area form \(dA\) and complex structure \(\mathcal{J}\) is given as]]></summary></entry><entry><title type="html">Visualization of Hilbert curve in Asymptote</title><link href="https://jihuan-tian.github.io/math/2023/08/01/visualization-of-hilbert-curve-in-asymptote.html" rel="alternate" type="text/html" title="Visualization of Hilbert curve in Asymptote" /><published>2023-08-01T00:00:00+08:00</published><updated>2023-08-01T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2023/08/01/visualization-of-hilbert-curve-in-asymptote</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2023/08/01/visualization-of-hilbert-curve-in-asymptote.html"><![CDATA[<p>I’ve written the following Asymptote script which produces Hilbert curve on a 2D grid. Hilbert curve is useful in the sequence partitioning of parallel processing of hierarchical matrices.</p>

<pre><code class="language-asymptote">unitsize(1mm);

real log2(real x) {static real log2=log(2); return log(x)/log2;}

// Generate a 2D grid
path[] grid2d(real x_spacing, real y_spacing, int x_cell_num, int y_cell_num)
{
  path [] grid;

  for (int i = 0; i &lt; x_cell_num; ++i)
    {
      for (int j = 0; j &lt; y_cell_num; ++j)
        {
          grid = grid^^box((i * x_spacing, j * y_spacing), (i * x_spacing + x_spacing, j * y_spacing + y_spacing));
        }
    }

  return grid;
}

// Draw Hilbert curve for the grid.
// Use 0~3 to represent block types from A to D.
pair draw_hilbert_for_grid(pen mypen, pair previous_segment_end, pair cell_center, real cell_size, int block_type, int max_level, int current_level, int curve_tension)
{
  if (current_level == max_level)
    {
      if (block_type == 0)
        {
          if (previous_segment_end != (0, 0))
            {
              draw(previous_segment_end..tension atleast curve_tension ..cell_center + (-cell_size / 2.0, -cell_size / 2.0)..tension atleast curve_tension ..cell_center + (-cell_size / 2.0, cell_size / 2.0)..tension atleast curve_tension ..cell_center + (cell_size / 2.0, cell_size / 2.0)..tension atleast curve_tension ..cell_center + (cell_size / 2.0, -cell_size / 2.0), p = mypen);
            }
          else
            {
              draw(cell_center + (-cell_size / 2.0, -cell_size / 2.0)..tension atleast curve_tension ..cell_center + (-cell_size / 2.0, cell_size / 2.0)..tension atleast curve_tension ..cell_center + (cell_size / 2.0, cell_size / 2.0)..tension atleast curve_tension ..cell_center + (cell_size / 2.0, -cell_size / 2.0), p = mypen);
            }

          return cell_center + (cell_size / 2.0, -cell_size / 2.0);
        }
      else
        {
          if (block_type == 1)
            {
              if (previous_segment_end != (0, 0))
                {
                  draw(previous_segment_end..tension atleast curve_tension ..cell_center + (-cell_size / 2.0, -cell_size / 2.0)..tension atleast curve_tension ..cell_center + (cell_size / 2.0, -cell_size / 2.0)..tension atleast curve_tension ..cell_center + (cell_size / 2.0, cell_size / 2.0)..tension atleast curve_tension ..cell_center + (-cell_size / 2.0, cell_size / 2.0), p = mypen);
                }
              else
                {
                  draw(cell_center + (-cell_size / 2.0, -cell_size / 2.0)..tension atleast curve_tension ..cell_center + (cell_size / 2.0, -cell_size / 2.0)..tension atleast curve_tension ..cell_center + (cell_size / 2.0, cell_size / 2.0)..tension atleast curve_tension ..cell_center + (-cell_size / 2.0, cell_size / 2.0), p = mypen);
                }

              return cell_center + (-cell_size / 2.0, cell_size / 2.0);
            }
          else
            {
              if (block_type == 2)
                {
                  if (previous_segment_end != (0, 0))
                    {
                      draw(previous_segment_end..tension atleast curve_tension ..cell_center + (cell_size / 2.0, cell_size / 2.0)..tension atleast curve_tension ..cell_center + (-cell_size / 2.0, cell_size / 2.0)..tension atleast curve_tension ..cell_center + (-cell_size / 2.0, -cell_size / 2.0)..tension atleast curve_tension ..cell_center + (cell_size / 2.0, -cell_size / 2.0), p = mypen);
                    }
                  else
                    {
                      draw(cell_center + (cell_size / 2.0, cell_size / 2.0)..tension atleast curve_tension ..cell_center + (-cell_size / 2.0, cell_size / 2.0)..tension atleast curve_tension ..cell_center + (-cell_size / 2.0, -cell_size / 2.0)..tension atleast curve_tension ..cell_center + (cell_size / 2.0, -cell_size / 2.0), p = mypen);
                    }

                  return cell_center + (cell_size / 2.0, -cell_size / 2.0);
                }
              else
                {
                  if (previous_segment_end != (0, 0))
                    {
                      draw(previous_segment_end..tension atleast curve_tension ..cell_center + (cell_size / 2.0, cell_size / 2.0)..tension atleast curve_tension ..cell_center + (cell_size / 2.0, -cell_size / 2.0)..tension atleast curve_tension ..cell_center + (-cell_size / 2.0, -cell_size / 2.0)..tension atleast curve_tension ..cell_center + (-cell_size / 2.0, cell_size / 2.0), p = mypen);
                    }
                  else
                    {
                      draw(cell_center + (cell_size / 2.0, cell_size / 2.0)..tension atleast curve_tension ..cell_center + (cell_size / 2.0, -cell_size / 2.0)..tension atleast curve_tension ..cell_center + (-cell_size / 2.0, -cell_size / 2.0)..tension atleast curve_tension ..cell_center + (-cell_size / 2.0, cell_size / 2.0), p = mypen);
                    }

                  return cell_center + (-cell_size / 2.0, cell_size / 2.0);
                }
            }
        }
    }
  else
    {
      if (block_type == 0)
        {
          previous_segment_end = draw_hilbert_for_grid(mypen, previous_segment_end, cell_center + (-cell_size / 2.0, -cell_size / 2.0), cell_size / 2.0, 1, max_level, current_level + 1, curve_tension);
          previous_segment_end = draw_hilbert_for_grid(mypen, previous_segment_end, cell_center + (-cell_size / 2.0, cell_size / 2.0), cell_size / 2.0, 0, max_level, current_level + 1, curve_tension);
          previous_segment_end = draw_hilbert_for_grid(mypen, previous_segment_end, cell_center + (cell_size / 2.0, cell_size / 2.0), cell_size / 2.0, 0, max_level, current_level + 1, curve_tension);
          previous_segment_end = draw_hilbert_for_grid(mypen, previous_segment_end, cell_center + (cell_size / 2.0, -cell_size / 2.0), cell_size / 2.0, 2, max_level, current_level + 1, curve_tension);
        }
      else
        {
          if (block_type == 1)
            {
              previous_segment_end = draw_hilbert_for_grid(mypen, previous_segment_end, cell_center + (-cell_size / 2.0, -cell_size / 2.0), cell_size / 2.0, 0, max_level, current_level + 1, curve_tension);
              previous_segment_end = draw_hilbert_for_grid(mypen, previous_segment_end, cell_center + (cell_size / 2.0, -cell_size / 2.0), cell_size / 2.0, 1, max_level, current_level + 1, curve_tension);
              previous_segment_end = draw_hilbert_for_grid(mypen, previous_segment_end, cell_center + (cell_size / 2.0, cell_size / 2.0), cell_size / 2.0, 1, max_level, current_level + 1, curve_tension);
              previous_segment_end = draw_hilbert_for_grid(mypen, previous_segment_end, cell_center + (-cell_size / 2.0, cell_size / 2.0), cell_size / 2.0, 3, max_level, current_level + 1, curve_tension);
            }
          else
            {
              if (block_type == 2)
                {
                  previous_segment_end = draw_hilbert_for_grid(mypen, previous_segment_end, cell_center + (cell_size / 2.0, cell_size / 2.0), cell_size / 2.0, 3, max_level, current_level + 1, curve_tension);
                  previous_segment_end = draw_hilbert_for_grid(mypen, previous_segment_end, cell_center + (-cell_size / 2.0, cell_size / 2.0), cell_size / 2.0, 2, max_level, current_level + 1, curve_tension);
                  previous_segment_end = draw_hilbert_for_grid(mypen, previous_segment_end, cell_center + (-cell_size / 2.0, -cell_size / 2.0), cell_size / 2.0, 2, max_level, current_level + 1, curve_tension);
                  previous_segment_end = draw_hilbert_for_grid(mypen, previous_segment_end, cell_center + (cell_size / 2.0, -cell_size / 2.0), cell_size / 2.0, 0, max_level, current_level + 1, curve_tension);
                }
              else
                {
                  previous_segment_end = draw_hilbert_for_grid(mypen, previous_segment_end, cell_center + (cell_size / 2.0, cell_size / 2.0), cell_size / 2.0, 2, max_level, current_level + 1, curve_tension);
                  previous_segment_end = draw_hilbert_for_grid(mypen, previous_segment_end, cell_center + (cell_size / 2.0, -cell_size / 2.0), cell_size / 2.0, 3, max_level, current_level + 1, curve_tension);
                  previous_segment_end = draw_hilbert_for_grid(mypen, previous_segment_end, cell_center + (-cell_size / 2.0, -cell_size / 2.0), cell_size / 2.0, 3, max_level, current_level + 1, curve_tension);
                  previous_segment_end = draw_hilbert_for_grid(mypen, previous_segment_end, cell_center + (-cell_size / 2.0, cell_size / 2.0), cell_size / 2.0, 1, max_level, current_level + 1, curve_tension);
                }
            }
        }

      return previous_segment_end;
    }
}

real grid_length = 400;
pair grid_center = (grid_length / 2.0, grid_length / 2.0);
int x_cell_num = 128;
real cell_size = grid_length / x_cell_num;
real first_level_cell_size = grid_length / 2.0;

draw(grid2d(cell_size, cell_size, x_cell_num, x_cell_num));
pen mypen = rgb(1.0, 0, 0);
draw_hilbert_for_grid(mypen, (0, 0), grid_center, first_level_cell_size, 0, (int)(log2(x_cell_num)), 1, 4);
</code></pre>

<p><img src="/figures/2023-07-30-hilbert-curve-n=16.jpg" alt="img" title="Hilbert curve on 16x16 grid" /></p>

<p><img src="/figures/2023-07-30-hilbert-curve-n=32.jpg" alt="img" title="Hilbert curve on 32x32 grid" /></p>

<p><img src="/figures/2023-07-30-hilbert-curve-n=64.jpg" alt="img" title="Hilbert curve on 64x64 grid" /></p>

<p><img src="/figures/2023-07-30-hilbert-curve-n=128.jpg" alt="img" title="Hilbert curve on 128x128 grid" /></p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="BEM" /><category term="Asymptote" /><category term="H-matrix" /><category term="math-visualization" /><summary type="html"><![CDATA[I’ve written the following Asymptote script which produces Hilbert curve on a 2D grid. Hilbert curve is useful in the sequence partitioning of parallel processing of hierarchical matrices.]]></summary></entry><entry><title type="html">Understanding about abstract Riemannian metric</title><link href="https://jihuan-tian.github.io/math/2023/07/24/understanding-about-abstract-riemannian-metric.html" rel="alternate" type="text/html" title="Understanding about abstract Riemannian metric" /><published>2023-07-24T00:00:00+08:00</published><updated>2023-07-24T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2023/07/24/understanding-about-abstract-riemannian-metric</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2023/07/24/understanding-about-abstract-riemannian-metric.html"><![CDATA[<p>In CMU DDG course <a href="https://youtu.be/sV58Fy2s6ac?list=PL9_jI1bdZmz0hIrNCMQW1YmZysAiIYSSS&amp;t=3840">Lecture 12 “Smooth Surfaces
I”</a>,
the “abstractness” of the Riemannian metric means as long as an inner product
is assigned to each point in the geometric object, it’s enough for us to
compute quantities such as distances, angles, lengths, areas, etc. formed by
vectors represented in the local coordinate chart, without the need to embed the
geometric object into some higher dimensional space for visualization.</p>

<p>This is a very important concept and methodology in that we only perform
abstract computations based on Riemannian metric, in order to know,
characterize, transform and control the properties of the geometric
object without actually seeing or feeling it. This again embodies the
power of mathematical abstraction. This is also a counter-example of the
popular pedagogy, which introduces lots of visualizations, animations
and jokes into classes just to make the courses appealing to students.
However, such “obsequious” style is weak and limited. I’m in favour of
the abstract style.</p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="differential-geometry" /><category term="cmu-ddg" /><category term="snippet" /><summary type="html"><![CDATA[In CMU DDG course Lecture 12 “Smooth Surfaces I”, the “abstractness” of the Riemannian metric means as long as an inner product is assigned to each point in the geometric object, it’s enough for us to compute quantities such as distances, angles, lengths, areas, etc. formed by vectors represented in the local coordinate chart, without the need to embed the geometric object into some higher dimensional space for visualization.]]></summary></entry><entry><title type="html">Algorithm for reconstruction of a discrete space curve</title><link href="https://jihuan-tian.github.io/math/2023/07/21/algorithm-for-reconstruction-of-a-discrete-space-curve.html" rel="alternate" type="text/html" title="Algorithm for reconstruction of a discrete space curve" /><published>2023-07-21T00:00:00+08:00</published><updated>2023-07-21T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2023/07/21/algorithm-for-reconstruction-of-a-discrete-space-curve</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2023/07/21/algorithm-for-reconstruction-of-a-discrete-space-curve.html"><![CDATA[<p>The algorithm given in the CMU’s discrete differential geometry course
<a href="https://youtu.be/IyJnd_LvGRI?list=PL9_jI1bdZmz0hIrNCMQW1YmZysAiIYSSS&amp;t=2349">Lecture 11</a>
only illustrates the basic concept of the workflow for reconstruction of
a discrete space curve, which cannot be directly used. Therefore, I
recapitulate the algorithm here.</p>

<p>Assume there are \(M\) vertices
\(\left\{ v_0, \cdots, v_{M-1} \right\}\). Then there are \(M-1\)
piecewise linear edges \(\left\{ e_0, \cdots, e_{M-2} \right\}\) with
the edge length \(\left\{ l_{01}, \cdots, l_{M-2,M-1} \right\}\). The
corresponding tangent vectors along the edges are
\(\left\{ T_{01},\cdots,T_{M-2,M-1} \right\}\).</p>

<p>Curvature \(\kappa_i\) is defined as the angle variation of the two
consecutive edges \(e_{i-1}\) and \(e_{i}\) sharing the \(i\)-th vertex.
Hence there are no curvatures defined at the starting and ending
vertices. The array of curvatures are
\(\left\{ \kappa_1, \cdots, \kappa_{M-2} \right\}\).</p>

<p>The two consecutive edges \(e_{i-1}\) and \(e_i\) are coplanar. The
normal vector of the corresponding plane is the binormal vector \(B_i\).
There are \(M-2\) number of such binormal vectors
\(\left\{ B_1,\cdots,B_{M-2} \right\}\), which can be considered as
associated with the interior vertices.</p>

<p>For two consecutive binormal vectors \(B_i\) and \(B_j\), their
associated planes may not be parallel. Hence the angle change
\(\tau_{ij}\) from \(B_i\) to \(B_j\) is not zero and this is just the
torsion. The array of torsion is
\(\left\{ \tau_{12}, \cdots, \tau_{M-3,M-2} \right\}\).</p>

<p>With the above definitions and array sizes clarified, the algorithm for
reconstruction of a discrete space curve is given below.</p>

<p><img src="/figures/2023-07-21-algorithm-reconstruct-discrete-space-curve.png" alt="img" /></p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="differential-geometry" /><category term="cmu-ddg" /><category term="snippet" /><summary type="html"><![CDATA[The algorithm given in the CMU’s discrete differential geometry course Lecture 11 only illustrates the basic concept of the workflow for reconstruction of a discrete space curve, which cannot be directly used. Therefore, I recapitulate the algorithm here.]]></summary></entry><entry><title type="html">Understanding about representing regular spatial curve using differential form</title><link href="https://jihuan-tian.github.io/math/2023/07/21/understanding-about-representing-regular-spatial-curve-using-differential-form.html" rel="alternate" type="text/html" title="Understanding about representing regular spatial curve using differential form" /><published>2023-07-21T00:00:00+08:00</published><updated>2023-07-21T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2023/07/21/understanding-about-representing-regular-spatial-curve-using-differential-form</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2023/07/21/understanding-about-representing-regular-spatial-curve-using-differential-form.html"><![CDATA[<ul>
  <li>
    <p>A spatial curve is represented as a vector-valued 0-form
  \(\gamma(s)\).</p>
  </li>
  <li>
    <p>The tangent vector of the curve is not the 1-form \(d\gamma\) but
the application of this 1-form to the basis
\(\frac{\pdiff }{\pdiff s}\). N.B. For a vector-valued 1-form, its
application to a vector is not a scalar but a vector.</p>
  </li>
  <li>
    <p>When we represent the tangent vectors of a curve using differential
form, it is obvious that if \(d\gamma\) is zero at a point \(s_0\),
then \(d\gamma(\frac{\pdiff }{\pdiff s})\vert_{s_0}\) is always a
zero vector. This means the tangent vector at \(s_0\) cannot be well
defined via a differential form, even though a tangent vector
geometrically exists. This is why the definition of a regular curve
requires \(d\gamma\neq 0\) everywhere on the curve.</p>
  </li>
</ul>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="differential-geometry" /><category term="cmu-ddg" /><category term="snippet" /><summary type="html"><![CDATA[A spatial curve is represented as a vector-valued 0-form \(\gamma(s)\).]]></summary></entry><entry><title type="html">Understanding about the coboundary of coboundary is empty</title><link href="https://jihuan-tian.github.io/math/2023/07/14/understanding-about-the-coboundary-of-coboundary-is-empty.html" rel="alternate" type="text/html" title="Understanding about the coboundary of coboundary is empty" /><published>2023-07-14T00:00:00+08:00</published><updated>2023-07-14T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2023/07/14/understanding-about-the-coboundary-of-coboundary-is-empty</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2023/07/14/understanding-about-the-coboundary-of-coboundary-is-empty.html"><![CDATA[<p>According to (Desbrun, Kanso, and Tong 2008), the exterior derivative
operator \(d\) is the adjoint of the boundary operator \(\partial\).
This is based on the Stokes’ theorem. Let \(\omega\) be a \(k\)-form and
\(\sigma\) be a simplicial chain. Then we have</p>

\[\int_{\sigma} d\omega = \int_{\partial\sigma} \omega.\]

<p>This can be
rewritten as
\(\left[ d\omega, \sigma \right] = \left[ \omega, \partial\sigma \right]\),
where the brackets \([\cdot,\cdot]\) group two entities comprising a
differential form and a simplicial chain via integration. And the
integration is also a linear operator. According to the concept of
adjoint operator in functional analysis, \(d\) is the adjoint of
\(\partial\).</p>

<p>We already know that the boundary of boundary is an empty set, i.e.
\(\partial \circ \partial = \emptyset\). Then we have</p>

\[\left[ \omega, \partial\circ\partial\sigma \right] = \left[ d\omega, \partial\sigma \right] = \left[ d\circ d\omega, \sigma \right] \equiv 0.\]

<p>This just brings about the exactness of the exterior derivative
operator, which means the coboundary of coboundary also vanishes.</p>

<div id="refs" class="references hanging-indent">

<div id="ref-DesbrunDiscrete2008">

    <p>Desbrun, Mathieu, Eva Kanso, and Yiying Tong. 2008. “Discrete
Differential Forms for Computational Modeling.” In <em>Discrete
Differential Geometry</em>, edited by Alexander I. Bobenko, John M.
Sullivan, Peter Schröder, and Günter M. Ziegler, 287–324. Oberwolfach
Seminars. Basel: Birkhäuser.
<a href="https://doi.org/10.1007/978-3-7643-8621-4_16">https://doi.org/10.1007/978-3-7643-8621-4_16</a>.</p>

  </div>

</div>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="differential-geometry" /><category term="cmu-ddg" /><category term="snippet" /><summary type="html"><![CDATA[According to (Desbrun, Kanso, and Tong 2008), the exterior derivative operator \(d\) is the adjoint of the boundary operator \(\partial\). This is based on the Stokes’ theorem. Let \(\omega\) be a \(k\)-form and \(\sigma\) be a simplicial chain. Then we have]]></summary></entry><entry><title type="html">Understanding about integration</title><link href="https://jihuan-tian.github.io/math/2023/07/13/understanding-about-integration.html" rel="alternate" type="text/html" title="Understanding about integration" /><published>2023-07-13T00:00:00+08:00</published><updated>2023-07-13T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2023/07/13/understanding-about-integration</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2023/07/13/understanding-about-integration.html"><![CDATA[<p>The essence of integration of a differential \(k\)-form
\(\int_{\Omega} \omega\) is to discretize the domain into infinitesimal
patches, project the \(k\)-form onto each of them, then take the sum.
Since there is projection, there is the inner product operation, which
naturally involves these dual pairs: vector and covector, \(k\)-vector
and \(k\)-form, \(k\)-vector field and differential \(k\)-form.</p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="differential-geometry" /><category term="cmu-ddg" /><category term="snippet" /><summary type="html"><![CDATA[The essence of integration of a differential \(k\)-form \(\int_{\Omega} \omega\) is to discretize the domain into infinitesimal patches, project the \(k\)-form onto each of them, then take the sum. Since there is projection, there is the inner product operation, which naturally involves these dual pairs: vector and covector, \(k\)-vector and \(k\)-form, \(k\)-vector field and differential \(k\)-form.]]></summary></entry><entry><title type="html">Understanding about the anti-symmetry of \(k \)-forms and geometric meaning of determinant</title><link href="https://jihuan-tian.github.io/math/2023/07/13/understanding-about-the-anti-symmetry-of-k-forms-and-geometric-meaning-of-determinant.html" rel="alternate" type="text/html" title="Understanding about the anti-symmetry of \(k \)-forms and geometric meaning of determinant" /><published>2023-07-13T00:00:00+08:00</published><updated>2023-07-13T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2023/07/13/understanding-about-the-anti-symmetry-of-k-forms-and-geometric-meaning-of-determinant</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2023/07/13/understanding-about-the-anti-symmetry-of-k-forms-and-geometric-meaning-of-determinant.html"><![CDATA[<p>When applying a differential \(k\)-form
\(\alpha(x) = \alpha_1(x) \wedge \cdots \wedge \alpha_k(x)\) to a
\(k\)-vector field \(u(x) = u_1(x) \wedge \cdots \wedge u_k(x)\),
swapping any pair of component differential \(1\)-forms in the
\(k\)-form, i.e. \((\alpha_i(x), \alpha_j(x))\), or any pair of
component vectors in the \(k\)-vector field, i.e. \((u_i(x), u_j(x))\),
will change the sign of the resulted scalar value \(\alpha(u)\). This is
because both the differential \(1\)-forms
\(\{\alpha_1(x), \cdots, \alpha_k(x)\}\) in \(\alpha(x)\) and the
vectors \(\{u_1(x), \cdots, u_k(x)\}\) in \(u(x)\) are ordered sets, the
permutation of which defines an orientation.</p>

<p>The resulted value \(\alpha(u)\) is actually a determinant<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>

\[\alpha(u) = 
\begin{vmatrix}
\alpha_1(u_1) &amp; \cdots &amp; \alpha_1(u_k) \\
\vdots &amp; \vdots &amp; \vdots \\
\alpha_k(u_1) &amp; \cdots &amp; \alpha_k(u_k)
\end{vmatrix},\]

<p>where each column is a projection of one component
vector in \(u(x)\) to the differential \(k\)-form. Also the determinant
itself is anti-symmetric, which is consistent with the rule for
evaluation of \(\alpha(u)\). Therefore, the signed volume of the
parallelogram spanned by the set of projected component vectors
\(\left\{ (\alpha_1(u_i), \cdots, \alpha_k(u_i)) \right\}_{i=1}^{k}\) is
the geometric meaning of determinant.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>The following display equation is wrapped around <code class="language-plaintext highlighter-rouge">\begin{equation*} ... \end{equation*}</code> for correct visualization in Emacs Org mode. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="differential-geometry" /><category term="cmu-ddg" /><category term="snippet" /><summary type="html"><![CDATA[When applying a differential \(k\)-form \(\alpha(x) = \alpha_1(x) \wedge \cdots \wedge \alpha_k(x)\) to a \(k\)-vector field \(u(x) = u_1(x) \wedge \cdots \wedge u_k(x)\), swapping any pair of component differential \(1\)-forms in the \(k\)-form, i.e. \((\alpha_i(x), \alpha_j(x))\), or any pair of component vectors in the \(k\)-vector field, i.e. \((u_i(x), u_j(x))\), will change the sign of the resulted scalar value \(\alpha(u)\). This is because both the differential \(1\)-forms \(\{\alpha_1(x), \cdots, \alpha_k(x)\}\) in \(\alpha(x)\) and the vectors \(\{u_1(x), \cdots, u_k(x)\}\) in \(u(x)\) are ordered sets, the permutation of which defines an orientation.]]></summary></entry><entry><title type="html">The meaning of applying a covector to a vector</title><link href="https://jihuan-tian.github.io/math/2023/07/09/the-meaning-of-applying-a-covector-to-a-vector.html" rel="alternate" type="text/html" title="The meaning of applying a covector to a vector" /><published>2023-07-09T00:00:00+08:00</published><updated>2023-07-09T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2023/07/09/the-meaning-of-applying-a-covector-to-a-vector</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2023/07/09/the-meaning-of-applying-a-covector-to-a-vector.html"><![CDATA[<p>Let \(\alpha\) be a covector and \(u\) be a vector. The application of
\(\alpha\) to \(u\), i.e. \(\alpha(u)\) can be considered as the vector \(u\)
being measured by the covector \(\alpha\) via projection. If the covector
\(\alpha\) does not have a unit magnitude or length, the resulted scalar
value will be scaled by this magnitude.</p>

<p>Therefore, the covector \(\alpha\) plays the role of a ruler and its
magnitude is just the <strong>reciprocal</strong> of the inter-distance between two
adjacent marks on the ruler’s scale. Such inter-distance is selected as
the unit for the said measurement. The smaller the unit, the larger the
register of the measurement. This understanding is consistent with the
illustration of covectors presented in (Burke 1985). In this book, a
covector is represented as two parallel lines with an arrow. The
inter-distance of the two lines is the reciprocal of the covector’s
length.</p>

<p align="center"><img src="/figures/2023-07-09-illustration-of-covector.png" alt="The covector has a unit length." /></p>
<p align="center">The covector has a unit length.</p>

<p align="center"><img src="/figures/2023-07-09-illustration-of-covector-magnitude=2.png" alt="The covector's length is 2." /></p>
<p align="center">The covector's length is 2.</p>

<h1 id="references">References</h1>
<div id="refs" class="references hanging-indent">

<div id="ref-BurkeApplied1985">

    <p>Burke, William L. 1985. <em>Applied Differential Geometry</em>. 1st edition.
Lanham, Md: Cambridge University Press.</p>

  </div>

</div>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="differential-geometry" /><summary type="html"><![CDATA[Let \(\alpha\) be a covector and \(u\) be a vector. The application of \(\alpha\) to \(u\), i.e. \(\alpha(u)\) can be considered as the vector \(u\) being measured by the covector \(\alpha\) via projection. If the covector \(\alpha\) does not have a unit magnitude or length, the resulted scalar value will be scaled by this magnitude.]]></summary></entry><entry><title type="html">Measurement and duality</title><link href="https://jihuan-tian.github.io/math/2023/07/07/measurement-and-duality.html" rel="alternate" type="text/html" title="Measurement and duality" /><published>2023-07-07T00:00:00+08:00</published><updated>2023-07-07T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2023/07/07/measurement-and-duality</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2023/07/07/measurement-and-duality.html"><![CDATA[<p><a href="https://youtu.be/xRf9-hdxB0w?list=PL9_jI1bdZmz0hIrNCMQW1YmZysAiIYSSS&amp;t=243">Measurement and duality</a> mentioned in Lecture 4 \(k\)-forms of the <a href="https://youtube.com/playlist?list=PL9_jI1bdZmz0hIrNCMQW1YmZysAiIYSSS">CMU Discrete Differential Geometry course</a> is enlightening. Inner product of two vectors \(u\) and \(v\) is just a kind of such measurement, which is intrinsically a projection operation. \(v\) is the vector to be measured and \(u\) is the vector for measuring. Even though \(u\) and \(v\) are both vectors in an Euclidean space, they have different identities. That’s why during the evaluation of \((u, v)\), \(u\) is written as a row vector, while \(v\) is a column vector. Similar cases arise in other areas, such as in functional analysis, a vector in a Hilbert space is associated with its dual vector via the Riesz representation theorem; in quantum mechanics, there is the demarcation of ket \(\langle\psi \vert\) and bra \(\vert\varphi\rangle\); in PDE or FEM, ansatz and test basis functions are adopted to discretize bilinear forms.</p>

<p>Backlinks: <a href="/math/2024/08/16/concept-of-duality.html">《Concept of duality》</a></p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="differential-geometry" /><category term="duality" /><summary type="html"><![CDATA[Measurement and duality mentioned in Lecture 4 \(k\)-forms of the CMU Discrete Differential Geometry course is enlightening. Inner product of two vectors \(u\) and \(v\) is just a kind of such measurement, which is intrinsically a projection operation. \(v\) is the vector to be measured and \(u\) is the vector for measuring. Even though \(u\) and \(v\) are both vectors in an Euclidean space, they have different identities. That’s why during the evaluation of \((u, v)\), \(u\) is written as a row vector, while \(v\) is a column vector. Similar cases arise in other areas, such as in functional analysis, a vector in a Hilbert space is associated with its dual vector via the Riesz representation theorem; in quantum mechanics, there is the demarcation of ket \(\langle\psi \vert\) and bra \(\vert\varphi\rangle\); in PDE or FEM, ansatz and test basis functions are adopted to discretize bilinear forms.]]></summary></entry><entry><title type="html">Understanding about the sharp and flat operators</title><link href="https://jihuan-tian.github.io/math/2023/07/07/understanding-about-the-sharp-and-flat-operators.html" rel="alternate" type="text/html" title="Understanding about the sharp and flat operators" /><published>2023-07-07T00:00:00+08:00</published><updated>2023-07-07T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2023/07/07/understanding-about-the-sharp-and-flat-operators</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2023/07/07/understanding-about-the-sharp-and-flat-operators.html"><![CDATA[<p>In Lecture 4 \(k\)-forms of the <a href="https://youtube.com/playlist?list=PL9_jI1bdZmz0hIrNCMQW1YmZysAiIYSSS">CMU Discrete Differential Geometry
course</a>,
it is
<a href="https://youtu.be/xRf9-hdxB0w?list=PL9_jI1bdZmz0hIrNCMQW1YmZysAiIYSSS&amp;t=938">mentioned</a>
that a vector and its covector is linked by the sharp (\(\sharp\)) and
flat (\(\flat\)) operators. In order to take the inner product of \(u\)
and \(v\) in the vector space, we treat \(v\) as the operand and
transform \(u\) into the dual space via the \(\flat\) operator, then
apply it to \(v\) as \(u^{\flat}(v)\). In order to take the inner
product of the two covectors \(\alpha\) and \(\beta\), we transform
\(\beta\) into the vector space via the \(\sharp\) operator and treat it
as the operand, then apply \(\alpha\) to it as
\(\alpha(\beta^{\sharp})\).</p>

<p>The reason for using the operators \(\sharp\) and \(\flat\) is because
as a convention in differential geometry, a vector (contravariant
vector) written in the coordinate component form is assigned with
superscripts, such as \(v^i\), while a covector (covariant vector) is
assigned with subscripts, such as \(u_i\). The \(\sharp\) operator
elevates subscripts into superscripts and vice versa for the \(\flat\)
operator.</p>

<p>We may also think that because a vector is represented as a row vector
and a covector is represented as a column vector, the above \(\sharp\)
and \(\flat\) operators are simply the transpose operation. However,
this is only true when the local coordinate frame for the neighborhood
of the interested vector adopts a orthonormal basis, i.e. let the basis
be
\(\left\{\frac{\vect{\pdiff} }{\vect{\pdiff} x^1}, \cdots, \frac{\vect{\pdiff} }{\vect{\pdiff} x^n}\right\}\),
and we have
\(\left\langle \frac{\vect{\pdiff} }{\vect{\pdiff} x^i}, \frac{\vect{\pdiff} }{\vect{\pdiff} x^j} \right\rangle = \delta_{ij}\).
When the basis is not orthonormal, a mass matrix \(M\) or metric tensor
\(g\) appears:</p>

\[M_{ij} = g_{ij} = \left\langle \frac{\vect{\pdiff} }{\vect{\pdiff} x^i}, \frac{\vect{\pdiff} }{\vect{\pdiff} x^j} \right\rangle = J^T J,\]

<p>where</p>

\[J = \begin{pmatrix}
   \frac{\vect{\pdiff} }{\vect{\pdiff} x^1} &amp; \cdots &amp; \frac{\vect{\pdiff} }{\vect{\pdiff} x^n}
   \end{pmatrix}.\]

<p>When there is a global coordinate frame in which we can explicitly represent the basis
\(\left\{\frac{\vect{\pdiff} }{\vect{\pdiff} x^1}, \cdots, \frac{\vect{\pdiff} }{\vect{\pdiff} x^n}\right\}\)
of the local coordinate frame, \(J\) is just the Jacobian matrix for the
map from this local frame to the global frame. Then the metric tensor
\(g\) is also called the Gramian matrix.</p>

<p>Take the spherical coordinate frame as an example.</p>

\[\begin{aligned}
x &amp;= r \sin\theta \cos\varphi \\
y &amp;= r \sin\theta \sin\varphi \\
z &amp;= r \cos\theta
\end{aligned}\]

<p>The basis is
\(\left\{ \frac{\pdiff }{\pdiff r}, \frac{\pdiff }{\pdiff \varphi}, \frac{\pdiff }{\pdiff \theta} \right\}\)
and the Jacobian matrix from this local coordinate frame to the global
Cartesian coordinate frame is</p>

\[J = \begin{pmatrix}
   \sin\theta \cos\varphi &amp; -r \sin\theta \sin\varphi &amp; r \cos\theta \cos\varphi \\
   \sin\theta \sin\varphi &amp; r \sin\theta \cos\varphi &amp; r \cos\theta \sin\varphi \\
   \cos\theta &amp; 0 &amp; -r \sin\theta
   \end{pmatrix}\]

<p>The Gramian matrix is</p>

\[g = J^T J = \begin{pmatrix}
   1 &amp; 0 &amp; 0 \\
   0 &amp; r^2\sin(\theta)^2 &amp; 0 \\
   0 &amp; 0 &amp; r^2
   \end{pmatrix}\]

<p>which is computed in Maxima as below.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>programmode : false;

J : matrix([sin(theta)*cos(phi), -r*sin(theta)*sin(phi), r*cos(theta)*cos(phi)],
           [sin(theta)*sin(phi), r*sin(theta)*cos(phi), r*cos(theta)*sin(phi)],
           [cos(theta),0,-r*sin(theta)]);
g : trigsimp(transpose(J) . J);
disp(g);
</code></pre></div></div>

<p>In general, the transformation between a vector and its covector
involves the metric tensor \(g\) and can be written as</p>

\[v_{j} = g_{ji} v^i, v^i = g^{ij} v_j,\]

<p>where \(g^{ij}\) is the inverse of the metric tensor. The metric tensor
\(g_{ij}\) and its inverse \(g^{ji}\) are respectively 2nd rank
covariant and contravariant tensors, which are not mixed 2nd rank
tensor. Hence they are not equivalent to a linear transformation. That’s
why if the above transformations are written in matrix form, there are
still transpose operations needed to make them correct. For example,</p>

<ol>
  <li>
    <p>The obtained column vector on the left should be transposed to
produce a covariant vector:</p>

\[\begin{pmatrix}
v_1 \\
\vdots \\
v_n
\end{pmatrix} =
\begin{pmatrix}
  g_{ij}
\end{pmatrix}
\begin{pmatrix}
  v^1 \\
  \vdots \\
  v^n
\end{pmatrix}\]
  </li>
  <li>
    <p>The input covariant vector should be tranposed first to get the
column vector on the right:</p>

\[\begin{pmatrix}
v^1 \\
\vdots \\
v^n
\end{pmatrix} =
\begin{pmatrix}
  g^{ji}
\end{pmatrix}
\begin{pmatrix}
  v_1 \\
  \vdots \\
  v_n
\end{pmatrix}\]
  </li>
</ol>

<p>With the above concepts clarified, we will understand the following
equations in the lecture slides (N.B. \(M\) is a symmetric matrix):</p>

\[u^{\flat}(v) = (Mu)^T v = u^T M^T v = u^T M v\]

<p>and</p>

\[\alpha(\beta^{\sharp}) = \alpha M^{-1} \beta^T.\]]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="differential-geometry" /><summary type="html"><![CDATA[In Lecture 4 \(k\)-forms of the CMU Discrete Differential Geometry course, it is mentioned that a vector and its covector is linked by the sharp (\(\sharp\)) and flat (\(\flat\)) operators. In order to take the inner product of \(u\) and \(v\) in the vector space, we treat \(v\) as the operand and transform \(u\) into the dual space via the \(\flat\) operator, then apply it to \(v\) as \(u^{\flat}(v)\). In order to take the inner product of the two covectors \(\alpha\) and \(\beta\), we transform \(\beta\) into the vector space via the \(\sharp\) operator and treat it as the operand, then apply \(\alpha\) to it as \(\alpha(\beta^{\sharp})\).]]></summary></entry><entry><title type="html">The meaning of Jacobian matrix having a rank \(r\)</title><link href="https://jihuan-tian.github.io/math/2023/07/05/the-meaning-of-jacobian-matrix-having-a-rank.html" rel="alternate" type="text/html" title="The meaning of Jacobian matrix having a rank \(r\)" /><published>2023-07-05T00:00:00+08:00</published><updated>2023-07-05T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2023/07/05/the-meaning-of-jacobian-matrix-having-a-rank</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2023/07/05/the-meaning-of-jacobian-matrix-having-a-rank.html"><![CDATA[<p>Let \(F(x) = 0\) be a set of \(r\) constraint functions defining a
locus, where \(x\in \mathbb{R}^{r+n}\). If the Jacobian matrix of the
multi-dimensional map \(F\) has rank \(r\), it means all the constraint
equations are effective or independent, and \(r\) coordinate components
in \(x\) can be eliminated, i.e. represented by the other \(n\)
coordinate components. Hence, the locus or submanifold defined by
\(F(x) = 0\) has \(n\) dimensions.</p>

<p>Here we should bear in mind that <strong>the rank of the Jacobian matrix is
actually the number of the constraints instead of the number of free
variables or dimensions of the submanifold</strong>. Hence, the submanifold
dimension is the co-dimension of \(r\) in \(\mathbb{R}^{r+n}\), i.e.
\(n\).</p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="differential-geometry" /><category term="cmu-ddg" /><category term="snippet" /><summary type="html"><![CDATA[Let \(F(x) = 0\) be a set of \(r\) constraint functions defining a locus, where \(x\in \mathbb{R}^{r+n}\). If the Jacobian matrix of the multi-dimensional map \(F\) has rank \(r\), it means all the constraint equations are effective or independent, and \(r\) coordinate components in \(x\) can be eliminated, i.e. represented by the other \(n\) coordinate components. Hence, the locus or submanifold defined by \(F(x) = 0\) has \(n\) dimensions.]]></summary></entry><entry><title type="html">Construct topological data structures from a mesh</title><link href="https://jihuan-tian.github.io/math/2023/07/03/construct-topological-data-structures-from-a-mesh.html" rel="alternate" type="text/html" title="Construct topological data structures from a mesh" /><published>2023-07-03T00:00:00+08:00</published><updated>2023-07-03T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2023/07/03/construct-topological-data-structures-from-a-mesh</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2023/07/03/construct-topological-data-structures-from-a-mesh.html"><![CDATA[<p>When we construct topological data structures from a mesh, such as the
signed incidence matrix associating \(k\)-simplices and
\(k+1\)-simplices therein, don’t be keen to directly achieve this target
in one round. Sometimes, we need several intermediate steps and data
structures as a scaffold for the construction. For example, we can
firstly create a collections of dynamic vectors storing the incident
edges of each cell in the mesh, then transform these data into the final
compressed sparse row (CSR) matrix format. We can see that to realize
the final algorithm with the minimal storage and time complexity, it is
worthwhile to implement complex mechanisms with affordable overheads.
There is actually no free lunch!</p>

<p>incidence OED The situation of one locus with respect to another
when they have a common point or points, but do not completely coincide;
e.g. of a point to a line on which it lies, of a point or a line to a
plane in which it lies, or of two intersecting lines to each other.</p>

<p>From the German of Schubert, Kalkul der Abzähl. Geom. (1879) 25.</p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="differential-geometry" /><category term="cmu-ddg" /><category term="snippet" /><summary type="html"><![CDATA[When we construct topological data structures from a mesh, such as the signed incidence matrix associating \(k\)-simplices and \(k+1\)-simplices therein, don’t be keen to directly achieve this target in one round. Sometimes, we need several intermediate steps and data structures as a scaffold for the construction. For example, we can firstly create a collections of dynamic vectors storing the incident edges of each cell in the mesh, then transform these data into the final compressed sparse row (CSR) matrix format. We can see that to realize the final algorithm with the minimal storage and time complexity, it is worthwhile to implement complex mechanisms with affordable overheads. There is actually no free lunch!]]></summary></entry><entry><title type="html">The star of a subset of simplices</title><link href="https://jihuan-tian.github.io/math/2023/07/03/the-star-of-a-subset-of-simplices.html" rel="alternate" type="text/html" title="The star of a subset of simplices" /><published>2023-07-03T00:00:00+08:00</published><updated>2023-07-03T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2023/07/03/the-star-of-a-subset-of-simplices</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2023/07/03/the-star-of-a-subset-of-simplices.html"><![CDATA[<p>In <a href="https://www.youtube.com/playlist?list=PL9_jI1bdZmz0hIrNCMQW1YmZysAiIYSSS">CMU DDG course</a>
<a href="https://youtu.be/TDic3pJyYb8?list=PL9_jI1bdZmz0hIrNCMQW1YmZysAiIYSSS&amp;t=2547">Lecture 2A</a>,
the star of a subset of simplices is defined as the union of
<strong>simplices</strong> but <strong>not simplicial complex</strong> which contains the given
subset of simplices. Therefore, for the central vertex \(v_1\) as shown
below, all edges and cells in red color form its star. The edges and
vertices on the boundary do not belong to the star, even though they
belong to the simplicial complex formed by the cells with their lower
dimensional descendants. Take cell 1 as example, the boundary edge
\(\left\{ v_2, v_3 \right\}\) and the boundary vertices \(v_2\) and
\(v_3\) are contained in the simplicial complex formed by cell 1 and its
lower dimensional descendants, i.e.
\(\left\{ \left\{ v_1,v_2,v_3 \right\}, \left\{ v_1,v_2 \right\}, \left\{ v_2,v_3 \right\}, \left\{ v_3,v_1 \right\}, \left\{ v_1 \right\}, \left\{ v_2 \right\}, \left\{ v_3 \right\}, \emptyset \right\}\).
However, they are not contained in the cell 1 itself as a 2-simplex,
i.e. \(\left\{ v_1,v_2,v_3 \right\}\).</p>

<p><img src="/figures/2023-07-03-star-of-simplices.png" alt="image" /></p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="differential-geometry" /><category term="cmu-ddg" /><category term="snippet" /><summary type="html"><![CDATA[In CMU DDG course Lecture 2A, the star of a subset of simplices is defined as the union of simplices but not simplicial complex which contains the given subset of simplices. Therefore, for the central vertex \(v_1\) as shown below, all edges and cells in red color form its star. The edges and vertices on the boundary do not belong to the star, even though they belong to the simplicial complex formed by the cells with their lower dimensional descendants. Take cell 1 as example, the boundary edge \(\left\{ v_2, v_3 \right\}\) and the boundary vertices \(v_2\) and \(v_3\) are contained in the simplicial complex formed by cell 1 and its lower dimensional descendants, i.e. \(\left\{ \left\{ v_1,v_2,v_3 \right\}, \left\{ v_1,v_2 \right\}, \left\{ v_2,v_3 \right\}, \left\{ v_3,v_1 \right\}, \left\{ v_1 \right\}, \left\{ v_2 \right\}, \left\{ v_3 \right\}, \emptyset \right\}\). However, they are not contained in the cell 1 itself as a 2-simplex, i.e. \(\left\{ v_1,v_2,v_3 \right\}\).]]></summary></entry><entry><title type="html">Voltage distribution simulation using 3D Galerkin BEM</title><link href="https://jihuan-tian.github.io/math/2023/05/25/voltage-distribution-simulation-using-3d-galerkin-bem.html" rel="alternate" type="text/html" title="Voltage distribution simulation using 3D Galerkin BEM" /><published>2023-05-25T00:00:00+08:00</published><updated>2023-05-25T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2023/05/25/voltage-distribution-simulation-using-3d-galerkin-bem</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2023/05/25/voltage-distribution-simulation-using-3d-galerkin-bem.html"><![CDATA[<p>今日完成混合边界条件下 spanner model 的求解。模型规模为 35796 个单元，求解时间为 56758.6 秒，大约 15 个半小时。</p>

<p align="center"><img src="/figures/2023-05-25-spanner-model-mesh.png" alt="Model mesh" /></p>
<p align="center"><img src="/figures/2023-05-25-spanner-model-mesh-left-zoom-in.png" alt="Model mesh: left end zoomed in " /></p>
<p align="center"><img src="/figures/2023-05-25-spanner-model-mesh-right-zoom-in.png" alt="Model mesh: right end zoomed in" /></p>
<p align="center"><img src="/figures/2023-05-25-spanner-model-mesh-label-zoom-in.png" alt="Model mesh: label zoomed in" /></p>
<p align="center"><img src="/figures/2023-05-25-potential-distribution.png" alt="Potential distribution" /></p>
<p align="center"><img src="/figures/2023-05-25-potential-distribution-left-zoom-in.png" alt="Potential distribution: left end zoomed in" /></p>
<p align="center"><img src="/figures/2023-05-25-potential-distribution-label-zoom-in.png" alt="Potential distribution: label zoomed in" /></p>

<p>在去年 12 月份，我已初步实现了求解混合边界条件下 Laplace 方程的<a href="/math/2022/12/30/%E8%BE%B9%E7%95%8C%E5%85%83%E7%AE%97%E6%B3%95%E5%BC%80%E5%8F%91%E7%8A%B6%E6%80%81%E4%B8%8E%E7%AE%97%E6%B3%95%E5%BA%93%E7%AE%80%E4%BB%8B.html">三维伽辽金边界元算法</a>。那个时候曾尝试求解该模型，程序徘徊于矩阵的构建长达五天时间，都未能进入到方程的求解阶段。今年以来通过学习、实践、应用 CUDA 编程技术，终于在今天收获上述结果。整个计算过程中，内存占用量在 13 GB，左右，并未超过我的服务器 16 GB 的总内存。</p>

<p>若再回溯到 2020 年 10 月项目初期，那时我使用了 GNU Octave 对奇异数值积分与基于满阵的边界元方程进行求解。仅仅是 200 多个单元的模型也要计算一个多小时。而当单元数增加到 2000 左右时， Octave 脚本运行了一个多星期，导致整个机器卡死，也未得到任何结果。由此看来，对于有限元、边界元这种内存用量大、计算密集型的算法，除了对个别理论公式做解析性的推导、验证，否则从一开始就要远离 Octave 、 Python 这样的脚本语言。</p>

<p>虽然目前的算法性能仍不能令人满意，但它就像一颗可以不断生长、壮大的种子，让我拥有一个能够持续改进与提升的平台，有了一个可以书写后续故事的起点。根据程序运行过程中的观察，矩阵计算耗费的时间依旧很长，需要做细致的性能分析，找出瓶颈；严谨、合理地使用 GPU 的多级内存；使用性能更好的计算机设备，将计算的压力从软件转移至硬件。这样一来，就可以形成通用算法、高度并行化设计与强大的并行计算硬件三者结合的技术路线，在软件层面的抽象设计与工程层面的高性能需求间找到平衡点。</p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="算法开发" /><category term="BEM" /><summary type="html"><![CDATA[今日完成混合边界条件下 spanner model 的求解。模型规模为 35796 个单元，求解时间为 56758.6 秒，大约 15 个半小时。]]></summary></entry><entry><title type="html">边界元算法开发状态与算法库简介</title><link href="https://jihuan-tian.github.io/math/2022/12/30/%E8%BE%B9%E7%95%8C%E5%85%83%E7%AE%97%E6%B3%95%E5%BC%80%E5%8F%91%E7%8A%B6%E6%80%81%E4%B8%8E%E7%AE%97%E6%B3%95%E5%BA%93%E7%AE%80%E4%BB%8B.html" rel="alternate" type="text/html" title="边界元算法开发状态与算法库简介" /><published>2022-12-30T00:00:00+08:00</published><updated>2022-12-30T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2022/12/30/%E8%BE%B9%E7%95%8C%E5%85%83%E7%AE%97%E6%B3%95%E5%BC%80%E5%8F%91%E7%8A%B6%E6%80%81%E4%B8%8E%E7%AE%97%E6%B3%95%E5%BA%93%E7%AE%80%E4%BB%8B</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2022/12/30/%E8%BE%B9%E7%95%8C%E5%85%83%E7%AE%97%E6%B3%95%E5%BC%80%E5%8F%91%E7%8A%B6%E6%80%81%E4%B8%8E%E7%AE%97%E6%B3%95%E5%BA%93%E7%AE%80%E4%BB%8B.html"><![CDATA[<p>目前，我已实现了基于纯代数方法的三维伽辽金边界元 C++ 算法库，主要包括奇异数值积分、矩阵压缩与操作、针对线性迭代求解器的大型矩阵预处理等关键模块。以拉普拉斯方程在狄利克雷边界、诺伊曼边界、混合边界条件下的三个子问题作为测试算例，验证了该算法库的正确性。</p>

<p>该算法库的主要特性与潜力如下：</p>

<ol>
  <li>伽辽金边界元方法相对于传统的配置方法与 Nyström 方法具有更好的数值稳定性，对于模型中的棱边与尖角无需特殊处理。</li>
  <li>与传统的快速多极方法不同，该算法库基于不依赖具体物理问题及其控制方程的纯代数方法实现。因此，可将其作为通用的基础架构，快速构建针对不同物理问题与工程应用的纯边界元求解器或边界元-有限元耦合求解器，例如，电磁学、声学、弹性力学、热学以及流场的仿真分析。</li>
  <li>相较于传统边界元方法，以伽辽金变分法作为统一的理论基础，将伽辽金边界元与伽辽金有限元方法直接耦合在数值分析与算法实现两个层面都更加容易。该耦合求解器使纯边界元方法能够处理包含多介质、复杂几何结构与非线性参数的场域，大大拓展其应用范围；同时，在求解无限大的开域问题上，比纯有限元方法更有优势。</li>
</ol>

<p>I have implemented a C++ library of 3D Galerkin boundary element method (BEM) based on pure algebraic methods, which include singular numerical quadrature, matrix compression and manipulation, preconditioner for iterative linear solvers. At the moment, this library has been verified for solving the Laplace problem with Dirichlet, Neumann and mixed boundary conditions.</p>

<p>Its main features and potentials are as follows.</p>

<ol>
  <li>Compared to traditional methods, such as the collocation and Nyström methods, Galerkin-BEM has a better numerical stability and does not need special treatment of sharp edges and corners in a model.</li>
  <li>Unlike the traditional fast multipole method (FMM), this library is not limited to a specific physical problem and its governing equations, due to the adoption of pure algebraic methods. Hence, it can be used as a general framework to efficiently build pure BEM or BEM-FEM (finite element method) coupled solvers for a variety of physical problems and engineering applications, such as electromagnetics, acoustics, elasticity, thermal and flow field.</li>
  <li>Under the unified theoretical foundation of Galerkin variation, direct coupling of the Galerkin BEM and Galerkin FEM is much easier in the senses of both numerical analysis and algorithm implementation. Such coupling endows BEM with the capability of handling field domains having multi-media, complex geometry and nonlinear parameters, which greatly enhances the applicability of BEM and outperforms FEM when solving infinite open domain problems.</li>
</ol>

<p>Backlinks: <a href="/thoughts/2022/12/31/%E5%8D%81%E5%B9%B4.html">《十年》</a>, <a href="/math/2023/05/25/voltage-distribution-simulation-using-3d-galerkin-bem.html">《Voltage distribution simulation using 3D Galerkin BEM》</a></p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="算法开发" /><category term="BEM" /><summary type="html"><![CDATA[目前，我已实现了基于纯代数方法的三维伽辽金边界元 C++ 算法库，主要包括奇异数值积分、矩阵压缩与操作、针对线性迭代求解器的大型矩阵预处理等关键模块。以拉普拉斯方程在狄利克雷边界、诺伊曼边界、混合边界条件下的三个子问题作为测试算例，验证了该算法库的正确性。]]></summary></entry><entry><title type="html">Clarification of basis, base and bases</title><link href="https://jihuan-tian.github.io/math/2022/11/14/clarification-of-basis-base-and-bases.html" rel="alternate" type="text/html" title="Clarification of basis, base and bases" /><published>2022-11-14T00:00:00+08:00</published><updated>2022-11-14T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2022/11/14/clarification-of-basis-base-and-bases</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2022/11/14/clarification-of-basis-base-and-bases.html"><![CDATA[<p>According to (Nelson 2008)<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> P58</p>

<ul>
  <li>base
    <ol>
      <li>(of a number system) The number represented by the numeral ‘10’ in a positional number system. Thus, in the decimal system the base (ten) is represented by 10; in a binary system the base (two) is also represented by 10.</li>
      <li>(of logarithms) The number which, raised to the power of a given logarithm, produces a given number. Thus, if the logarithm of x to base b (written as logb x) is y, then by = x.</li>
      <li>A line or plane in a geometric figure relative to which the altitude of the figure is measured.</li>
    </ol>
  </li>
  <li>basis pl. bases: A subset of a vector space that is linearly independent and spans the space.</li>
</ul>

<p>Therefore, when we talk about the basis of some space, such as an Euclidean space or Sobolev space, the word <strong>basis</strong> itself is already a collection of elements in this space, while any single one of them should be described as <strong>basis element</strong> or <strong>basis function</strong>. On the other hand, <strong>base</strong> should never appear in such occasions. Also note the plural form of basis coincides with that of base.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Nelson, David, ed. 2008. The Penguin Dictionary of Mathematics: Fourth Edition. 4th edition. London: Penguin Books. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="fundamental-concepts" /><summary type="html"><![CDATA[According to (Nelson 2008)1 P58 Nelson, David, ed. 2008. The Penguin Dictionary of Mathematics: Fourth Edition. 4th edition. London: Penguin Books. &#8617;]]></summary></entry><entry><title type="html">Typical equivalence relations between mathematical spaces</title><link href="https://jihuan-tian.github.io/math/2022/10/06/typical-equivalence-relations-between-mathematical-spaces.html" rel="alternate" type="text/html" title="Typical equivalence relations between mathematical spaces" /><published>2022-10-06T00:00:00+08:00</published><updated>2022-10-06T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2022/10/06/typical-equivalence-relations-between-mathematical-spaces</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2022/10/06/typical-equivalence-relations-between-mathematical-spaces.html"><![CDATA[<ul>
  <li>Bijection: this is the equivalence in the sense of one-to-one and surjective mapping or correspondence between two sets.</li>
  <li>Isomorphism: this is the equivalence in the sense of algebraic structure of groups.</li>
  <li>Homeomorphism: this is the equivalence in the sense of topology and requires the forward map \(f\) and backward map \(f^{-1}\) to be continuous. It includes bijection.</li>
  <li>Isometry: this is the equivalence in the sense of metric.</li>
  <li>Differentiable homeomorphism: it is a homeomorphism with the forward map \(f\) being differentiable.</li>
  <li>Diffeomorphism: it is a homeomorphism with both the forward map \(f\) and backward map \(f^{-1}\) being differentiable.</li>
</ul>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="fundamental-concepts" /><summary type="html"><![CDATA[Bijection: this is the equivalence in the sense of one-to-one and surjective mapping or correspondence between two sets. Isomorphism: this is the equivalence in the sense of algebraic structure of groups. Homeomorphism: this is the equivalence in the sense of topology and requires the forward map \(f\) and backward map \(f^{-1}\) to be continuous. It includes bijection. Isometry: this is the equivalence in the sense of metric. Differentiable homeomorphism: it is a homeomorphism with the forward map \(f\) being differentiable. Diffeomorphism: it is a homeomorphism with both the forward map \(f\) and backward map \(f^{-1}\) being differentiable.]]></summary></entry><entry><title type="html">Understanding about submanifold and manifold</title><link href="https://jihuan-tian.github.io/math/2022/10/05/understanding-about-submanifold-and-manifold.html" rel="alternate" type="text/html" title="Understanding about submanifold and manifold" /><published>2022-10-05T00:00:00+08:00</published><updated>2022-10-05T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2022/10/05/understanding-about-submanifold-and-manifold</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2022/10/05/understanding-about-submanifold-and-manifold.html"><![CDATA[<p>For any point \(p\) of a submanifold \(M\) in an Euclidean space \(\mathbb{R}^{n+r}\), it is already assigned the global coordinates in \(\mathbb{R}^{n+r}\). Here \(\mathbb{R}^{n+r}\) plays the role of the absolute space-time proposed by Newton. Then, the submanifold \(M\) is defined as: for each point \(p\) in \(M\), there exists a neighborhood \(U\), where \(r\) coordinate components can be differentiably represented by the remaining \(n\) components. Therefore, these remaining coordinate components are indepedent, the number of which is the dimension of the submanifold \(M\).</p>

<p>Another understanding about submanifold is not based on the above <strong>explicit</strong> representation of some \(r\) coordinate components by the \(n\) independent components, but is described as the common locus of a set of constraint functions \(F(x)=0\) or \(F(x)=t\), where 0 or \(t\) belongs to \(\mathbb{R}^{r}\) and there are \(r\) equations in the system. Then we need to check the Jacobian matrix of the map \(F(x)\). If it has rank \(r\), \(M\) is a \(n\)-dimensional submanifold in \(\mathbb{R}^{n+r}\). With the help of implicit function theorem, this definition is equivalent to the first one.</p>

<p>Here we should bear in mind that <strong>the rank of the Jacobian matrix is actually the number of the constraints instead of the number of free variables or dimensions of the submanifold</strong>. Hence, the submanifold dimension is the co-dimension of \(r\) in \(\mathbb{R}^{r+n}\), i.e. \(n\).</p>

<p>For the definition of a manifold \(M\), the global Euclidean space is not mandatory and the absolute space-time notion is abandoned. Instead, it relies on two points. Assume there is an open covering of \(M\),</p>

<ol>
  <li>for each open set \(U\) in this covering, there is a one-to-one correspondence between \(U\) and an open set in \(\mathbb{R}^{n}\). N.B. At the moment, we only have this local bijection, but not a homeomorphism, since no topology has been constructed yet. In this way, each open set in the covering of \(M\) is assigned a coordinate chart.</li>
  <li>The coordinate transformation between any pair of these coordinate charts is differentiable.</li>
</ol>

<p>The definition of a submanifold \(M^r\) in the manifold \(M^n\) is similar to that for the submanifold in an Euclidean space. But now there is no <strong>global</strong> coordinate frame any more, but only a collection of <strong>local</strong> coordinate charts on \(M^n\). For each point \(p\) in \(M^r\), it must be contained in an open set \(U\) in the open covering of \(M^n\), which is assigned a local coordinate chart. With respect to this chart, \(U \cap M^r\) can be represented as a locus by a system of constraints, the Jacobian matrix of which has rank \(n-r\). Or in another way, \(n-r\) coordinate components can be locally and differentiably represented by the remaining \(r\) coordinate components.</p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="differential-geometry" /><summary type="html"><![CDATA[For any point \(p\) of a submanifold \(M\) in an Euclidean space \(\mathbb{R}^{n+r}\), it is already assigned the global coordinates in \(\mathbb{R}^{n+r}\). Here \(\mathbb{R}^{n+r}\) plays the role of the absolute space-time proposed by Newton. Then, the submanifold \(M\) is defined as: for each point \(p\) in \(M\), there exists a neighborhood \(U\), where \(r\) coordinate components can be differentiably represented by the remaining \(n\) components. Therefore, these remaining coordinate components are indepedent, the number of which is the dimension of the submanifold \(M\).]]></summary></entry><entry><title type="html">Verification of singularity order of BEM kernels</title><link href="https://jihuan-tian.github.io/math/2022/08/27/verification-of-singularity-order-of-bem-kernels.html" rel="alternate" type="text/html" title="Verification of singularity order of BEM kernels" /><published>2022-08-27T00:00:00+08:00</published><updated>2022-08-27T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2022/08/27/verification-of-singularity-order-of-bem-kernels</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2022/08/27/verification-of-singularity-order-of-bem-kernels.html"><![CDATA[<p>According to (Erichsen and Sauter 1998), kernel functions in BEM are
Gâteaux derivatives of the fundamental solution of the underlying
boundary value problem. For elliptic boundary value problems, a kernel
function \(k(x,y)\) has the general form</p>

\[k(x,y)=\sum_{\abs{\alpha}=t}^{t+a} S_{\alpha}(x,y)\eta_{\alpha}(\norm{x-y})\frac{(y-x)^{\alpha}}{\norm{y-x}^{\sigma+t}},\]

<p>where \(S_{\alpha}: \Gamma\times\Gamma \rightarrow \mathbb{C}\) are
analytic on each pair of smooth surface patches of \(\Gamma\). The
scalar function \(\eta_{\alpha}\) is also analytic.
\(\alpha\in \mathbb{N}_0^3\) is a multi-index and \(\sigma\) is defined
as the singularity order of \(k(x,y)\).</p>

<p>When \(\sigma\leq 2\), the kernel has Cauchy singularity. This means the
integral formed with this kernel exists in the sense of Cauchy principal
value, which can be evaluated by first removing an open ball
\(B_{\varepsilon}(x_0)\) around the singular point \(x_0\) from the
integration domain and then taking the limit
\(\varepsilon \rightarrow 0\). When \(\sigma&gt;2\), the kernel is
hyper-singular, which should be regularized by introducing surface curl
and integration by parts (Steinbach 2007).</p>

<p>For 3D Laplace problem, the kernel for the single layer potential is</p>

\[U^{*}(x,y)=\frac{1}{4\pi\norm{x-y}}.\]

<p>By letting \(t=0\), \(a=0\),
\(\sigma=1\), \(\alpha=(0,0,0)\),
\(S_{\alpha}(x,y)\cdot\eta_{\alpha}(\norm{y-x})=\frac{1}{4\pi}\), this
kernel can be obtained by evaluating the general form. Therefore, the
singularity order of the single layer potential kernel is 1.</p>

<p>The double layer potential kernel is</p>

\[K(x,y)=\gamma_{1,y}^{\rm int}U^{*}(x,y)=\frac{n(y)\cdot(x-y)}{4\pi \norm{x-y}^3},\]

<p>where \(\gamma_{1,y}^{\rm int}\) is the interior conormal derivative
operator. It can be derived from the general form with \(\sigma=2\),
\(t=1\), \(a=0\), \(\alpha_1=(1,0,0)\), \(\alpha_2=(0,1,0)\),
\(\alpha_3=(0,0,1)\) and</p>

\[\begin{aligned}
S_{\alpha_1}(x,y)\cdot\eta_{\alpha_1}(\norm{y-x}) &amp;= -\frac{n_1(y)}{4\pi} \\
S_{\alpha_2}(x,y)\cdot\eta_{\alpha_2}(\norm{y-x}) &amp;= -\frac{n_2(y)}{4\pi} \\
S_{\alpha_3}(x,y)\cdot\eta_{\alpha_3}(\norm{y-x}) &amp;= -\frac{n_3(y)}{4\pi} \\
\end{aligned}.\]

<p>Hence, the singularity order of the double layer
potential kernel is 2.</p>

<p>Similarly, for the adjoint double layer potential kernel</p>

\[K^{*}(x,y)=\gamma_{1,x}^{\rm int}U^{*}(x,y)=\frac{n(x)\cdot(y-x)}{4\pi \norm{x-y}^3},\]

<p>let</p>

\[\begin{aligned}
S_{\alpha_1}(x,y)\cdot\eta_{\alpha_1}(\norm{y-x}) &amp;= \frac{n_1(x)}{4\pi} \\
S_{\alpha_2}(x,y)\cdot\eta_{\alpha_2}(\norm{y-x}) &amp;= \frac{n_2(x)}{4\pi} \\
S_{\alpha_3}(x,y)\cdot\eta_{\alpha_3}(\norm{y-x}) &amp;= \frac{n_3(x)}{4\pi} \\
\end{aligned},\]

<p>while keeping other parameters fixed, we can see its
singularity order is still 2.</p>

<p>For the hyper-singular potential kernel,</p>

\[D(x,y)=-\gamma_{1,x}^{\rm int}\gamma_{1,y}^{\rm int}U^{*}(x,y)=\frac{1}{4\pi} \left[ -\frac{n(x) \cdot n(y)}{r^3} + \frac{3[n(y) \cdot (x - y)] [n(x) \cdot (x -y)]}{r^5} \right],\]

<p>we notice that the term \([n(y) \cdot (x - y)] [n(x) \cdot (x -y)]\) can
be expanded as</p>

\[\begin{aligned}
[n(y) \cdot (x - y)] [n(x) \cdot (x -y)] &amp;= n_1(x)n_1(y)(x_1-y_1)^2 + n_2(x)n_2(y)(x_2-y_2)^2 + n_3(x)n_3(y)(x_3-y_3)^2 \\
&amp;\quad + \left[ n_1(x)n_2(y) + n_2(x)n_1(y) \right](x_1-y_1)(x_2-y_2) \\
&amp;\quad + \left[ n_1(x)n_3(y) + n_3(x)n_1(y) \right](x_1-y_1)(x_3-y_3) \\
&amp;\quad + \left[ n_2(x)n_3(y) + n_3(x)n_2(y) \right](x_2-y_2)(x_3-y_3).
\end{aligned}\]

<p>This prompts us to define \(\alpha_1=(2,0,0)\),
\(\alpha_2=(0,2,0)\), \(\alpha_3=(0,0,2)\), \(\alpha_4=(1,1,0)\),
\(\alpha_5=(1,0,1)\), \(\alpha_6=(0,1,1)\) and</p>

\[\begin{aligned}
S_{\alpha_1}(x,y)\cdot\eta_{\alpha_1}(\norm{y-x}) &amp;= -\frac{n(x)\cdot n(y)}{4\pi} + \frac{3n_1(x)n_1(y)}{4\pi}\\
S_{\alpha_2}(x,y)\cdot\eta_{\alpha_2}(\norm{y-x}) &amp;= -\frac{n(x)\cdot n(y)}{4\pi} + \frac{3n_2(x)n_2(y)}{4\pi}\\
S_{\alpha_3}(x,y)\cdot\eta_{\alpha_3}(\norm{y-x}) &amp;= -\frac{n(x)\cdot n(y)}{4\pi} + \frac{3n_3(x)n_3(y)}{4\pi}\\
S_{\alpha_4}(x,y)\cdot\eta_{\alpha_4}(\norm{y-x}) &amp;= \frac{3[n_1(x)n_2(y)+n_2(x)n_1(y)]}{4\pi}\\
S_{\alpha_5}(x,y)\cdot\eta_{\alpha_5}(\norm{y-x}) &amp;= \frac{3[n_1(x)n_3(y)+n_3(x)n_1(y)]}{4\pi}\\
S_{\alpha_6}(x,y)\cdot\eta_{\alpha_6}(\norm{y-x}) &amp;= \frac{3[n_2(x)n_3(y)+n_3(x)n_2(y)]}{4\pi}\\
\end{aligned}.\]

<p>Then the kernel function can be derived from the general form with
\(\sigma=3\), \(t=2\), \(a=0\). Hence, the singularity order of the
hyper-singular potential kernel is 3.</p>

<h1 id="references">References</h1>

<div id="refs" class="references hanging-indent">

<div id="ref-ErichsenEfficient1998">

    <p>Erichsen, Stefan, and Stefan A. Sauter. 1998. “Efficient Automatic
Quadrature in 3-d Galerkin Bem.” <em>Computer Methods in Applied Mechanics and Engineering</em>, Papers presented at the seventh conference on
numerical methods and computational mechanics in science and
engineering, 157 (3): 215–24.
<a href="https://doi.org/10.1016/S0045-7825(97)00236-3">https://doi.org/10.1016/S0045-7825(97)00236-3</a>.</p>

  </div>

<div id="ref-SteinbachNumerical2007">

    <p>Steinbach, Olaf. 2007. <em>Numerical Approximation Methods for Elliptic Boundary Value Problems: Finite and Boundary Elements</em>. Springer Science
&amp; Business Media.</p>

  </div>

</div>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="BEM" /><summary type="html"><![CDATA[According to (Erichsen and Sauter 1998), kernel functions in BEM are Gâteaux derivatives of the fundamental solution of the underlying boundary value problem. For elliptic boundary value problems, a kernel function \(k(x,y)\) has the general form]]></summary></entry><entry><title type="html">Summary of Sobolev spaces and their norms</title><link href="https://jihuan-tian.github.io/math/2022/07/22/sobolev-spaces.html" rel="alternate" type="text/html" title="Summary of Sobolev spaces and their norms" /><published>2022-07-22T00:00:00+08:00</published><updated>2022-07-22T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2022/07/22/sobolev-spaces</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2022/07/22/sobolev-spaces.html"><![CDATA[<p>Norm of Sobolev space \(W_p^s(\Omega)\)</p>

<ul>
  <li>
    <p>When \(s\geq 0\)</p>

    <ul>
      <li>
        <p>When \(s\in\mathbb{N}_0\), write it as \(k\)</p>

\[\norm{u}_{W_p^k(\Omega)} =
      \begin{cases}
        \displaystyle{\left( \sum_{\abs{\alpha}\leq k} \norm{D^{\alpha}u}_{L_p(\Omega)}^p
          \right)^{\frac{1}{p}}} &amp; p \in [1,\infty) \\
        \displaystyle{\max_{\abs{\alpha}\leq k} \norm{D^{\alpha}u}_{L_{\infty}(\Omega)}} &amp; p = \infty
      \end{cases}\]
      </li>
      <li>
        <p>When \(s &gt; 0\) and is fractional, let it be \(s=k+\kappa\), with
\(k=\lfloor s \rfloor\) and \(\kappa\in(0,1)\)
\(\norm{u}_{W_p^s(\Omega)} = \left( \norm{u}_{W_p^k(\Omega)}^p + \abs{u}_{W_p^s(\Omega)}^p \right)^{\frac{1}{p}}\)</p>

        <p>where \(\abs{u}_{W_p^s(\Omega)}\) is the Sobolev-Slobodeckii
semi-norm</p>

\[\abs{u}_{W_p^s(\Omega)}^p = \sum_{\abs{\alpha}=k}\int_{\Omega}\int_{\Omega}
      \frac{\abs{D^{\alpha}u(x) - D^{\alpha}u(y)}^p}{\abs{x-y}^{d+p\kappa}} \intd x \intd y\]
      </li>
    </ul>
  </li>
  <li>
    <p>When \(s &lt; 0\) and \(p \in (1, \infty)\)</p>

    <ul>
      <li>\(W_p^s(\Omega) = \left( \overset{\circ}{W}_q^{-s} \right)'\) with
\(\frac{1}{p} +
    \frac{1}{q}=1\). Then its norm is the operator norm
\(\norm{u}_{W_p^s(\Omega)}=\sup_{0\neq v \in \overset{\circ}{W}_q^{-s}(\Omega)} \frac{\abs{\left\langle u,v \right\rangle_{\Omega}}}{\norm{v}_{\overset{\circ}{W}_q^{-s}(\Omega)}}\)</li>
      <li>\(\overset{\circ}{W}_p^s(\Omega) = \left( W_q^{-s}(\Omega) \right)'\),
its operator norm is
\(\norm{u}_{\overset{\circ}{W}_p^s(\Omega)} = \sup_{0\neq v \in W_{q}^{-s}(\Omega)} \frac{\abs{\left\langle u,v \right\rangle_{\Omega}}}{\norm{v}_{W_q^{-s}(\Omega)}}\)</li>
    </ul>
  </li>
</ul>

<p>Generate Sobolev spaces by taking closure</p>
<ul>
  <li>\(W_p^s(\Omega) = \overline{C^{\infty}(\Omega)}^{\norm{\cdot}_{W_p^s(\Omega)}}\)
with \(s \geq 0\) and \(p \in [1, \infty]\)</li>
  <li>\(\overset{\circ}{W}_p^s(\Omega) =
  \overline{C_0^{\infty}(\Omega)}^{\norm{\cdot}_{W_p^s(\Omega)}}\)
with \(s \geq 0\) and \(p \in [1,
  \infty]\)</li>
  <li>\(\widetilde{H}^s(\Omega) = \overline{C_0^{\infty}(\Omega)}^{\norm{\cdot}_{H^s(\mathbb{R}^d)}}
  = \overline{C_0^{\infty}(\Omega)}^{\norm{\cdot}_{W_2^s(\mathbb{R}^d)}}\)
with \(s\in \mathbb{R}\)</li>
  <li>\(H_0^s(\Omega) = \overline{C_0^{\infty}(\Omega)}^{\norm{\cdot}_{H^s(\Omega)}}\)</li>
</ul>

<p>Generate Sobolev spaces by taking restriction</p>
<ul>
  <li>\(H^s(\Omega) = \left\{ v=\widetilde{v}\vert_{\Omega}: \widetilde{v}\in H^s(\mathbb{R}^d) \right\}\)</li>
</ul>

<p>Duality relation between Sobolev spaces</p>
<ul>
  <li>\(W_p^s(\Omega) = \left( \overset{\circ}{W}_q^{-s} \right)'\) with
\(s &lt; 0\) and \(p \in (1, \infty)\)</li>
  <li>\(\overset{\circ}{W}_p^s(\Omega) = \left( W_q^{-s}(\Omega) \right)'\)
with \(s &lt; 0\) and \(p \in
  (1, \infty)\)</li>
  <li>\(\widetilde{H}^s(\Omega) = \left[ H^{-s}(\Omega) \right]'\) for all
\(s\in \mathbb{R}\) when \(\Omega\) is a Lipschitz domain</li>
  <li>\(H^s(\Omega)=\left[ \widetilde{H}^{-s}(\Omega) \right]'\) for all
\(s\in \mathbb{R}\) when \(\Omega\) is a Lipschitz domain</li>
  <li>\(H^s(\Gamma)=\left[ H^{-s}(\Gamma) \right]'\) for \(s&lt;0\)</li>
</ul>

<p>Equivalent Sobolev spaces</p>
<ul>
  <li>\(H^s(\mathbb{R}^d) = W_2^s(\mathbb{R}^d)\) for all \(s\in \mathbb{R}\)</li>
  <li>\(H^s(\Omega)=W_2^s(\Omega)\) for all \(s &gt; 0\) when \(\Omega\) is a
Lipschitz domain</li>
  <li>\(\widetilde{H}^s(\Omega)=H_0^s(\Omega)\) for \(s\geq 0\) and
\(s\neq \left\{
    \frac{1}{2},\frac{3}{2},\frac{5}{2}, \cdots \right\}\), when
\(\Omega\) is a Lipschitz domain</li>
</ul>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="PDE" /><category term="Sobolev-space" /><summary type="html"><![CDATA[Norm of Sobolev space \(W_p^s(\Omega)\)]]></summary></entry><entry><title type="html">Matrix assembly paradigms in BEM</title><link href="https://jihuan-tian.github.io/math/2022/06/09/matrix-assembly-paradigms-in-bem.html" rel="alternate" type="text/html" title="Matrix assembly paradigms in BEM" /><published>2022-06-09T00:00:00+08:00</published><updated>2022-06-09T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2022/06/09/matrix-assembly-paradigms-in-bem</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2022/06/09/matrix-assembly-paradigms-in-bem.html"><![CDATA[<ul>
  <li><strong>Assembly on one cell</strong>: for FEM matrices, such as the mass matrix \(M_{ij}=(\varphi_j,\varphi_i)\), the assembly is carried out by iterating over each cell. This is because the adopted basis functions for both the trial and test finite element spaces have compact support. As a cell-wise local patch or restriction of the global basis function associated with a node, a local basis function has non-zero interaction or overlapping integral only with those in a same cell.</li>
  <li><strong>Assembly on a pair of cells</strong>: for full matrices corresponding to boundary integral operators in BEM, such as \(\mathscr{V}, \mathscr{K}, \mathscr{K'}, \mathscr{D}\), the assembly is performed on each pair of cells in the interested boundary subdomains. Since the integral operators have long range interaction effect, the basis functions in any pair of cells have non-zero Galerkin integral, no matter if they are separated, in contact or overlapped.</li>
  <li><strong>Assembly on a pair of DoFs</strong>: for the approximation of boundary integral operators using \(\mathcal{H}\)-matrices, different methods are adopted for building far field and near field matrices. A far field matrix in the rank-k matrix format is built by using ACA. A near field matrix in the full matrix format is built element by element. Since the cluster trees associated with the \(\mathcal{H}\)-matrix are built based on the partition of DoF support points instead of cells, the assembly of a near field full matrix follows the paradigm of “on a pair of DoFs”.</li>
</ul>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="BEM" /><summary type="html"><![CDATA[Assembly on one cell: for FEM matrices, such as the mass matrix \(M_{ij}=(\varphi_j,\varphi_i)\), the assembly is carried out by iterating over each cell. This is because the adopted basis functions for both the trial and test finite element spaces have compact support. As a cell-wise local patch or restriction of the global basis function associated with a node, a local basis function has non-zero interaction or overlapping integral only with those in a same cell. Assembly on a pair of cells: for full matrices corresponding to boundary integral operators in BEM, such as \(\mathscr{V}, \mathscr{K}, \mathscr{K'}, \mathscr{D}\), the assembly is performed on each pair of cells in the interested boundary subdomains. Since the integral operators have long range interaction effect, the basis functions in any pair of cells have non-zero Galerkin integral, no matter if they are separated, in contact or overlapped. Assembly on a pair of DoFs: for the approximation of boundary integral operators using \(\mathcal{H}\)-matrices, different methods are adopted for building far field and near field matrices. A far field matrix in the rank-k matrix format is built by using ACA. A near field matrix in the full matrix format is built element by element. Since the cluster trees associated with the \(\mathcal{H}\)-matrix are built based on the partition of DoF support points instead of cells, the assembly of a near field full matrix follows the paradigm of “on a pair of DoFs”.]]></summary></entry><entry><title type="html">Discretization of bilinear forms in BEM</title><link href="https://jihuan-tian.github.io/math/2022/06/01/discretization-of-bilinear-forms.html" rel="alternate" type="text/html" title="Discretization of bilinear forms in BEM" /><published>2022-06-01T00:00:00+08:00</published><updated>2022-06-01T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2022/06/01/discretization-of-bilinear-forms</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2022/06/01/discretization-of-bilinear-forms.html"><![CDATA[<p>For mixed boundary value Laplace problem with \(\Gamma=\Gamma_{\rm D}\cup\Gamma_{\rm N}\), involved boundary integral operators are \(I, V, K, K', D\). The corresponding bilinear forms are \(b_I, b_V, b_K, b_{K'}, b_D\). The matrices for their discretization are \(\mathscr{I}, \mathscr{V}, \mathscr{K}, \mathscr{K'}, \mathscr{D}\). Then the system equation to be solved is as below.</p>

\[\begin{equation}
\begin{pmatrix}
  -\mathscr{V} &amp; \mathscr{K} \\ \mathscr{K}' &amp; \mathscr{D} \end{pmatrix}
\begin{pmatrix}
  t \big\vert_{\Gamma_{\rm D}} \\ u \big\vert_{\Gamma_{\rm N}} \end{pmatrix}=
\begin{pmatrix}
  -\frac{1}{2}\mathscr{I} - \mathscr{K} &amp; \mathscr{V} \\ \mathscr{-D} &amp; \frac{1}{2}\mathscr{I} - \mathscr{K}' \end{pmatrix}
\begin{pmatrix}
  g_D \\ g_N \end{pmatrix}
\end{equation}\]

<p>The bilinear forms related to the left hand side matrix blocks are:</p>

\[\begin{equation}
\begin{aligned}
b_V: H^{-\frac{1}{2}+s}(\Gamma_{\rm D}) \times H^{-\frac{1}{2}+s}(\Gamma_{\rm D}) \rightarrow \mathbb{C}\\
b_K: H^{\frac{1}{2}+s}(\Gamma_{\rm N}) \times H^{-\frac{1}{2}+s}(\Gamma_{\rm D}) \rightarrow \mathbb{C}\\
b_{K'}: H^{-\frac{1}{2}+s}(\Gamma_{\rm D}) \times H^{\frac{1}{2}+s}(\Gamma_{\rm N}) \rightarrow \mathbb{C}\\
b_D: H^{\frac{1}{2}+s}(\Gamma_{\rm N}) \times H^{\frac{1}{2}+s}(\Gamma_{\rm N}) \rightarrow \mathbb{C}
\end{aligned}
\end{equation}\]

<p>For the first row of the equation, the test function space, corresponding to the rows, is \(H^{-\frac{1}{2}+s}(\Gamma_{\rm D})\), which should be the same as the test function space for the right hand side. For the second row of the equation, the test function space is \(H^{\frac{1}{2}+s}(\Gamma_{\rm N})\). The matrices \(\mathscr{V}\) and \(\mathscr{D}\) corresponding to \(b_V\) and \(b_D\) respectively are symmetric. The matrix symmetry requires both the function space and the spatial domain to be the same for the two components in a bilinear form.</p>

<p>On the right hand side, the two mass matrices \(\mathscr{I}\) in the block matrix are actually different. We call them \(\mathscr{I}_1\) and \(\mathscr{I}_2\) from now on. The bilinear forms related to the matrix blocks on the right hand side are:</p>

\[\begin{equation}
\begin{aligned}
b_{I_1}:&amp; H^{\frac{1}{2}+s}(\Gamma_{\rm D})\times H^{-\frac{1}{2}+s}(\Gamma_{\rm D}) \rightarrow \mathbb{C} \\
b_K:&amp; H^{\frac{1}{2}+s}(\Gamma_{\rm D})\times H^{-\frac{1}{2}+s}(\Gamma_{\rm D}) \rightarrow \mathbb{C} \\
b_V:&amp; H^{-\frac{1}{2}+s}(\Gamma_{\rm N}) \times H^{-\frac{1}{2}+s}(\Gamma_{\rm D}) \rightarrow \mathbb{C}\\
b_D:&amp; H^{\frac{1}{2}+s}(\Gamma_{\rm D}) \times H^{\frac{1}{2}+s}(\Gamma_{\rm N}) \rightarrow \mathbb{C}\\
b_{I_2}:&amp; H^{-\frac{1}{2}+s}(\Gamma_{\rm N})\times H^{\frac{1}{2}+s}(\Gamma_{\rm N}) \rightarrow \mathbb{C} \\
b_{K'}:&amp; H^{-\frac{1}{2}+s}(\Gamma_{\rm N}) \times H^{\frac{1}{2}+s}(\Gamma_{\rm N}) \rightarrow \mathbb{C}
\end{aligned}
\end{equation}\]

<p>We can see \(\mathscr{I}_1\) and \(\mathscr{I}_2\) are not identity matrices. This further strengthens our understanding about the fact that they are mass matrices instead of identity matrices. Meanwhile, \(\mathscr{K}\) on the right hand side is different from \(\mathscr{K}\) on the left hand side. This also holds for \(\mathscr{K}'\), \(\mathscr{V}\) and \(\mathscr{D}\). Moreover, \(\mathscr{V}\) and \(\mathscr{D}\) on the right hand side are not symmetric matrices.</p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="BEM" /><summary type="html"><![CDATA[For mixed boundary value Laplace problem with \(\Gamma=\Gamma_{\rm D}\cup\Gamma_{\rm N}\), involved boundary integral operators are \(I, V, K, K', D\). The corresponding bilinear forms are \(b_I, b_V, b_K, b_{K'}, b_D\). The matrices for their discretization are \(\mathscr{I}, \mathscr{V}, \mathscr{K}, \mathscr{K'}, \mathscr{D}\). Then the system equation to be solved is as below.]]></summary></entry><entry><title type="html">Difference between interpolation and projection in FEM</title><link href="https://jihuan-tian.github.io/math/2022/05/24/difference-between-interpolation-and-projection-in-fem.html" rel="alternate" type="text/html" title="Difference between interpolation and projection in FEM" /><published>2022-05-24T00:00:00+08:00</published><updated>2022-05-24T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2022/05/24/difference-between-interpolation-and-projection-in-fem</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2022/05/24/difference-between-interpolation-and-projection-in-fem.html"><![CDATA[<p>Assume \(\mathcal{V}_h\) is a finite element space with the nodal basis \(\left\{ \phi_1,\phi_2,\cdots,\phi_n \right\}\). Using this basis as the trial space, the continuous function \(f\) can be discretized as \(f_h\) by following the ansatz \(f_h=\sum_{i=1}^n a_i\phi_i\). There are two ways to achieve this: interpolation and projection (or more specifically, \(L_2\)-projection).</p>

<p>Interpolation means ensuring \(f_h\) to have exact function values as \(f\) at all support points of the trial functions. The support point of a trial function is the point where its value equals one and this is obvious for a nodal basis function \(\phi_j\), because \(l_i(\phi_j) = \delta_{ij}\) should be satisfied, where \(l_i\) is the \(i\)-th degree of freedom in the dual space of the finite element space. Let \(\left\{ x_1,x_2,\cdots,x_n \right\}\) be the list of nodal points associated with the basis functions and evaluate \(f_h\) at each point</p>

\[f_h(x_k)=\sum_{i=1}^n a_i\phi_i(x_k)=\sum_{i=1}^n a_i\delta_{ik}=a_k \quad k=1,\cdots,n.\]

<p>Then \(a_k\) can be directly assigned with the value \(f(x_k)\).</p>

<p>To compute the \(L_2\)-projection of \(f\) onto \(\mathcal{V}_h\), we need to ensure that for any function \(v_h\) in \(\mathcal{V}_h\), its \(L_2\) inner product with \(f_h\) and \(f\) should be the same, i.e. \(f\) is equivalent to \(f_h\) in the sense of \(L_2\)-projection. Then the following equation should be solved</p>

\[(f_{h},\phi_i) =(f,\phi_i) \quad i=1,\cdots,n.\]

<p>This is equivalent to</p>

\[\left(\sum_{j=1}^n a_j\phi_j,\phi_i\right) =(f,\phi_i) \quad i=1,\cdots,n.\]

<p>In matrix form, this is \(M F_h = F\), where \(M_{ij}=\int_{\Omega}\phi_i\phi_j dx\) is the mass matrix, \(F_i=\int_{\Omega} f\phi_i dx\) and \(F_h=(a_1,\cdots,a_n)^T\) is the solution vector.</p>

<p>From above we can conclude the following points:</p>

<ol>
  <li>Unless the adopted finite element nodal basis is orthonormal, the mass matrix \(M\) is usually not an identity matrix and the \(L_2\)-projection of \(f\) onto \(\mathcal{V}_h\) is not a simple inner product with each basis function \(\phi_i\).</li>
  <li>Interpolation and \(L_2\)-projection of a function onto a finite element space usually generate different vectors of expansion coefficients for \(f_h\). Because interpolation produces accurate function values on support points, it is suitable for assigning the boundary or initial value data.</li>
  <li>From a differential geometry point of view, the mass matrix \(M\) is actually the metric tensor \(g\) (2-rank covariant tensor) for the function space \(\mathcal{V}_h\) with \(\left\{ \phi_1,\cdots,\phi_n \right\}\) as its basis. The above vector \(F\) derived from the right hand side of the equation, i.e. \((f,\phi_i), i=1,\cdots,n\), is a cotangent vector belonging to the dual space. After applying the inverse of the metric tensor \(M^{-1}\), which is equivalent to the \(\sharp\) operator in differential geometry, \(F\) is transformed to the tangent vector \(F_h\).</li>
</ol>

<p>Backlinks: <a href="/math/2024/07/04/domain-range-and-dual-spaces-in-bem.html">《Domain, range and dual spaces in BEM》</a></p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="FEM" /><summary type="html"><![CDATA[Assume \(\mathcal{V}_h\) is a finite element space with the nodal basis \(\left\{ \phi_1,\phi_2,\cdots,\phi_n \right\}\). Using this basis as the trial space, the continuous function \(f\) can be discretized as \(f_h\) by following the ansatz \(f_h=\sum_{i=1}^n a_i\phi_i\). There are two ways to achieve this: interpolation and projection (or more specifically, \(L_2\)-projection).]]></summary></entry><entry><title type="html">聚合物绝缘空间电荷动力学的理论模型与数值仿真研究进展</title><link href="https://jihuan-tian.github.io/math/2022/04/21/%E8%81%9A%E5%90%88%E7%89%A9%E7%BB%9D%E7%BC%98%E7%A9%BA%E9%97%B4%E7%94%B5%E8%8D%B7%E5%8A%A8%E5%8A%9B%E5%AD%A6%E7%9A%84%E7%90%86%E8%AE%BA%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%95%B0%E5%80%BC%E4%BB%BF%E7%9C%9F%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95.html" rel="alternate" type="text/html" title="聚合物绝缘空间电荷动力学的理论模型与数值仿真研究进展" /><published>2022-04-21T00:00:00+08:00</published><updated>2022-04-21T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2022/04/21/%E8%81%9A%E5%90%88%E7%89%A9%E7%BB%9D%E7%BC%98%E7%A9%BA%E9%97%B4%E7%94%B5%E8%8D%B7%E5%8A%A8%E5%8A%9B%E5%AD%A6%E7%9A%84%E7%90%86%E8%AE%BA%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%95%B0%E5%80%BC%E4%BB%BF%E7%9C%9F%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2022/04/21/%E8%81%9A%E5%90%88%E7%89%A9%E7%BB%9D%E7%BC%98%E7%A9%BA%E9%97%B4%E7%94%B5%E8%8D%B7%E5%8A%A8%E5%8A%9B%E5%AD%A6%E7%9A%84%E7%90%86%E8%AE%BA%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%95%B0%E5%80%BC%E4%BB%BF%E7%9C%9F%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95.html"><![CDATA[<p>原文：<a href="http://e-press.dwjs.com.cn/pcsee/weixin/2022-42-8-3037.html">张灵, 陈健宁, 周远翔, 田冀焕. 聚合物绝缘空间电荷动力学的理论模型与数值仿真研究进展. 中国电机工程学报, 2022, 42(8): 3037-3054.</a></p>

<h1 id="摘要">摘要</h1>

<p>强电场下聚合物空间电荷动力学过程的物理机制、实验观测及其与电气绝缘特性的关联是高电压绝缘领域的研究热点之一。目前，空间电荷测量技术的信号质量、空间分辨率与测量速度难以满足实际工程需求，而空间电荷动力学数值仿真能提供实验难以获得的数据以及发现规律，判定起主导作用的经典电导模型，有助于揭示绝缘老化机理、指导产品设计、预测服役寿命。该文首先介绍聚合物绝缘空间电荷动力学的基本理论和数学方程的一般形式，梳理2种典型绝缘结构的空间电荷仿真模型，并述评常用数值算法的特点。其次，介绍空间电荷数值仿真在基础电气性能表征、空间电荷包现象模拟以及电树枝老化特性阐释的初步应用。最后，针对空间电荷动力学模型算法的完善、微观材料参数的提取、多尺度仿真路线的提出、大尺寸和多层介质空间电荷的测量问题进行了展望。</p>

<h1 id="引言">引言</h1>

<p>聚合物绝缘材料在电力工业、航空航天、高铁船舶、武器装备、消费电子等领域具有重要的应用价值。随着电气设备朝着高电压、大电流、高功率、小型化等方向发展，越来越多极端环境的应用场合对聚合物绝缘材料、绝缘结构及绝缘系统的服役性能提出更加严苛的要求。</p>

<p>空间电荷和电导机理研究对聚合物绝缘材料走向应用具有重要指导意义。早期绝缘材料电导的经典物理模型主要是解释置于金属–绝缘体–金属或金属–绝缘体–半导体系统中薄膜试样的电流–电压规律。由于当时测量手段的局限，只能通过测量不同电压和温度下绝缘材料外部的稳态或暂态电流，基于预先选定的解析物理模型拟合实验数据，间接推断起主导作用的载流子注入和输运等机制，从中得出表征材料电导特性的物理参数。这包括反映电极–介质界面注入特性的Schottky图与Fowler-Nordheim图、描述体电导特性的Poole-Frenkel图与Arrhenius图、强电场下体现Mott-Gurney电导规律与空间电荷限制流(space charge limited current，SCLC)的log(J)~log(V)图、判断离子电导的log(σ)~1/T图和极化子电导的μ~1/T图等。可以看出，经典电导理论的提出往往基于二分法的思路对复杂现象进行简化处理，以便在有限的测量与计算条件下抓住问题的主要矛盾，例如电子电导与离子电导、低电场条件与高电场条件、低温范围与高温范围等。</p>

<p>20世纪80年代出现的空间电荷直接测量技术可以获得更加精细、实时的介质内部电荷与电场分布及其动态变化过程。此时，用于解释绝缘材料电压–电流的外部与整体关系的经典理论和解析模型便显得粗糙了。目前聚合物绝缘空间电荷直接测量技术仅适用于规则的平板或同轴结构，并且由于厚试样、微纳米分辨率、微纳秒高速动态、多维度等测量需求受测量原理、软件算法与硬件参数的限制，难以满足实际工程需求。</p>

<p>为了推动经典电导与空间电荷理论的进一步发展，以期为实验提供理性指导，为绝缘材料的改进与工程应用提供有力支撑，有必要构建描述空间电荷动态过程的动力学偏微分方程组，实现该方程组的数值求解，以及系统地开展包含不同物理过程与参数组合的仿真工作。空间电荷动力学数值仿真是集偏微分方程理论、电介质物理、高性能计算的前沿性交叉学科，始于1994年Alison等提出的空间电荷动力学数学模型，至今仍在快速发展，也是本文回顾与述评的重点。</p>

<p>在空间电荷动力学偏微分方程的求解方面，针对一阶双曲型对流–反应–扩散方程，要保证求解所得的电荷空间分布精度高、耗散低，同时能有效抑制甚至完全消除数值色散(即伪振荡)现象。方程的时间离散既要保证积分的数值精度，又要满足数值稳定性的要求。在电介质物理层面，需要在深刻理解与掌握经典电导与空间电荷理论的前提下，根据具体的实验现象，有理有据地将涉及载流子的不同效应引入方程组中，既要全面考虑不同物理因素的影响，又要通过定性估算忽略弱项，以减少计算量。同时，有必要将宏观尺度的仿真与介观、微观尺度的仿真相结合，以减少人为估计或根据实验数据拟合的参数个数，使空间电荷仿真结果从定性解释提升至定量预测的水平。在高性能计算方面，如何实现并行、稳定的数值算法，使其能够准确高效模拟出在常见的空间电荷测量时间范围内(如48h)完整的载流子动态过程，并满足参数遍历与优化的要求。</p>

<p>空间电荷的产生、迁移、入陷、脱陷、复合等过程会对材料的电学、力学、理化性能等产生影响。空间电荷的积聚导致局部电场增强，强电场下空间电荷包现象往往导致材料的击穿，严重影响聚合物绝缘材料的安全可靠服役。聚合物绝缘空间电荷动力学过程数值仿真研究对于解释绝缘材料老化、劣化、击穿与失效机理、提高绝缘材料性能，具有重要的理论价值和工程意义，有望解决材料开发周期长、成本高的难题。经过二十多年不断发展，空间电荷动力学数值仿真技术取得了初步应用。已有企业尝试将绝缘空间电荷效应纳入高电压设备绝缘裕度设计或主绝缘电场分布校核方法。然而，耦合空间电荷的电磁场数值仿真当前尚未得到实验的验证，仅限于现象解释和故障分析。</p>

<p>本文首先介绍聚合物绝缘空间电荷的基本理论模型，包括载流子产生、迁移、积聚和消散过程；总结空间电荷动力学数学方程的一般形式；述评不同学者对陷阱模型的考虑与处理。随后，阐述空间电荷仿真模型的改进与提升，包括同轴型绝缘结构和温度场变量的引入以及二维动力学模型的求解；对比几种空间电荷数值算法及其精度和效率；介绍空间电荷动力学数值仿真具体应用，包括基础电气性能表征、空间电荷包现象模拟以及电树枝老化特性阐释。最后，对空间电荷动力学模型算法的完善、微观材料参数的提取、多尺度仿真技术路线的提出、大尺寸和多层介质中空间电荷分布测量问题等方面进行了展望。</p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="space-charge" /><category term="paper" /><summary type="html"><![CDATA[原文：张灵, 陈健宁, 周远翔, 田冀焕. 聚合物绝缘空间电荷动力学的理论模型与数值仿真研究进展. 中国电机工程学报, 2022, 42(8): 3037-3054.]]></summary></entry><entry><title type="html">Prove a function space is a Banach space</title><link href="https://jihuan-tian.github.io/math/2022/03/25/prove-a-function-space-is-banach.html" rel="alternate" type="text/html" title="Prove a function space is a Banach space" /><published>2022-03-25T00:00:00+08:00</published><updated>2022-03-25T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2022/03/25/prove-a-function-space-is-banach</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2022/03/25/prove-a-function-space-is-banach.html"><![CDATA[<ul>
  <li>
    <p>Assume the function space \(X\) is assigned the norm \(\norm{\cdot}\).
Show that the definition of \(\norm{\cdot}\) is really a norm, i.e.
it should satisfy positive definiteness, scalar multiplication and
triangle inequality.</p>
  </li>
  <li>
    <p>\((X,\norm{\cdot})\) is a linear space, i.e. closeness of addition and
scalar multiplication should be proved in the sense of finite norm.</p>
  </li>
  <li>
    <p>Prove every Cauchy sequence in \(X\) is convergent.</p>

    <ul>
      <li>
        <p>Select a Cauchy sequence \(\left\langle u_n \right\rangle\) in the
function space \(X\) and show that it converges to a function \(u\)
pointwise. This is natural as long as the codomain of
\(\left\langle u_n \right\rangle\) which contain their actual
ranges is complete, which can be ensured when the codomain is
\(\mathbb{R}\) or \(\mathbb{C}\).</p>
      </li>
      <li>
        <p>Prove \(u \in X\) in the sense of finite norm.</p>
      </li>
      <li>
        <p>Up to now, the convergence of \(u_n\) to \(u\) is still in the
pointwise sense. Hence we still need to prove \(u_n\) converges to
\(u\) in the norm assigned to \(X\).</p>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="real-analysis" /><summary type="html"><![CDATA[Assume the function space \(X\) is assigned the norm \(\norm{\cdot}\). Show that the definition of \(\norm{\cdot}\) is really a norm, i.e. it should satisfy positive definiteness, scalar multiplication and triangle inequality.]]></summary></entry><entry><title type="html">Summary of multi-index convention for partial derivatives</title><link href="https://jihuan-tian.github.io/math/2022/02/04/summary-of-multi-index-convention-for-partial-derivatives.html" rel="alternate" type="text/html" title="Summary of multi-index convention for partial derivatives" /><published>2022-02-04T00:00:00+08:00</published><updated>2022-02-04T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2022/02/04/summary-of-multi-index-convention-for-partial-derivatives</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2022/02/04/summary-of-multi-index-convention-for-partial-derivatives.html"><![CDATA[<p>Let \(d\) be the spatial dimension. \(\alpha = (\alpha_1, \cdots, \alpha_d) \in \mathbb{N}_0^d\).</p>

\[\lvert \alpha \rvert = \sum_{i = 1}^d \alpha_i\]

<p>The factorization of a multi-index is equal to the product of the factorization of each component index:</p>

\[\alpha ! = \prod_{i = 1}^d \alpha_i !\]

<p>The multi-index is distributed to each coordinate component of \(x \in \mathbb{R}^d\):</p>

\[x^{\alpha} = \prod_{i = 1}^d x_i^{\alpha_i}\]

<p>The \(\lvert \alpha \rvert\)-fold mixed derivative with respect to \(x\):</p>

\[\begin{array}{rl} \partial_x^{\alpha} &amp; = \prod_{i = 1}^d \partial_{x_i}^{\alpha_i} \\ \partial_x^{\alpha} x^{\beta} &amp; = \frac{\beta !}{(\beta - \alpha) !} x^{\beta - \alpha} \quad (\alpha \leq \beta) \end{array}\]

<p>where \(\beta \in \mathbb{N}_0^d\), \(\alpha \leq \beta\) means \(\alpha_i \leq \beta_i\) for \(i = 1, \cdots, d\) and \(\beta - \alpha\) is the normal subtraction of two vectors. When \(\beta \equiv \alpha\),</p>

\[\partial_x^{\alpha} x^{\alpha} = \alpha !\]

<p>The combination for multi-index:</p>

\[C_{\alpha}^{\beta} = \prod_{i = 1}^d C_{\alpha_i}^{\beta_i} \quad (\beta \leq \alpha)\]

<p>Proposition 1. (Leibniz formula for \(\lvert \alpha \rvert\)-fold partial derivatives)</p>

\[\partial_x^{\alpha} (uv) = \sum_{\beta \leq \alpha} C_{\alpha}^{\beta} (\partial_x^{\beta} u) (\partial_x^{\alpha - \beta} v),\]

<p>where \(\alpha\) and \(\beta\) are multi-indices and \(x \in \mathbb{R}^d\).</p>

<p>Proof 1. When \(d=1\), it is the classical Leibiniz formula in calculus.</p>

<p>2. Assume it holds for \(d=n\). Then for \(d=n+1\), we have</p>

\[\begin{equation*}
    \pdiff_x^{\alpha}(uv) =\left( \prod_{i=1}^{n+1}\pdiff_{x_i}^{\alpha_i} \right)(uv) =
    \pdiff_{x_1}^{\alpha_1} \left( \prod_{i=2}^{n+1}\pdiff_{x_i}^{\alpha_i} \right)
\end{equation*}\]

<p>Let \(\tilde{\alpha}=(\alpha_2,\cdots,\alpha_{n+1})\) and \(\tilde{x}=(x_2,\cdots,x_{n+1})\),</p>

\[\begin{equation*}
  \begin{split}
      \text{Above} &amp;= \pdiff_{x_1}^{\alpha_1}
      \pdiff_{\tilde{x}}^{\tilde{\alpha}}(uv) = \pdiff_{x_1}^{\alpha_1}\left(
        \sum_{\tilde{\beta}\leq\tilde{\alpha}}C_{\tilde{\alpha}}^{\tilde{\beta}}(\pdiff_x^{\tilde{\beta}}u)(\pdiff_x^{\tilde{\alpha}-\tilde{\beta}}v)
      \right) \\
      &amp;= \sum_{\tilde{\beta}\leq\tilde{\alpha}}C_{\tilde{\alpha}}^{\tilde{\beta}}
      \underbrace{\pdiff_{x_1}^{\alpha_1}\left[(\pdiff_x^{\tilde{\beta}}u)(\pdiff_x^{\tilde{\alpha}-\tilde{\beta}}v)\right]}_{\text{Apply the Leibniz formula for $d=1$}} \\
      &amp;= \sum_{\tilde{\beta}\leq\tilde{\alpha}}C_{\tilde{\alpha}}^{\tilde{\beta}}\left[
        \sum_{\beta_1\leq\alpha_1}C_{\alpha_1}^{\beta_1}(\pdiff_{x_1}^{\beta_1}\pdiff_{\tilde{x}}^{\tilde{\beta}}u)(\pdiff_{x_1}^{\alpha_1-\beta_1}\pdiff_{\tilde{x}}^{\tilde{\alpha}-\tilde{\beta}}v)
      \right] \\
      &amp;= \sum_{\tilde{\beta}\leq\tilde{\alpha}}\sum_{\beta_1\leq\alpha_1}C_{\tilde{\alpha}}^{\tilde{\beta}}C_{\alpha_1}^{\beta_1}(\pdiff_x^{\beta}u)(\pdiff_x^{\alpha-\beta}v) \\
      &amp;= \sum_{\beta\leq\alpha}C_{\alpha}^{\beta}(\pdiff_x^{\beta}u)(\pdiff_x^{\alpha-\beta}v).
  \end{split}
\end{equation*}\]

<p>3. Apply the principle of mathematical induction, the proposition is proved.</p>

<p>Proposition 2. (Taylor expansion for multi-dimensional functions) Let \(f : X \rightarrow \mathbb{R}\) be a function from \(C^m (X)\) with \(X \subset \mathbb{R}^d\). Let \(x_0 \in X\) be an expansion center. The Taylor expansion around \(x_0\) is</p>

\[f (x) = \sum_{\text{$\begin{array}{c} \alpha \in \mathbb{N}_0^d\\ \lvert \alpha \rvert \leq m \end{array}$}} (x - x_0)^{\alpha} \frac{1}{\alpha !} \partial_x^{\alpha} f (x_0) + R_r,\]

<p>where \(R_r\) is the high order remainder.</p>

<p>Proposition 3. The total number of terms in the \(m\)-th order Taylor expansion for a function defined on \(X \subset \mathbb{R}^d\) is \(C_{m + d}^d\).</p>

<p>Proof  The number of terms in the Taylor expansion for \(\abs{\alpha}=k\) is equivalent to the number of ways for distributing the \(k\) times of partial derivatives to the \(d\) coordinate components, some of which may be assigned no derivatives at all. Hence the answer is \(C_{k+d-1}^{d-1}\). The total number of terms in the Taylor expansion is</p>

\[\begin{equation*}
  \begin{split}
    \sum_{k=0}^{m}C_{k+d-1}^{d-1} &amp;= C_{d-1}^{d-1} + C_d^{d-1} + \cdots + C_{m+d-1}^{d-1} \\
    \text{Replace $C_{d-1}^{d-1}$ with $C_d^d$,} \\
    &amp;= C_d^d + C_d^{d-1} + C_{d+1}^{d-1} + \cdots + C_{m+d-1}^{d-1} \\
    \text{Apply $C_n^{m-1} + C_n^m = C_{n+1}^m$,} \\
    &amp;= C_{d+1}^d + C_{d+1}^{d-1} + \cdots + C_{m+d-1}^{d-1} \\
    &amp;= C_{m+d}^d.
  \end{split}
\end{equation*}\]

<p>Comment For Sobolev norms in <sup id="fnref:Ada75" role="doc-noteref"><a href="#fn:Ada75" class="footnote" rel="footnote">1</a></sup>, there are also partial derivatives (in the weak sense) of different orders:</p>

\[\lVert u \rVert_{m, p} = \left( \sum_{\lvert \alpha \rvert \leq m} \lVert \partial^{\alpha} u \rVert_p^p \right)^{1 / p} \quad (1 \leq p &lt; \infty) .\]

<p>References</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:Ada75" role="doc-endnote">
      <p>Robert Alexander Adams. Sobolev Spaces (Pure and applied mathematics, a series of monographs and textbooks).  PURE AND APPLIED MATHEMATICS: A series of Monographs and Textbooks ISBN: 9780120441501. Academic Press, June 1975. <a href="#fnref:Ada75" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="PDE" /><summary type="html"><![CDATA[Let \(d\) be the spatial dimension. \(\alpha = (\alpha_1, \cdots, \alpha_d) \in \mathbb{N}_0^d\).]]></summary></entry><entry><title type="html">Isometric embedding of metric space</title><link href="https://jihuan-tian.github.io/math/2020/09/26/isometric-embedding-of-metric-space.html" rel="alternate" type="text/html" title="Isometric embedding of metric space" /><published>2020-09-26T00:00:00+08:00</published><updated>2020-09-26T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2020/09/26/isometric-embedding-of-metric-space</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2020/09/26/isometric-embedding-of-metric-space.html"><![CDATA[<p>This post summarizes the proof for Theorem 7.9 in Royden’s “Real Analysis”.</p>

<p><strong>Theorem 9</strong> If \(\langle X, \rho \rangle\) is an incomplete metric space, it is possible to find a complete metric space \(X^*\) in which \(X\) is isometrically embedded as a dense subset. If \(X\) is contained in an arbitrary complete space \(Y\), then \(X^*\) is isometric with the closure of \(X\) in \(Y\).</p>

<p><strong>Analysis:</strong> The proof about the completion of \(X\) is not straightforward because this problem is given in an abstract way that there is nothing to manipulate or we cannot construct a specific example as the completion of \(X\) for facilitating our understanding. Hence a new structure must be introduced to fulfill this purpose, which is the set of equivalence classes \(X^*\) derived from the collection of all Cauchy sequences in \(X\). Then it is to be proved that \(X\) is identified with \(F(X)\) in \(X^*\), where \(F\) is an isometry from \(X\) to \(F(X)\) and \(F(X)\) is dense in \(X^*\).</p>

<p>The proof of this theorem is divided into 5 steps as suggested by Exercise 17.</p>

<ol>
  <li>
    <p>If \(\{x_n\}_{n \geq 1}\) and \(\{y_n\}_{n \geq 1}\) are Cauchy sequences from a metric space \(X\), then \(\{\rho(x_n, y_n)\}_{n \geq 1}\) converges.</p>

    <p><strong>Proof:</strong> Because \(\langle X, \rho \rangle\) is an incomplete space, the two sequences \(\{x_n\}_{n \geq 1}\) and \(\{y_n\}_{n \geq 1}\) do not necessarily have their limits within \(X\). However, this proposition indicates that the inter-distance between \(x_n\) and \(y_n\) does converge when \(n\) approaches to \(\infty\).</p>

    <p>Because \(\rho(x_n, y_n) \in \mathbb{R}\) and \(\mathbb{R}\) is complete, to prove \(\{\rho(x_n, y_n)\}_{n \geq 1}\) converges, we need to show that it is a Cauchy sequence.</p>

    <p>From the given condition, for any \(\varepsilon &gt; 0\), there exists \(N \in \mathbb{N}\) such that when \(m_1, m_2 &gt; N\), \(\abs{x_{m_1} - x_{m_2}} &lt; \frac{\varepsilon}{2}\) and when \(n_1, n_2 &gt; N\), \(\abs{y_{n_1} - y_{n_2}} &lt; \frac{\varepsilon}{2}\).</p>

    <p>For any \(m, n &gt; N\),</p>

\[\abs{\rho(x_n, y_n) - \rho(x_m ,y_m)} = \abs{\rho(x_n, y_n) - \rho(x_n, y_m) + \rho(x_n, y_m) - \rho(x_m, y_m)}.\]

    <p>Due to the triangle inequality satisfied by the metric \(\rho\), we have</p>

\[\rho(x_n, y_n) \leq \rho(x_n, y_m) + \rho(y_m, y_n)\]

    <p>and</p>

\[\rho(x_n, y_m) \leq \rho(x_n, y_n) + \rho(y_n, y_m).\]

    <p>Hence,</p>

\[\rho(x_n, y_n) - \rho(x_n, y_m) \leq \rho(y_m, y_n)\]

    <p>and</p>

\[\rho(x_n, y_m) - \rho(x_n, y_n) \leq \rho(y_m, y_n),\]

    <p>which is actually</p>

\[\abs{\rho(x_n, y_n) - \rho(x_n, y_m)} \leq \rho(y_m, y_n).\]

    <p>This is just a variation of the triangle inequality for a metric which says that the difference between the lengths of two edges of a triangle is smaller than or equal to the length of the third edge. Similarly, we have</p>

\[\abs{\rho(x_n, y_m) - \rho(x_m, y_m)} \leq \rho(x_m, x_n).\]

    <p>Then</p>

\[\begin{aligned}
\abs{\rho(x_n, y_n) - \rho(x_m, y_m)} &amp;= \abs{\rho(x_n, y_n) - \rho(x_n, y_m) + \rho(x_n, y_m) - \rho(x_m, y_m)} \\
&amp; \leq \abs{\rho(x_n, y_n) - \rho(x_n, y_m)} + \abs{\rho(x_n, y_m) - \rho(x_m, y_m)} \\
&amp; \leq \rho(y_m, y_n) + \rho(x_m, x_n) \\
&amp; &lt; \frac{\varepsilon}{2} + \frac{\varepsilon}{2} \\
&amp; = \varepsilon.
\end{aligned}\]

    <p>Therefore, \(\{\rho(x_n, y_n)\}_{n \geq 1}\) is a Cauchy sequence in \(\mathbb{R}\) and converges to some \(a\) in \(\mathbb{R}\).</p>
  </li>
  <li>
    <p>The set of all Cauchy sequences from a metric space \(X\) becomes a pseudometric space if \(\rho^*(\{x_n\}_{n \geq 1}, \{y_n\}_{n \geq 1}) = \lim_{n \rightarrow \infty} \rho(x_n, y_n)\)</p>

    <p><strong>Proof:</strong> Because \(\{\rho(x_n, y_n)\}_{n \geq 1}\) is convergent, the above definition of \(\rho^*(\{x_n\}_{n \geq 1}, \{y_n\}_{n \geq 1})\) is meaningful. It can be verified that \(\rho^*\) satisfies the conditions of positiveness, symmetry and triangle inequality, which are derived from those of \(\rho\). Then we only need to find two different Cauchy sequences, the distance between which is zero, so that \(\rho^*\) is a pseudometric.</p>

    <p>Let \(\{z_k\}_{k \geq 1}\) be a Cauchy sequence. Let \(x_n = z_{2n-1}\) and \(y_n = z_{2n}\). Then \(\{x_n\}_{n \geq 1}\) and \(\{y_n\}_{n \geq 1}\) are two different Cauchy sequences with \(\rho^*(\{x_n\}_{n \geq 1}, \{y_n\}_{n \geq 1}) = \lim_{n \rightarrow \infty} \rho(x_n, y_n) = 0\). Therefore, \(\rho^*\) defined on the collection of all Cauchy sequences in \(X\) is a pseudometric.</p>
  </li>
  <li>
    <p>This pseudometric space becomes a metric space \(X^*\) when we identify elements for which \(\rho^* = 0\) and \(X\) is isometrically embedded in \(X^*\).</p>

    <p><strong>Proof:</strong> According to Exercise 3 in Section 1, by letting \(R := \{\rho^*(\{x_n\}_{n \geq 1}, \{y_n\}_{n \geq 1}) = 0\}\) be the equivalence condition on the set of all Cauchy sequences in \(X\), the obtained collection of equivalence classes \(X^*\) is a metric space. This can be verified as below.</p>

    <p>Let \(\mathcal{X}\) and \(\mathcal{Y}\) be two different equivalence classes in \(X^*\). Let \(x_0\) be the representative element of \(\mathcal{X}\) and \(y_0\) be that of \(\mathcal{Y}\). Then for any \(x\) in \(\mathcal{X}\) and any \(y\) in \(\mathcal{Y}\), we have \(\rho^*(x, y) = \rho^*(x_0, y_0) = \rho^*(\mathcal{X}, \mathcal{Y}) \geq 0\). If \(\rho^*(\mathcal{X}, \mathcal{Y}) = 0\), \(\rho^*(x_0, y_0) = 0\) and for any \(y\) in \(\mathcal{Y}\), \(\rho^*(x_0, y) = 0\). Because of the equivalence relation \(R\), \(y\) belongs to \(\mathcal{X}\). Similarly, for any \(x\) in \(\mathcal{X}\), \(x\) belongs to \(\mathcal{Y}\). Therefore, \(\rho^*(\mathcal{X}, \mathcal{Y}) = 0\) implies \(\mathcal{X} = \mathcal{Y}\). On the other hand, when \(\mathcal{X} = \mathcal{Y}\), \(\rho^*(\mathcal{X}, \mathcal{Y}) = \rho^*(x_0, x_0) = 0\). So \(\rho^*\) has the property of positive definitiveness.</p>

    <p>The commutativity of \(\rho^*\) is obvious, which is derived from that of \(\rho\).</p>

    <p>Finally, for \(\mathcal{X}\), \(\mathcal{Y}\), \(\mathcal{Z}\) in \(X^*\) with their respective representative elements \(x_0\), \(y_0\) and \(z_0\), \(\rho(\mathcal{X}, \mathcal{Y}) = \rho(x_0, y_0)\), \(\rho(\mathcal{X}, \mathcal{Z}) = \rho(x_0, z_0)\) and \(\rho(\mathcal{Z}, \mathcal{Y}) = \rho(z_0, y_0)\). Because \(\rho(x_0, y_0) \leq \rho(x_0, z_0) + \rho(z_0, y_0)\), we have the triangle inequality for \(\rho^*\). Therefore, \(X^*\) with \(\rho^*\) is a metric space.</p>

    <p>Next, let \(F: X \rightarrow X^*\) which associates each \(x\) in \(X\) with the equivalence class in \(X^*\) that contains the Cauchy sequence \(\{x, x, \cdots\}\). It is easy to see that for any \(x\), \(y\) in \(X\),</p>

\[\rho^*(F(x), F(y)) = \rho^*(\{x, x, \cdots \}, \{y, y, \cdots\}) = \lim_{n \rightarrow \infty} \rho(x, y) = \rho(x, y).\]

    <p>Meanwhile, if \(F(x) = F(y)\), we have \(\rho^*(F(x), F(y)) = \rho(x, y) =\rho(x, x) = 0\). Because \(\rho\) is a standard metric, \(x = y\). Hence, \(F\) is injective. For any open ball \(B(\{x, x, \cdots\}, \varepsilon)\) with a radius \(\varepsilon\) in \(F(X)\), its inverse image under \(F^{-1}\) is \(B(x, \varepsilon)\) in \(X\), which is open in \(X\). Then \(F\) is a continuous map. Similarly, \(F^{-1}\) is also continuous. Therefore, \(F\) is a homeomorphism between \(X\) and \(F(X)\). Moreover, because \(\rho^*(F(x), F(y)) = \rho(x, y)\), \(F\) is an isometry.</p>

    <p>Then, we will prove \(X\) is isometrically embedded in \(X^*\) as a dense subset.</p>

    <p>Let \(B(\{x_n\}_{n \geq 1}, \varepsilon)\) be an open ball in \(X^*\), which is centered at an arbitrary element \(\{x_n\}_{n \geq 1}\) in \(X^*\). Because \(\{x_n\}_{n \geq 1}\) is a Cauchy sequence, there exists \(N\) in \(\mathbb{N}\) such that when \(m, n &gt; N\), \(\abs{x_m - x_n} &lt; \varepsilon\). Then \(\rho^*(\{x_n\}_{n \geq 1}, \{x_m, x_m, \cdots\}) = \lim_{n \rightarrow \infty} \abs{x_n - x_m} &lt; \varepsilon\). Hence \(\{x_m, x_m, \cdots\}\) belongs to \(B(\{x_n\}_{n \geq 1}, \varepsilon)\) and \(F(X)\) is dense in \(X^*\). Because \(F\) is an isometry from \(X\) to \(F(X)\), \(X\) can be identified with \(F(X)\). Therefore, \(X\) is isometrically embedded in \(X^*\).</p>
  </li>
  <li>
    <p>The metric space \(\langle X^*, \rho^* \rangle\) is complete. (N.B. What is convergent here is a sequence of Cauchy sequences.)</p>

    <p><strong>Proof:</strong> Let \(\{x_n\}_{n \geq 1}\) be any Cauchy sequence in \(X\). We can extract a subsequence from it as \(\{x_{n_k}\}_{k \geq 1}\) such that \(\rho(x_{n_k}, x_{n_{k+1}}) &lt; 2^{-k}\). This subsequence can be rewritten as \(\{\tilde{x}_k\}_{k \geq 1}\) with the condition \(\rho(\tilde{x}_k, \tilde{x}_{k+1}) &lt; 2^{-k}\). Then we select an arbitrary Cauchy sequence of such sequences as \(\{S_m\}_{m \geq 1}\) with \(S_m = \{\tilde{x}_{k,m}\}_{k \geq 1}\) satisfying for any \(\varepsilon &gt; 0\), there exists \(N\) in \(\mathbb{N}\) such that when \(m, n &gt; N\), \(\rho^*(S_m, S_n) = \lim_{k \rightarrow \infty} \rho(\tilde{x}_{k,m}, \tilde{x}_{k,n}) &lt; \varepsilon\). This suggests that there exists \(K\) in \(\mathbb{N}\) such that when \(k &gt; K\), \(\rho(\tilde{x}_{k,k}, \tilde{x}_{k,n}) &lt; \varepsilon\). This Cauchy sequence of Cauchy sequences can be illustrated as a 2-dimensional matrix with infinite length as below,</p>

\[\begin{pmatrix}
\tilde{x}_{1,1} &amp; \tilde{x}_{1,2} &amp; \tilde{x}_{1,3} &amp; \cdots \\
\tilde{x}_{2,1} &amp; \tilde{x}_{2,2} &amp; \tilde{x}_{2,3} &amp; \cdots \\
\tilde{x}_{3,1} &amp; \tilde{x}_{3,2} &amp; \tilde{x}_{3,3} &amp; \cdots \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots
\end{pmatrix},\]

    <p>from which we extract the diagonal elements to construct a new sequence \(S^* = \{\tilde{x}_{k,k}\}_{k \geq 1}\). For any \(\varepsilon &gt; 0\), there exists \(N\) in \(\mathbb{N}\) such that when \(m, n &gt; N\), \(\rho(\tilde{x}_{m,m}, \tilde{x}_{n,n}) &lt; \varepsilon\). Hence \(S^*\) is a Cauchy sequence so it belongs to \(X^*\). The distance between \(S_m\) and \(S^*\) is \(\rho^*(S_m, S^*) = \lim_{k \rightarrow \infty} \rho(\tilde{x}_{k,m}, \tilde{x}_{k,k})\). It is obvious that for any \(\varepsilon &gt; 0\), when \(m &gt; N\) and \(k &gt; K\), \(\rho(\tilde{x}_{k,m}, \tilde{x}_{k,k}) &lt; \varepsilon\). Therefore, \(\lim_{m \rightarrow \infty} \rho^*(S_m, S^*) = 0\) and \(\langle X^*, \rho^* \rangle\) is complete.</p>

    <p>Up to now, the first part of Theorem 9 is proved, i.e. we have found the completion of \(X\) as \(X^*\) in which \(X\) is isometrically embedded as a dense subset.</p>
  </li>
  <li>
    <p>The above isometry \(F\) from \(X\) to \(F(X)\) in \(X^*\) is uniformly continuous, which is because for any \(x\) and \(y\) in \(X\) such that \(\rho(x, y) &lt; \varepsilon\), \(\rho^*(F(x), F(y)) = \lim_{n \rightarrow \infty} \rho(x, y) = \rho(x, y) &lt; \varepsilon\). Then according to Proposition 11 in Section 5 of this Chapter, viewing \(X\) as a subset of \(Y\), \(F\) is a uniformly continuous mapping from \(X\) into the complete space \(X^*\). Then there exists a unique continuous extension \(G\) of \(F\) from \(X\) to \(\overline{X}\) with \(\overline{X}\) being the closure of \(X\) with respect to the standard topology induced by the metric. Because \(Y\) is also complete with respect to this topology, \(\overline{X}\) is contained within \(Y\). Also note that, for any \(x\) in \(\overline{X}\) but not in \(X\), there exists a Cauchy sequence \(\{x_n\}_{n \geq 1}\) in \(X\) convergent to \(x\). Then the value \(G(x)\) only depends on \(x\), i.e. \(G(x) = \lim_{n \rightarrow \infty} F(x_n)\).</p>

    <p>Due to Proposition 10 in Section 5, when \(\{x_n\}_{n \geq 1}\) is a Cauchy sequence in \(X\), \(\{F(x_n)\}_{n \geq 1}\) is also a Cauchy sequence because \(F\) is uniformly continuous. Therefore, \(G(x)\) belongs to the closure of \(F(X)\) in \(X^*\), i.e.  \(\overline{F(X)}\). Meanwhile, for any \(y\) in \(\overline{F(X)}\), there exists a Cauchy sequence \(\{y_n\}_{n \geq 1}\) in \(F(X)\) and \(\{x_n\}_{n \geq 1}\) in \(X\) such that \(x_n = F^{-1}(y_n)\). Because \(F\) is an isometry from \(X\) to \(F(X)\), \(F^{-1}\) is an isometry from \(F(X)\) to \(X\) and hence \(F^{-1}\) is also uniformly continuous. Therefore, \(\{x_n\}_{n \geq 1}\) is a Cauchy sequence in \(X\). Then, according to the definition of \(G\), let \(x\) in \(\overline{X}\) and \(x = \lim_{n \rightarrow \infty} x_n\), we have \(G(x) = y\). This means that the actual range of \(G\) is \(\overline{F(X)} = X^*\).</p>

    <p>On the other hand, viewing \(F(X)\) as a subset of \(X^*\), \(F^{-1}\) is an isometry from \(F(X)\) to \(X \subset \overline{X}\), which is also uniformly continuous. Then there exists a unique extension \(H\) of \(F^{-1}\) from \(F(X)\) to \(\overline{F(X)} = X^*\). So \(H\) is a map from \(X^*\) into \(Y\). With a similar analysis as that for \(G\), the actual range of \(H\) is \(\overline{X}\).</p>

    <p>Then we have \(H \circ G = {\rm id}_{\overline{X}}\) and \(G \circ H = {\rm id}_{X^*}\). Therefore, \(G\) is the inverse of \(H\) and vice versa. Because \(G\) is uniformly continuous, \(G\) is a homeomorphism. Then we need to prove \(G\) is isometric. We already know that when \(G\) constrained to \(X\), \(G\vert_X = F\) is isometric. Furthermore, for any \(x_1\) and \(x_2\) in \(\overline{X}\), we should prove \(\rho(x_1, x_2) = \rho^*(\{a_n\}_{n \geq 1}, \{b_n\}_{n \geq 1})\) where \(a_n \rightarrow x_1\) and \(b_n \rightarrow x_2\), which is quite obvious: \(\rho^*(\{a_n\}_{n \geq 1}, \{b_n\}_{n \geq 1}) = \lim_{n \rightarrow \infty} \rho(a_n, b_n) = \rho(x_1, x_2)\). Hence, \(G\) is an isometry between \(\overline{X}\) and \(X^*\).</p>
  </li>
</ol>

<p>Backlinks: <a href="/thoughts/2020/09/26/%E5%8B%A4%E9%9D%A1%E4%BD%99%E5%8A%B3-%E5%BF%83%E6%9C%89%E5%B8%B8%E9%97%B2.html">《勤靡余劳，心有常闲》</a></p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="real-analysis" /><summary type="html"><![CDATA[This post summarizes the proof for Theorem 7.9 in Royden’s “Real Analysis”.]]></summary></entry><entry><title type="html">Mindmap for “Principles of Boundary Element Methods”</title><link href="https://jihuan-tian.github.io/math/2019/12/25/Mindmap-for-Principles-of-boundary-element-methods.html" rel="alternate" type="text/html" title="Mindmap for “Principles of Boundary Element Methods”" /><published>2019-12-25T00:00:00+08:00</published><updated>2019-12-25T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2019/12/25/Mindmap%20for%20Principles%20of%20boundary%20element%20methods</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2019/12/25/Mindmap-for-Principles-of-boundary-element-methods.html"><![CDATA[<p>This is a mindmap made from my notes for the paper “<a href="https://perso.univ-rennes1.fr/martin.costabel/publis/Co_PrinciplesBEM.pdf">Principles of boundary element methods</a>”, which is written by Martin Costabel.</p>

<p align="center"><img src="/figures/p75819011.jpg" alt="Mindmap for Principles of Boundary Element Methods" /></p>
<p align="center">Mindmap for Principles of Boundary Element Methods</p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="BEM" /><category term="思维导图" /><summary type="html"><![CDATA[This is a mindmap made from my notes for the paper “Principles of boundary element methods”, which is written by Martin Costabel.]]></summary></entry><entry><title type="html">Exercise 6 in Section 22 of James Munkres Topology</title><link href="https://jihuan-tian.github.io/math/2019/02/24/munkres-topology-s22e6.html" rel="alternate" type="text/html" title="Exercise 6 in Section 22 of James Munkres Topology" /><published>2019-02-24T00:00:00+08:00</published><updated>2019-02-24T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2019/02/24/munkres-topology-s22e6</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2019/02/24/munkres-topology-s22e6.html"><![CDATA[<p><strong>Exercise 22.6</strong> Recall that \(\mathbb{R}_{K}\) denotes the real line in the \(K\)-topology. Let \(Y\) be the quotient space obtained from \(\mathbb{R}_K\) by collapsing the set \(K\) to a point; let \(p: \mathbb{R}_K \rightarrow Y\) be the quotient map.</p>

<p>(a) Show that \(Y\) satisfies the \(T_1\) axiom, but is not Hausdorff.</p>

<p>(b) Show that \(p \times p: \mathbb{R}_K \times \mathbb{R}_K \rightarrow Y \times Y\) is not a quotient map.</p>

<p><strong>Comment</strong> This exercise shows that the product map of two quotient maps is not necessarily a quotient map.</p>

<p><strong>Proof</strong>: (a) At first, we will clarify the forms of open sets in the quotient space \(Y\), which are defined as the images of saturated open sets in \(\mathbb{R}_K\) under the quotient map \(p\). Assume the set \(K\) coalesces to \(\alpha\), \(Y\) can be written as: \(Y = (\mathbb{R} - K) \cup \{\alpha\}\). For any \(x\) in \(\mathbb{R} - K\), \(p^{-1}(x) = x\) and \(p^{-1}(\alpha) = K\).  Then the saturated open sets in \(\mathbb{R}_K\) have the following two forms:</p>

<ol>
  <li>open set \(U\) of \(\mathbb{R}_K\) which contains \(K\);</li>
  <li>\(U - K\) with \(U\) being arbitrary open set in \(\mathbb{R}_K\).</li>
</ol>

<p>Then their images under the quotient map \(p\) are</p>

<ol>
  <li>\((U - K) \cup \{\alpha\}\) with \(K \subsetneq U\)</li>
  <li>$U - K$</li>
</ol>

<p>which comprise the quotient topology on \(Y\). To prove \(Y\) satisfies the \(T_1\)-axiom, by referring to Theorem 17.8, we only need to show that one-point set \(\{x_0\}\) is closed. Then finite union of such closed singletons is also closed. To achieve this, there are two cases to be discussed.</p>

<ol>
  <li>
    <p>If \(x_0 = \alpha\), for any point \(x \in Y\) and \(x \neq x_0\), i.e. \(x \in \mathbb{R} - K\), there exists an open set \(U - K\) in \(Y\) containing \(x\), which does not contain \(x_0\). Therefore, for all \(x \in \mathbb{R} - K\), it does not belong to the closure of \(\{\alpha\}\). Hence \(\{\alpha\}\) is closed.</p>
  </li>
  <li>
    <p>If \(x_0 \in \mathbb{R} - K\), there are further two sub-cases:</p>

    <ul>
      <li>
        <p>For any \(x \in \mathbb{R} - K\) and \(x \neq x_0\), because \(\mathbb{R}_K\) is Hausdorff, there exists open sets \(U\) and \(V\) in \(\mathbb{R}_K\), such that \(x_0 \in U\), \(x \in V\) and \(U \cap V = \Phi\). Then \(x_0 \in (U - K)\), \(x \in (V - K)\) and \((U - K) \cap (V - K) = \Phi\), where both \(U - K\) and \(V - K\) are open in \(Y\). Hence \(\{x_0\} \cap (V - K) = \Phi\).</p>
      </li>
      <li>
        <p>For \(x = \alpha\), the open set containing \(x\) has the form \((U - K) \cup \{\alpha\}\) where \(U\) is an open set in \(\mathbb{R}_K\) containing \(K\). Then,</p>

        <ul>
          <li>when \(x_0 \in (-\infty, 0]\), let \(U = (0, 2)\);</li>
          <li>when \(x_0 \in (0, 1]\), let \(U = (0,x_0) \cup (x_0, \frac{3}{2})\);</li>
          <li>when \(x_0 \in (1, +\infty)\), let \(U = (0,x_0)\),</li>
        </ul>

        <p>such that \(K \subset U\) and \(\{x_0\} \cap ((U - K) \cup \{\alpha\}) = \Phi\).</p>
      </li>
    </ul>

    <p>Combining the above two sub-cases, we have for any \(x \neq x_0\) in \(Y\), it does not belong to the closure of \(\{x_0\}\). Hence \(\{x_0\}\) is closed.</p>
  </li>
</ol>

<p>Summarize the above cases, one-point set in \(Y\) is closed. Hence \(Y\) satisfies the \(T_1\)-axiom.</p>

<p>Next, we will show \(Y​\) is not Hausdorff.</p>

<p>Let \(x_1, x_2 \in Y\), \(x_1 = \alpha\) and \(x_2 = 0\). For any open set in \(Y\) containing 0 but not \(\alpha\), it must have the form \(V - K\) with \(V\) being open in \(\mathbb{R}_K\). Then there exists an open interval \((a_2, b_2)\) with \(a_2 &lt; 0\) and \(b_2 &gt; 0\) such that \(0 \in (a_2, b_2)\) and \((a_2, b_2) \subset V\). We can find an \(n_0 \in \mathbb{Z}_+\) such that \(\frac{1}{n_0} &lt; b_2\) and hence \(\frac{1}{n_0} \in (a_2, b_2)\). Meanwhile, any open set containing \(\alpha\) has the form \((U - K ) \cup \{\alpha\}\) with \(U\) being open in \(\mathbb{R}_K\) and \(K \subsetneq U\). Then there exists an open interval \((a_1,b_1)\) such that \(\frac{1}{n_0} \in (a_1, b_1)\) and \((a_1, b_1) \subset U\). Therefore, \((a_1,b_1) \cap (a_2,b_2) \neq \Phi\) and \(U \cap V \neq \Phi\), especially, \((U-K)\cap(V-K)\neq\Phi\). Hence, \(((U-K)\cup\{\alpha\}) \cap (V-K) \neq \Phi\). Therefore, for any open set containing 0, there is no open set containing \(\alpha\) which has no intersection with it. So \(Y\) is not Hausdorff.</p>

<p>(b) To prove this part, Exercise 13 in Section 17 should be adopted, which is presented below:</p>

<blockquote>
  <p>\(X\) is Hausdorff if and only if the diagonal \(\Delta = \{x \times x \vert x \in X \}\) is closed in \(X \times X\).</p>
</blockquote>

<ol>
  <li>If \(X\) is Hausdorff, for any \(x_1, x_2 \in X\) and \(x_1 \neq x_2\), there exist \(U\) and \(V\) open in \(X\) such that \(x_1 \in U\), \(x_2 \in V\) and \(U \cap V = \Phi\). Because \(U\) and \(V\) have no common points, \((U \times V) \cap \Delta = \Phi\). Then according to Theorem 17.5, \((x_1, x_2)\) does not belong to the closure of \(\Delta\). Because \(x_1\) and \(x_2\) are arbitrary two different points in \(X\), \(\Delta\) is closed.</li>
  <li>On the contrary, if \(\Delta\) is closed, for all \(x_1, x_2 \in X\) and \(x_1 \neq x_2\), there exists an open set \(W\) in \(X \times X\) containing \((x_1,x_2)\) such that \(W \cap \Delta = \Phi\). Then there exists a basis element \(U \times V\) in \(X \times X\) such that \((x_1, x_2) \subset U \times V \subset W\). Hence \(x_1 \in U\) and \(x_2 \in V\). Because \((U \times V) \cap \Delta = \Phi\), \(U \cap V = \Phi\). Because \(x_1\) and \(x_2\) are arbitrary two different points in \(X\), \(X\) is Hausdorff.</li>
</ol>

<p>With the proved S17E13 and the obtained conclusion in part (a) that \(Y\) is no Hausdorff, we know that the diagonal set \(\Delta\) is not closed in \(Y \times Y\). Meanwhile, because its preimage \((p \times p)^{-1}(\Delta) = \{x \times x \vert x \in \mathbb{R}\}\) is closed in \(\mathbb{R}_K \times \mathbb{R}_K\), the product map \(p \times p\) is not a quotient map.</p>

<p>Finally, the following figure illustrates the original space \(\mathbb{R}_K\) and the quotient space \(Y\). The transformation from \(\mathbb{R}_K\) to \(Y\) can be considered as merging a countable number of knots on a rope.</p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="topology" /><summary type="html"><![CDATA[Exercise 22.6 Recall that \(\mathbb{R}_{K}\) denotes the real line in the \(K\)-topology. Let \(Y\) be the quotient space obtained from \(\mathbb{R}_K\) by collapsing the set \(K\) to a point; let \(p: \mathbb{R}_K \rightarrow Y\) be the quotient map.]]></summary></entry><entry><title type="html">Metaphor of quotient space</title><link href="https://jihuan-tian.github.io/math/2019/02/17/metaphor-of-quotient-space.html" rel="alternate" type="text/html" title="Metaphor of quotient space" /><published>2019-02-17T00:00:00+08:00</published><updated>2019-02-17T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2019/02/17/metaphor-of-quotient-space</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2019/02/17/metaphor-of-quotient-space.html"><![CDATA[<p>In James Munkres “Topology” Section 22, the quotient space is defined as below.</p>

<p><strong>Definition</strong> Let \(X\) be a topological space, and let \(X^*\) be a partition of \(X\) into disjoint subsets whose union is \(X\). Let \(p: X \rightarrow X^*\) be the surjective map that carries each point of \(X\) to the element of \(X^*\) containing it. In the quotient topology induced by \(p\), the space \(X^*\) is called a quotient space of \(X​\).</p>

<p>The key factors in this definition are:</p>

<ol>
  <li>According to section 3, the quotient space \(X^*\) as a partition of \(X\) is associated with a unique equivalence relation on \(X\). This equivalence relation specifies which points in the original space \(X\) will be treated as a same point in the new space \(X^*\).</li>
  <li>The quotient map \(p: X \rightarrow X^*\) for constructing the quotient topology on \(X^*\) introduces the concept of saturated sets, which are pre-images of subsets in \(X^*\). \(p\) ensures the image of any saturated open/closed set in \(X\) is still open/closed in \(X^*\).</li>
</ol>

<p>With these concepts in mind, we can take paper folding and pasting as an example. Let the space \(X\) be a piece of paper. The equivalence classes on \(X\) determine which parts of this piece of paper will be pasted together. Meanwhile, the quotient map \(p\) collects the neighborhoods around every points in \(X\) that are to be pasted into a common point \(x_0\) in \(X^*\) and builds up a new neighborhood of \(x_0\) in \(X^*\). The neighborhoods of points in \(X\) are defined with respect to the subspace topology on \(X\), which is induced from the standard topology on \(\mathbb{R}^2\). The neighborhoods of points in \(X^*\) are defined with respect to the quotient topology on \(X^*\). That the quotient map \(p\) is surjective implies the whole paper is kept during the operations without cutting off any part. Hence, the obtained quotient space \(X^*\) is just the piece of paper after these folding and pasting operations. The following figure illustrates the above metaphor of quotient space by folding a piece of rectangular paper into a cylinder.</p>

<p><img src="/figures/2019-02-17 Metaphor of quotient space.png" alt="" /></p>

<p>Figure. Illustration of quotient space using the example of paper folding and pasting.</p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="topology" /><summary type="html"><![CDATA[In James Munkres “Topology” Section 22, the quotient space is defined as below.]]></summary></entry><entry><title type="html">Example 1 in Section 22 of James Munkres Topology</title><link href="https://jihuan-tian.github.io/math/2019/02/09/munkres-topology-s22b1.html" rel="alternate" type="text/html" title="Example 1 in Section 22 of James Munkres Topology" /><published>2019-02-09T00:00:00+08:00</published><updated>2019-02-09T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2019/02/09/munkres-topology-s22b1</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2019/02/09/munkres-topology-s22b1.html"><![CDATA[<p><strong>Example 1</strong> Let \(X\) be the subspace \([0,1]\cup[2,3]\) of \(\mathbb{R}\), and let \(Y\) be the subspace \([0,2]\) of \(\mathbb{R}\). The map \(p: X \rightarrow Y\) defined by</p>

\[p(x)=\begin{cases}
x &amp; \text{for}\; x \in [0,1],\\
x-1 &amp; \text{for}\; x \in [2,3]
\end{cases}\]

<p>is a closed map thus a quotient map, but not open.</p>

<p><strong>Proof</strong> (a) \(p\) is surjective is obvious.</p>

<p>(b) Prove \(p\) is continuous.</p>

<p>\(p\) is a piecewise function comprised of two parts \(p_1 = x\) with \(x \in [0,1])\) and \(p_2=x-1\) with \(x\in[2,3]\). We extend the domains and ranges of \(p_1\) and \(p_2\) to \(\mathbb{R}\) and obtain two continuous functions \(\tilde{p}_1\) and \(\tilde{p}_2\). According to Theorem 18.2 (d) and (e), as the restrictions of \(\tilde{p}_1\) and \(\tilde{p}_2\), \(p_1\) and \(p_2\) are continuous. Because \(X\) comprises two disjoint parts \([0,1]\) and \([2,3]\), both of them are both open and closed in \(X\). By treating them as open sets, according to Theorem 18.2 (f) the local formulation of continuity, \(p\) is continuous. Or if we treat \([0,1]\) and \([2,3]\) as closed sets, according to Theorem 18.3 the pasting lemma, \(p\) is also continuous.</p>

<p><strong>Comment</strong> To prove the continuity of a piecewise function, it is very cumbersome if we start the proof from the raw definition of continuity, which will involve lots of cases for discussion. The appropriate way is to use Theorem 18.2 and Theorem 18.3, especially extensions and restriction of function’s domain and range.</p>

<p>(c) Prove \(p\) is a closed map, thus a quotient map.</p>

<p>It is obvious to see that \(\tilde{p}_1\) is an identity map and \(\tilde{p}_2\) is a merely a translation. Both of them are closed maps. For a closed set \(C\) in \(X\), there exists a closed set \(C'\) in \(\mathbb{R}\) such that \(C = C'\cap X\). The image of \(C\) under \(p\) is</p>

\[\begin{aligned}
p(C) &amp;= p(C'\cap X) = p(C' \cap ([0,1] \cup [2,3])) \\
&amp;= p\left( (C'\cap[0,1]) \cup (C'\cap[2,3]) \right) \\
&amp;= p(C'\cap[0,1]) \cup p(C'\cap[2,3])
\end{aligned}.\]

<p>According to Theorem 17.2, both \(C'\cap[0,1]\) and \(C'\cap[2,3]\) are closed in \(\mathbb{R}\). Meanwhile, we have \(p(C'\cap[0,1])=\tilde{p}_1(C'\cap[0,1])\) and \(p(C'\cap[2,3])=\tilde{p}_2(C'\cap[2,3])\), both of which are closed in \(\mathbb{R}\) because \(\tilde{p}_1\) and \(\tilde{p}_2\) are closed maps. Because \(Y\) is closed in \(\mathbb{R}\), by applying Theorem 17.2 again,  \(p(C'\cap[0,1])\) and \(p(C'\cap[2,3])\) are closed in \(Y\), so is their union \(p(C)\). Hence, \(p\) is a closed map.</p>

<p>(d) Prove \(p\) is not an open map.</p>

<p>\([0,1]\) is open in \(X\) but \(p([0,1])=[0,1]\), which is closed in \(Y\). Therefore, \(p\) is not an open map.</p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="topology" /><summary type="html"><![CDATA[Example 1 Let \(X\) be the subspace \([0,1]\cup[2,3]\) of \(\mathbb{R}\), and let \(Y\) be the subspace \([0,2]\) of \(\mathbb{R}\). The map \(p: X \rightarrow Y\) defined by]]></summary></entry><entry><title type="html">Exercise 3 in Section 22 of James Munkres Topology</title><link href="https://jihuan-tian.github.io/math/2019/02/09/munkres-topology-s22e3.html" rel="alternate" type="text/html" title="Exercise 3 in Section 22 of James Munkres Topology" /><published>2019-02-09T00:00:00+08:00</published><updated>2019-02-09T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2019/02/09/munkres-topology-s22e3</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2019/02/09/munkres-topology-s22e3.html"><![CDATA[<p><strong>Exercise 22.3</strong> Let \(\pi_1: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}\) be projection on the first coordinate. Let \(A\) be the subspace of \(\mathbb{R}\times\mathbb{R}\) consisting of all points \(x \times y\) for which either \(x \geq 0\) or \(y = 0\) (or both); let \(q: A \rightarrow \mathbb{R}\) be obtained by restricting \(\pi_1\). Show that \(q\) is a quotient map that is neither open nor closed.</p>

<p><strong>Proof</strong> (a) Show \(q\) is a quotient map.</p>

<p>The projection map \(\pi_1\) is continuous because the pre-image of any open set \(U\) in \(\mathbb{R}\) under \(\pi_1\) is \(U \times \mathbb{R}\), which is open in the product space \(\mathbb{R}\times\mathbb{R}\). Then its restriction \(q\) is also continuous due to Theorem 18.2.</p>

<p>According to the illustrated domain of \(q\) in Figure 1 which is marked in light grey, it is obvious that \(q\) is surjective. It also shows the three types of saturated open sets in \(A\) with respect to \(q\), which are marked in red:</p>

<ul>
  <li>\((a,b) \times \{0\}\) with \(a &lt; 0\) and \(b \leq 0\) and its image under \(q\) is \((a, b)\).</li>
  <li>\((a,b) \times \mathbb{R}\) with \(a \geq 0\) and \(b &gt; 0\) and its image under \(q\) is \((a, b)\).</li>
  <li>\((a, 0) \times \{0\} \cup [0,b) \times \mathbb{R}\) with \(a &lt; 0\) and \(b &gt; 0\). Because a map preserves set union operation, its image under \(q\) is \((a, b)\).</li>
</ul>

<p>It can be seen that for the three types of saturated open sets, their images are all open in \(\mathbb{R}​\). Meanwhile, arbitrary union of the above three types saturated open sets is also a saturated open set with its image open in \(\mathbb{R}​\). Therefore, \(q​\) is a quotient map.</p>

<p><img src="./media/2019-02-09 S22E3.jpg" alt="Figure 1. Illustration of the domain of $$q$$ and saturated open sets in $$A$$." /></p>

<p>Figure 1. Illustration of the domain of \(q\) and saturated open sets in \(A\).</p>

<p>(b) Show \(q\) is neither an open nor a closed map.</p>

<p>Let \(U = [0, 1) \times (1, 2)\) be an open set of \(A\) in the subspace topology, which is not saturated. \(q(U) = [0, 1)\) is not open in \(\mathbb{R}\). Hence \(q\) is not an open map.</p>

<p>Let \(U = \{(x,y) \vert xy = 1 \;\text{and}\; x &gt; 0 \}\) which is closed in \(\mathbb{R} \times \mathbb{R}\). According to Theorem 17.2, \(U\) is also closed in the subspace \(A\). Then \(q(U)=(0,+\infty)\), which is not closed in \(\mathbb{R}\). Hence \(q\) is not a closed map.</p>

<p><strong>Comment</strong> This exercise shows that a function being open or closed map is a sufficient but not a necessary condition for the function to be a quotient map.</p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="topology" /><summary type="html"><![CDATA[Exercise 22.3 Let \(\pi_1: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}\) be projection on the first coordinate. Let \(A\) be the subspace of \(\mathbb{R}\times\mathbb{R}\) consisting of all points \(x \times y\) for which either \(x \geq 0\) or \(y = 0\) (or both); let \(q: A \rightarrow \mathbb{R}\) be obtained by restricting \(\pi_1\). Show that \(q\) is a quotient map that is neither open nor closed.]]></summary></entry><entry><title type="html">Continuity of arithmetic operations</title><link href="https://jihuan-tian.github.io/math/2019/02/03/continuity-of-arithmetic-operations.html" rel="alternate" type="text/html" title="Continuity of arithmetic operations" /><published>2019-02-03T00:00:00+08:00</published><updated>2019-02-03T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2019/02/03/continuity-of-arithmetic-operations</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2019/02/03/continuity-of-arithmetic-operations.html"><![CDATA[<p>Arithmetic operations taught in elementary schools are continuous in the high level topological point of view. This signifies that there is literally no clear boundary between simple and complex, low and high concepts. Instead, they both play indispensable roles in mathematics with their conflation forming a unified logical system. In this post, a proof will be provided for the continuity of arithmetic operations, which are depicted in Exercise 12 of Section 21 and Theorem 21.5 in James Munkres “Topology”.</p>

<h1 id="arithmetic-operations-on-real-numbers-are-continuous">Arithmetic operations on real numbers are continuous</h1>

<p><strong>Exercise 21.12</strong> Prove continuity of the algebraic operations on \(\mathbb{R}\), as follows: Use the metric \(d(a, b) = \abs{a - b}\) on \(\mathbb{R}\) and the metric on \(\mathbb{R}^2\) given by the equation</p>

\[\rho((x,y),(x_0,y_0))=\max\{\abs{x-x_0},\abs{y-y_0}\}.\]

<p><strong>Analysis</strong> The proof relies on Theorem 21.1. For a function \(f: X \rightarrow Y\), by fixing an arbitrary \(x_0\) in \(X\) and confining the variation of the independent variable \(x\) around this \(x_0\) to a specified range \(\delta\), the variation \(\varepsilon\) of the function value \(f(x)\) around \(f(x_0)\) can be arbitrarily small. By the way, it can be extended that an upper bound is set to \(\varepsilon\) and Theorem 21.1 still holds.</p>

<p><strong>Proof</strong> (a) Show the addition operation is continuous.</p>

<p>Fix \((x_0, y_0)\) in \(\mathbb{R}\times\mathbb{R}\) and select \((x,y)\) in a range which ensures that for all \(\varepsilon&gt;0\), \(d(x+y, x_0+y_0) &lt; \varepsilon\). Then</p>

\[\begin{aligned}
d(x+y,x_0+y_0)&amp;=\abs{(x+y)-(x_0+y_0)}\\
&amp;\leq \abs{x-x_0}+\abs{y-y_0}\\
&amp;\leq 2\rho((x,y),(x_0,y_0))
\end{aligned}.\]

<p>By enforcing \(2\rho((x,y),(x_0,y_0)) &lt; \varepsilon\), we have \(\rho((x,y),(x_0,y_0)) &lt; \frac{\varepsilon}{2}\). Therefore, let \(\delta=\frac{\varepsilon}{2}\), when \(\rho((x,y),(x_0,y_0)) &lt; \delta\), \(d(x+y,x_0+y_0) &lt; \varepsilon\). Hence, the addition operation is continuous.</p>

<p>(b) Show the multiplication operation is continuous.</p>

<p>Fix \((x_0,y_0)\) in \(\mathbb{R}\times\mathbb{R}\), we have</p>

\[\begin{aligned}
d(xy, x_0y_0) &amp;= \abs{xy - x_0y_0} = \abs{xy - x_0y + x_0y - x_0y_0}\\
&amp;=\abs{(x-x_0)y + x_0(y-y_0)} \\
&amp;=\abs{(x-x_0)y - (x-x_0)y_0 + (x-x_0)y_0 + x_0(y-y_0)} \\
&amp;=\abs{(x-x_0)(y-y_0) + (x-x_0)y_0 + x_0(y-y_0)}\\
&amp;\leq \abs{x-x_0}\cdot\abs{y-y_0} + \abs{x-x_0}\cdot\abs{y_0} + \abs{x_0}\cdot\abs{y-y_0} \\
&amp;\leq \rho((x,y),(x_0,y_0))^2 + \rho((x,y),(x_0,y_0))\abs{x_0} + \rho((x,y),(x_0,y_0))\abs{y_0}
\end{aligned}.\]

<p>Then, for all \(0 &lt; \varepsilon \leq 1\), enforce the above inequality less than \(\varepsilon\):</p>

\[\begin{aligned}
d(xy, x_0y_0) &amp;\leq \rho((x,y),(x_0,y_0))^2 + \rho((x,y),(x_0,y_0))\abs{x_0} + \rho((x,y),(x_0,y_0))\abs{y_0} \\
&amp; &lt; \varepsilon \leq 1
\end{aligned}.\]

<p>Because \(\rho((x,y),(x_0,y_0))^2&lt;1\), \(\rho((x,y),(x_0,y_0))^2 \leq \rho((x,y),(x_0,y_0))\). Then we adopt a stronger enforcement by letting</p>

\[\begin{aligned}
d(xy, x_0y_0) &amp;\leq \rho((x,y),(x_0,y_0))^2 + \rho((x,y),(x_0,y_0))\abs{x_0} + \rho((x,y),(x_0,y_0))\abs{y_0} \\
&amp; \leq \rho((x,y),(x_0,y_0)) + \rho((x,y),(x_0,y_0))\abs{x_0} + \rho((x,y),(x_0,y_0))\abs{y_0} \\
&amp; = \rho((x,y),(x_0,y_0)) (1 + \abs{x_0} + \abs{y_0}) \\
&amp; &lt; \varepsilon
\end{aligned}.\]

<p>This leads to</p>

\[\rho((x,y),(x_0,y_0)) &lt; \frac{\varepsilon}{1 + \abs{x_0} + \abs{y_0}}.\]

<p>Because \((x_0,y_0)\) is given as a fixed point, the right hand side of the above inequality is a definite value. By letting \(\delta = \frac{\varepsilon}{1 + \abs{x_0} + \abs{y_0}}\), when \(\rho((x,y),(x_0,y_0))&lt;\delta\), we have \(d(xy,x_0y_0)&lt;\varepsilon\) and the multiplication operation is continuous.</p>

<p>(c) Show the subtraction operation is continuous.</p>

<p>First, let \(f: \mathbb{R} \rightarrow \mathbb{R}\) with \(f(x)=-x\) be the negation operation. For any open interval \((a,b)\) in \(\mathbb{R}\), \(f^{-1}((a,b)) = (-b,-a)\), which is also open. Hence \(f\) is continuous.</p>

<p>Then we prove Exercise 10 in Section 18, which will be used afterwards.</p>

<p><strong>Exercise 18.10</strong> Let \(f: A \rightarrow B\) and \(g: C \rightarrow D\) be continuous functions. Let us define a map \(f \times g: A \times C \rightarrow B \times D\) by the equation</p>

\[(f \times g)(a \times c) = f(a) \times g(c).\]

<p>Show that \(f \times g\) is continuous.</p>

<p>Let \(U\) be open in \(B\) and \(V\) be open in \(D\). Then \(U \times V\) is a topological basis of the produce space \(B \times D\). According to the definition of the product map \(f \times g\),</p>

\[(f \times g)^{-1}(U \times V) = f^{-1}(U) \times g^{-1}(V).\]

<p>Because both \(f\) and \(g\) are continuous, \(f^{-1}(U)\) is open in \(A\) and \(g^{-1}(V)\) is open in \(C\). Therefore, \(f^{-1}(U) \times g^{-1}(V)\) is a basis of \(A \times C\). Hence \(f \times g\) is continuous.</p>

<p>With Exercise 18.10 proved, \(F: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R} \times \mathbb{R}\) with \(F(x,y)=(x,-y)\) is a continuous function, because its first coordinate map is the continuous identity map and its second coordinate map is the continuous negation operation. Then, the subtraction operation can be treated as a composition of \(F\) and the continuous addition operation. According to Theorem 18.2 (c), the subtraction operation is continuous.</p>

<p>(d) Show the reciprocal operation \(f: \mathbb{R} - \{0\} \rightarrow \mathbb{R}\) with \(f(x) = \frac{1}{x}\) is continuous.</p>

<p>N.B. The domain of \(f\) is a subspace of \(\mathbb{R}\) having the subspace topology.</p>

<p>Let \((a,b)\) be an arbitrary open interval in \(\mathbb{R}\) and we consider the following five cases.</p>

<ul>
  <li>For \(a&gt;0\) and \(b&gt;0\): \(f^{-1}((a,b)) = (\frac{1}{b}, \frac{1}{a})\).</li>
  <li>For \(a&lt;0\) and \(b&lt;0\): \(f^{-1}((a,b)) = (\frac{1}{b}, \frac{1}{a})\).</li>
  <li>For \(a=0\) and \(b&gt;0\): \(f^{-1}((a,b)) = (\frac{1}{b}, \infty)\).</li>
  <li>For \(a&lt;0\) and \(b=0\): \(f^{-1}((a,b)) = (-\infty, \frac{1}{a})\).</li>
  <li>
    <p>For \(a&lt;0​\) and \(b&gt;0​\): \(f^{-1}((a,b)) = f^{-1}((a,0) \cup (0,b))​\). Because the inverse map preserves set operations,</p>

\[f^{-1}((a,0) \cup (0,b)) = f^{-1}((a,0)) \cup f^{-1}((0,b)) = (-\infty, \frac{1}{a}) \cup (\frac{1}{b}, \infty).\]
  </li>
</ul>

<p>\(f^{-1}((a,b))\) is open in \(\mathbb{R} - \{0\}\) under the above five cases, so \(f\) is continuous.</p>

<p>(e) Show the quotient operation is continuous.</p>

<p>Define a function \(G: \mathbb{R} \times \mathbb{R} - \{0\} \rightarrow \mathbb{R} \times \mathbb{R} - \{0\}\) with \(G(x,y) = (x,\frac{1}{y})\). \(G\) is a continuous function according to Exercise 18.10 and part (d). Furthermore, the multiplication operation with its domain restricted to \(\mathbb{R} - \{0\}\) is also continuous due to Theorem 18.2 (d). Then the quotient operation as a composition of \(G\) and the domain-restricted multiplication operation is continuous.</p>

<h1 id="arithmetic-operations-on-the-space-of-continuous-functions-are-continuous">Arithmetic operations on the space of continuous functions are continuous</h1>

<p><strong>Theorem 21.5</strong> If \(X\) is a topological space, and if \(f, g: X \rightarrow \mathbb{R}\) are continuous functions, then \(f+g\), \(f-g\) and \(f \cdot g\) are continuous. If \(g(x) \neq 0\) for all \(x\), then \(f/g\) is continuous.</p>

<p><strong>Comment</strong></p>

<ol>
  <li>This theorem is a high level version of Exercise 21.12. It states the arithmetic operations on real-valued functions instead of on real numbers.</li>
  <li>As already introduced in <a href="https://www.cnblogs.com/peabody/p/10145036.html">this post</a>, arithmetic operations on continuous functions is one of the ways to construct new continuous functions.</li>
</ol>

<p><strong>Proof</strong> Take the addition operation \(f+g\) as example. It is defined as \((f+g)(x)=f(x)+g(x)\), which can be considered as a composition of two functions \(h_1: \mathbb{R} \rightarrow f(\mathbb{R}) \times g(\mathbb{R})\) with \(h_1(x)=(f(x),g(x))\) and \(h_2: f(\mathbb{R}) \times g(\mathbb{R}) \rightarrow \mathbb{R}\) with \(h_2(x,y)=x+y\). For each coordinate map of \(h_1\), it is the continuous identity map. According to Theorem 18.4 (Maps into products), \(h_1\) is continuous. \(h_2\) is the addition operation with a restricted domain, which is also continuous. Therefore \(f+g=h_2 \circ h_1\) is continuous.</p>

<p>Similarly, we can prove \(f-g\), \(f \cdot g\) and \(f/g\) with \(g(x) \neq 0\) for all \(x\) are continuous.</p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="topology" /><summary type="html"><![CDATA[Arithmetic operations taught in elementary schools are continuous in the high level topological point of view. This signifies that there is literally no clear boundary between simple and complex, low and high concepts. Instead, they both play indispensable roles in mathematics with their conflation forming a unified logical system. In this post, a proof will be provided for the continuity of arithmetic operations, which are depicted in Exercise 12 of Section 21 and Theorem 21.5 in James Munkres “Topology”.]]></summary></entry><entry><title type="html">The sequence lemma in James Munkres Topology</title><link href="https://jihuan-tian.github.io/math/2019/01/19/munkres-topology-lemma-21-2-the-sequence-lemma.html" rel="alternate" type="text/html" title="The sequence lemma in James Munkres Topology" /><published>2019-01-19T00:00:00+08:00</published><updated>2019-01-19T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2019/01/19/munkres-topology-lemma-21-2-the-sequence-lemma</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2019/01/19/munkres-topology-lemma-21-2-the-sequence-lemma.html"><![CDATA[<p><strong>Lemma 21.2 (The sequence lemma)</strong> Let \(X\) be a topological space; let \(A \subset X\). If there is a sequence of points of \(A\) converging to \(x\), then \(x \in \bar{A}\); the converse holds if \(X\) is metrizable.</p>

<p><strong>Proof</strong> a) Sequence convergence \(\Longrightarrow\) the limit point belongs to \(\bar{A}\).</p>

<p>Let \(\{x_n\}_{n \in \mathbb{Z_+}}\) be a sequence of points in \(A\). When \(n \rightarrow \infty\), it converges to \(x\) topologically in \(X\). Then for all open set \(U\) containing \(x\), there exists an \(N \in \mathbb{Z_+}\), such that when \(n &gt; N\), \(x_n \in U\). Hence \(U \cap A \neq \Phi\). According to Theorem 17.5 (a), \(x \in \bar{A}\).</p>

<p>b) \(x\) belongs to \(\bar{A}\) and \(X\) is metrizable \(\Longrightarrow\) Sequence convergence to \(x\).</p>

<p>Still according to Theorem 17.5 (a), when \(x \in \bar{A}\), for all open set \(U\) containing \(x\), \(U \cap A \neq \Phi\). However, this only ensures that the intersection is nonempty but is not enough to promise that there exists an \(N\) in \(\mathbb{Z}_+\), such that for all \(n &gt; N\), \(x_n\) belongs to \(U\). Hence, the desired convergence sequence in \(A​\) does not necessarily exist.</p>

<p>If \(X​\) is assigned a metric, a collection of nested open balls \(\left\{B_n(x, \frac{1}{n})\right\}_{n \in \mathbb{Z}_+}​\) centered at \(x \in \bar{A}​\) can be constructed. For all \(n \in \mathbb{Z}_+​\), \(B_{n}(x, \frac{1}{n}) \cap A \neq \Phi​\) and an element \(x_n​\) can be selected from this intersection. Thus a sequence \(\{x_n\}_{n \in \mathbb{Z}_+}​\) convergent to \(x​\) is obtained.</p>

<p><strong>Remark</strong></p>

<ol>
  <li>
    <p>As shown in b) above, a metric assigned to the space \(X\) which generates the same topology as that used for defining sequence convergence is mandatory to ensure the existence of a convergent sequence to \(x\).  This contradicts our common conception about the equivalence between the closeness of a set \(A\) and the existence of a convergent sequence with its limit point within \(A\). This is because the spaces we are dealing with in everyday life, such as Banach spaces, Hilbert spaces, have sound properties which have already included a well defined metric. However, when we come to the study of topology, such nice property is stripped away for the purpose of establishing a more abstract and general theory underpinning those high level and realistic topics.</p>
  </li>
  <li>
    <p>It is natural for us to ask that if an example can be given, where the space \(X\) has no associated metric and there is no sequence \(\{x_n\}_{n \in \mathbb{Z}_+}\) in \(A\) convergent to a point \(x \in \bar{A}\).</p>

    <p>Let’s consider a set \(X = S_{\Omega} \cup \{\Omega\}\) with \(S_{\Omega}\) being the minimal uncountable well-ordered set as defined in Lemma 10.2. Let \(X\) be assigned the order topology and let \(A = S_{\Omega}\) be a subset of \(X\). Because \(S_{\Omega}\) is the largest element in \(X\), any open set \(U\) in \(X\) containing \(\Omega\) must have the form \((x ,\Omega]\) with \(x \in S_{\Omega}\). Then it is obvious that \(U \cap S_{\Omega} \neq \Phi\) and thus \(\Omega\) belongs to \(\bar{S}_{\Omega}\). More accurately speaking, \(\Omega\) is a limit point of \(S_{\Omega}\).</p>

    <p>Next, we show that there is no sequence \(\{x_n\}_{n \in \mathbb{Z}_+}\) in \(S_{\Omega}\) convergent to \(\Omega\).</p>

    <p>Assume such sequence really exists. Because it is a countable set, according to Theorem 10.3, it has an upper bound \(x^*\) in \(S_{\Omega}\). We know from Lemma 10.2 that \(S_{\Omega}\) is uncountable and the section \(S_{x^*}\) is countable, therefore the set \(V = \{x \vert x \in S_{\Omega} \;\text{and}\; x &gt; x^*  \}\) is not empty. Then the open set \((x^*, \Omega]\) in \(X\) containing \(\Omega\) has an empty intersection with the sequence \(\{x_n\}_{n \in \mathbb{Z}_+}\) in \(S_{\Omega}\). Therefore, \(\{x_n\}_{n \in \mathbb{Z}_+}\) is not convergent to \(\Omega\).</p>
  </li>
  <li>
    <p>The contrapositive of part b) in this lemma can be used to prove that a space with a certain topology is not metrizable, i.e. by showing that there exists an <em>abnormal</em> point \(x​\)  in the closure of \(A​\), to which there is no convergent sequence in \(A​\), we can prove that there is no metric for the space \(X​\) which can induce the same topology as that used for defining the sequence convergence.</p>
  </li>
  <li>
    <p>For all \(A \subset X\) and for all \(x \in \bar{A}\), if there is always a sequence in \(A\) convergent to \(x\), we still cannot assert that \(X\) is metrizable.</p>

    <p>This can be verified by giving a counter example. Let \(X = \mathbb{R}\) be given the finite complement topology, i.e. the space satisfies the \(T_1\) axiom. Then for any \(A \subset X\), if \(A\) is a finite set, \(A\) itself is closed. For all \(x \in \bar{A} = A\), \(\{x_n = x\}_{n \in \mathbb{Z}_+}\) is a sequence in \(A\) convergent to \(x\).</p>

    <p>If \(A\) is an infinite set, \(\bar{A} = \mathbb{R}\). This is because for all \(x \in \mathbb{R}\) and for all open set \(U\) in \(\mathbb{R}\) containing \(x\), its complement \(U^{\rm c}\) is closed and is thus finite. Assume \(U \cap A = \Phi\), then \(A \subset U^{\rm c}\). However, because \(A\) is infinite, it cannot be contained within the finite set \(U^{\rm c}\). Therefore, \(U \cap A \neq \Phi\) and it proves that for all \(x \in \mathbb{R}\) it belongs to the closure of \(A\). Hence \(\bar{A} = \mathbb{R}\).</p>

    <p>Let \(\{x_n\}_{n \in \mathbb{Z}_+}​\) be a sequence in \(A​\) which has an infinite number of different elements. This is feasible because \(A​\) itself is an infinite set. For all \(x \in \mathbb{R}​\) and for all open set \(U​\) in \(\mathbb{R}​\) containing \(x​\), its complement \(U^{\rm c}​\) is a closed finite set. Then we consider following two complete cases.</p>

    <ul>
      <li>If for all \(y \in U^{\rm c}\), \(y \notin \{x_n\}_{n \in \mathbb{Z}_+}\), we have \(\{x_n\}_{n \in \mathbb{Z}_+} \subset U\), i.e. in the language of convergence, for all \(n &gt;1\), \(x_n \in U​\).</li>
      <li>If there exists a finite subset \(V\) of \(U^{\rm c}\) such that \(V \subset \{x_n\}_{n \in \mathbb{Z}_+}\) and let \(N\) be the maximum index in the sequence for those elements in \(V\), then for all \(n &gt; N\), \(x_n \in U\).</li>
    </ul>

    <p>Therefore, the sequence \(\{x_n\}_{n \in \mathbb{Z}_+}\) converges to \(x\) in any of the above two cases.</p>

    <p>This conclusion can be restated as below.</p>

    <blockquote>
      <p>Let \(\mathbb{R}\) be assigned the finite complement topology. Any sequence \(\{x_n\}_{n \in \mathbb{Z}_+} \subset \mathbb{R}\) having an infinite number of different elements can converge to any point \(x\) in \(\mathbb{R}\).</p>
    </blockquote>

    <p>Next, we need a small lemma to be proved:</p>

    <blockquote>
      <p>Every topological space \(X​\) with a metric \(d​\) satisfies the Hausdorff axiom.</p>
    </blockquote>

    <p><strong>Proof</strong> For all \(x, y \in X\), let their distance be \(d(x, y) = \epsilon\). Select an open ball \(B_d(x, \frac{\epsilon}{2})\) and for all \(z \in B_d(x, \frac{\epsilon}{2})\), we have \(d(x, y) \leq d(x, z) + d(z, y)\) and thus \(d(z, y) \geq d(x, y) - d(x, z)\). Because \(d(x, z) &lt; \frac{\epsilon}{2}\), \(d(z, y) &gt; \epsilon - \frac{\epsilon}{2} = \frac{\epsilon}{2}\). Hence \(z \notin B_d(y, \frac{\epsilon}{2})\). Similarly, for all \(z \in B_d(y, \frac{\epsilon}{2})\), \(z \notin B_d(x, \frac{\epsilon}{2})\). Therefore, \(X\) satisfies the Hausdorff axiom.</p>

    <p>Up to now, the conditions in the proposition of this part of the remark have been met. Because \(\mathbb{R}\) with the finite complement topology only satisfies the \(T_1\) axiom, which is a weaker condition than the Hausdorff axiom, according to the contrapositive of the above lemma, \(\mathbb{R}\) is not metrizable.</p>
  </li>
</ol>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="topology" /><summary type="html"><![CDATA[Lemma 21.2 (The sequence lemma) Let \(X\) be a topological space; let \(A \subset X\). If there is a sequence of points of \(A\) converging to \(x\), then \(x \in \bar{A}\); the converse holds if \(X\) is metrizable.]]></summary></entry><entry><title type="html">Matrix norm</title><link href="https://jihuan-tian.github.io/math/2019/01/10/matrix-norm.html" rel="alternate" type="text/html" title="Matrix norm" /><published>2019-01-10T00:00:00+08:00</published><updated>2019-01-10T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2019/01/10/matrix-norm</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2019/01/10/matrix-norm.html"><![CDATA[<p>In our <a href="/math/2019/01/08/munkres-topology-theo20-3-and-metric-equivalence.html">previous post</a>, we introduced various definitions of matrix norms in \(\mathbb{R}^{n \times n}\) based on the corresponding vector norms in \(\mathbb{R}^n\). Meanwhile, the equivalence of different vector norms and their induced metrics and topologies in \(\mathbb{R}^n\) is also inherited into \(\mathbb{R}^{n \times n}\). In this article, we’ll show why the above defined matrix norms are valid.</p>

<p>Generally, the definition of a matrix norm in \(\mathbb{R}^{n \times n}\) should satisfy the following four conditions:</p>

<ol>
  <li>Positive definiteness: for all \(A \in \mathbb{R}^{n \times n}\), \(\norm{A} \geq 0\). \(\norm{A} = 0\) if and only if \(A = 0\).</li>
  <li>Absolute homogeneity: for all \(\alpha \in \mathbb{R}\) and \(A \in \mathbb{R}^{n \times n}\), \(\norm{\alpha A} = \abs{\alpha} \norm{A}\).</li>
  <li>Triangle inequality: for all \(A, B \in \mathbb{R}^{n \times n}\), \(\norm{A + B} \leq \norm{A} + \norm{B}\).</li>
  <li>Sub-multiplicity: for all \(A, B \in \mathbb{R}^{n \times n}\), \(\norm{AB} \leq \norm{A} \norm{B}\).</li>
</ol>

<p>Therefore, we need to prove the following theorem in order to meet the above requirements.</p>

<p><strong>Theorem</strong> Let \(\norm{\cdot}\) be a norm on \(\mathbb{R}^n\). Then for all \(A \in \mathbb{R}^{n \times n}\), its matrix norm \(\zeta: \mathbb{R}^{n \times n} \rightarrow \mathbb{R}\) can be defined as</p>

\[\zeta(A) = \sup_{\forall \vect{x} \in \mathbb{R}^n, \vect{x} \neq 0} \frac{\norm{A \vect{x}}}{\norm{\vect{x}}} = \sup_{\forall \vect{x} \in \mathbb{R}^n, \norm{\vect{x}}=1} \norm{A \vect{x}}\]

<p><strong>Proof</strong> a) Positive definiteness and absolute homogeneity directly inherit from vector norms.</p>

<p>b) The triangle inequality can be proved as following.</p>

\[\begin{aligned}
\zeta(A + B) &amp;= \sup_{\forall \vect{x} \in \mathbb{R}^n, \vect{x} \neq 0} \frac{\norm{(A + B) \vect{x}}}{\norm{\vect{x}}} = \sup_{\forall \vect{x} \in \mathbb{R}^n, \vect{x} \neq 0} \frac{\norm{A\vect{x} + B\vect{x}}}{\norm{\vect{x}}} \\
&amp; \leq \sup_{\forall \vect{x} \in \mathbb{R}^n, \vect{x} \neq 0} \frac{\norm{A\vect{x}} + \norm{B\vect{x}}}{\norm{\vect{x}}} \leq \sup_{\forall \vect{x} \in \mathbb{R}^n, \vect{x} \neq 0} \frac{\norm{A\vect{x}}}{\norm{\vect{x}}} + \sup_{\forall \vect{x} \in \mathbb{R}^n, \vect{x} \neq 0} \frac{\norm{B\vect{x}}}{\norm{\vect{x}}} = \zeta(A) + \zeta(B).
\end{aligned}\]

<p>c) For sub-multiplicity, we have</p>

\[\begin{aligned}
\zeta(AB) &amp;= \sup_{\forall \vect{x} \in \mathbb{R}^n, \vect{x} \neq 0} \frac{\norm{AB\vect{x}}}{\norm{\vect{x}}} = \sup_{\forall \vect{x} \in \mathbb{R}^n, \vect{x} \neq 0} \frac{\norm{AB\vect{x}} \norm{B\vect{x}}}{\norm{B\vect{x}}\norm{\vect{x}}} \\
&amp;\leq \sup_{\forall \vect{x} \in \mathbb{R}^n, \vect{x} \neq 0} \frac{\norm{A\vect{x}}}{\norm{\vect{x}}} \cdot \sup_{\forall \vect{x} \in \mathbb{R}^n, \vect{x} \neq 0} \frac{\norm{B\vect{x}}}{\norm{\vect{x}}} = \norm{A} \cdot \norm{B}.
\end{aligned}\]

<p>d) Prove \(\zeta(A) = \sup_{\forall \vect{x} \in \mathbb{R}^n, \norm{\vect{x}} = 1} \norm{A\vect{x}}\).</p>

<p>Note that \(\frac{1}{\norm{\vect{x}}}\) is a scalar value in \(\mathbb{R}\), then with the proved absolute homogeneity, we have</p>

\[\zeta(A) = \sup_{\forall \vect{x} \in \mathbb{R}^n, \vect{x} \neq 0} \frac{\norm{A\vect{x}}}{\norm{\vect{x}}} = \sup_{\forall \vect{x} \in \mathbb{R}^n, \vect{x} \neq 0} \left\Vert A \cdot \frac{\vect{x}}{\norm{\vect{x}}} \right\Vert.\]

<p>By letting \(\vect{x}' = \frac{\vect{x}}{\norm{\vect{x}}}\), we have this part proved.</p>

<p>Summarizing a) to d), \(\norm{\cdot}\) is literally a matrix norm induced from the corresponding vector norm.</p>

<p>Next, we prove the validity of the detailed formulations of the matrix norms, i.e.</p>

<ol>
  <li>1-norm: \(\norm{A}_1 = \max_{1 \leq j \leq n} \sum_{i=1}^n \abs{a_{ij}}\), which is the maximum absolute column sum;</li>
  <li>2-norm: \(\norm{A}_2 = \sqrt{\rho(A^T A)}\), where \(\rho\) represents the spectral radius, i.e. the maximum eigenvalue of \(A^TA\);</li>
  <li>\(\infty\)-norm: \(\norm{A}_{\infty} = \max_{1 \leq i \leq n} \sum_{j=1}^n \abs{a_{ij}}\), which is the maximum absolute row sum.</li>
</ol>

<p>a) 1-norm: Because</p>

\[\begin{aligned}
\norm{A\vect{x}}_1 &amp;= \sum_{i=1}^n \left\vert \sum_{j=1}^n a_{ij} x_j \right\vert \leq \sum_{i=1}^n \sum_{j=1}^n \abs{a_{ij} x_j} = \sum_{j=1}^n \left( \abs{x_j} \sum_{i=1}^n \abs{a_{ij}} \right) \\
&amp;\leq \left( \sum_{j=1}^n \abs{x_j} \right) \cdot \max_{1 \leq j \leq n} \left( \sum_{i=1}^n \abs{a_{ij}} \right),
\end{aligned}\]

<p>we have</p>

\[\norm{A}_1 \leq \sup_{\forall \vect{x} \in \mathbb{R}^n, \vect{x} \neq 0} \frac{\norm{A\vect{x}}_1}{\norm{\vect{x}}_1} \leq \frac{\left( \sum_{j=1}^n \abs{x_j} \right) \cdot \max_{1 \leq j \leq n} \left( \sum_{i=1}^n \abs{a_{ij}} \right)}{\sum_{j=1}^n \abs{x_j}} = \max_{1 \leq j \leq n} \left( \sum_{i=1}^n \abs{a_{ij}} \right).\]

<p>Then, we need to show that the maximum value on the right hand side is achievable.</p>

<p>Assume that when \(j = j_0\), \(\sum_{i=1}^n \abs{a_{ij}}\) has the maximum value. If this value is zero, it means \(A\) is a zero matrix and the definition of matrix 1-norm is trivially true. If this value is not zero, by letting \(\vect{x} = (\delta_{ij_0})_{i \geq 1}^n\) with \(\delta_{ij_0}\) being the Kronecker delta, we have</p>

\[\frac{\norm{A\vect{x}}_1}{\norm{\vect{x}}_1} = \frac{\sum_{i=1}^n \abs{a_{ij_0}}}{1} = \max_{1 \leq j \leq n} \left( \sum_{i=1}^n \abs{a_{ij}} \right).\]

<p>b) 2-norm: The proof for this part needs the <em>intervention</em> of inner product \(\langle \cdot, \cdot \rangle\) of vectors in \(\mathbb{R}^n\), from which the vector 2-norm can be induced. Then we have</p>

\[\norm{A}_2 = \sup_{\forall \vect{x} \in \mathbb{R}^n, \vect{x} \neq 0} \frac{\norm{A\vect{x}}_2}{\norm{\vect{x}}_2} = \sup_{\forall \vect{x} \in \mathbb{R}^n, \vect{x} \neq 0} \sqrt{\frac{\langle A\vect{x}, A\vect{x} \rangle}{\langle \vect{x}, \vect{x} \rangle}} = \sup_{\forall \vect{x} \in \mathbb{R}^n, \vect{x} \neq 0} \sqrt{\frac{\langle A^*A\vect{x}, \vect{x} \rangle}{\langle \vect{x}, \vect{x} \rangle}},\]

<p>where \(A^*\) is the adjoint operator, i.e. transpose of \(A\). Therefore, \(A^*A\) is a real valued symmetric matrix which has \(n\) real eigenvalues \(\{\lambda_i\}_{i=1}^n\) with \(0 \leq \lambda_1 \leq \cdots \leq \lambda_n\) and \(n\) corresponding orthonormal eigenvectors \(\{\vect{v}_i\}_{i=1}^n\) (N.B. There may be duplicates in the eigenvalues).  For all \(\vect{x} \in \mathbb{R}^n\), it can be expanded as \(\vect{x} = \sum_{i=1}^n a_i \vect{v}_i\) and \(A^*A\vect{x} = \sum_{i=1}^n a_i A^*A \vect{v}_i = \sum_{i=1}^n a_i \lambda_i \vect{v}_i\). Then we have</p>

\[\begin{aligned}
\langle A^*A\vect{x}, \vect{x} \rangle &amp;= \left\langle \sum_{i=1}^n a_i \lambda_i \vect{v}_i, \sum_{j=1}^n a_j \vect{v}_j \right\rangle = \sum_{i=1}^n \sum_{j=1}^n \lambda_i a_i^2 \langle \vect{v}_i, \vect{v}_j \rangle \\
&amp;= \sum_{i=1}^n \sum_{j=1}^n \lambda_i a_i^2 \delta_{ij} = \sum_{i=1} \lambda_i a_i^2.
\end{aligned}\]

<p>Meanwhile,</p>

\[\langle \vect{x}, \vect{x} \rangle = \left\langle \sum_{i=1}^n a_i \vect{v}_i, \sum_{j=1}^n a_j \vect{v}_j \right\rangle = \sum_{i=1}^n \sum_{j=1}^n a_i a_j \langle \vect{v}_i, \vect{v}_j \rangle = \sum_{i=1}^n a_i^2.\]

<p>Therefore,</p>

\[\frac{\norm{A\vect{x}}_2}{\norm{\vect{x}}_2} \leq \sqrt{\frac{\lambda_n \sum_{i=1}^n a_i^2}{\sum_{i=1}^n a_i^2}} = \sqrt{\lambda_n}.\]

<p>By letting \(a_1 = a_2 = \cdots = a_{n-1} = 0\) and \(a_n = 1\), we have \(\frac{\norm{A\vect{x}}_2}{\norm{\vect{x}}_2} = \sqrt{\lambda_n}\). Hence,</p>

\[\norm{A}_2 = \sqrt{\lambda_n} = \sqrt{\rho(A^*A)}\]

<p>and the definition of matrix 2-norm is valid.</p>

<p>c) \(\infty\)-norm:</p>

\[\begin{aligned}
\norm{A\vect{x}}_{\infty} &amp;= \max_{1 \leq i \leq n} \left( \left\vert \sum_{j=1}^n a_{ij} x_j \right\vert \right) \leq \max_{1 \leq i \leq n} \left( \sum_{j=1}^n \abs{a_{ij}} \cdot \abs{x_j} \right) \\
&amp;= \max_{1 \leq i \leq n} \left( \left( \sum_{j=1}^n \abs{a_{ij}} \right) \cdot \left( \max_{1 \leq j \leq n} \abs{x_j} \right) \right) = \left( \max_{1 \leq i \leq n} \sum_{j=1}^n \abs{a_{ij}} \right) \cdot \left( \max_{1 \leq j \leq n} \abs{x_j} \right) \\
\norm{\vect{x}}_{\infty} &amp;= \max_{1 \leq i \leq n} \abs{x_i}
\end{aligned}\]

<p>Therefore, \(\frac{\norm{A\vect{x}}_{\infty}}{\norm{\vect{x}}_{\infty}} \leq \max_{1 \leq i \leq n} \sum_{j=1}^n \abs{a_{ij}}\). Then, we need to prove this maximum value is achievable.</p>

<p>Assume when \(i = i_0\), \(\sum_{j=1}^n \abs{a_{i_0 j}}\) achieves the maximum. If this value is zero, \(A\) is a zero matrix and the definition of matrix \(\infty\)-norm is trivially true. If this value is not zero, by letting \(\vect{x} = (\sgn(a_{i_0 1}), \cdots, \sgn(a_{i_0 n}))^{\rm T}\), we have \(\norm{\vect{x}}_{\infty} = 1\) and \(\norm{A\vect{x}}_{\infty} = \sum_{j=1}^n \abs{a_{i_0 j}} = \max_{1 \leq i \leq n} \sum_{j=1}^n \abs{a_{ij}}\).  Hence, \(\frac{\norm{A\vect{x}}_{\infty}}{\norm{\vect{x}}_{\infty}} = \max_{1 \leq i \leq n} \sum_{j=1}^n \abs{a_{ij}}\) and the definition of \(\infty\)-norm is valid.</p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="topology" /><category term="linear-algebra" /><summary type="html"><![CDATA[In our previous post, we introduced various definitions of matrix norms in \(\mathbb{R}^{n \times n}\) based on the corresponding vector norms in \(\mathbb{R}^n\). Meanwhile, the equivalence of different vector norms and their induced metrics and topologies in \(\mathbb{R}^n\) is also inherited into \(\mathbb{R}^{n \times n}\). In this article, we’ll show why the above defined matrix norms are valid.]]></summary></entry><entry><title type="html">Theorem 20.3 and metric equivalence in James Munkres Topology</title><link href="https://jihuan-tian.github.io/math/2019/01/08/munkres-topology-theo20-3-and-metric-equivalence.html" rel="alternate" type="text/html" title="Theorem 20.3 and metric equivalence in James Munkres Topology" /><published>2019-01-08T00:00:00+08:00</published><updated>2019-01-08T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2019/01/08/munkres-topology-theo20-3-and-metric-equivalence</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2019/01/08/munkres-topology-theo20-3-and-metric-equivalence.html"><![CDATA[<h1 id="proof-of-theorem-203">Proof of Theorem 20.3</h1>

<p><strong>Theorem 20.3</strong> The topologies on \(\mathbb{R}^n\) induced by the euclidean metric \(d\) and the square metric \(\rho\) are the same as the product topology on \(\mathbb{R}^n\).</p>

<p><strong>Proof:</strong> a) Prove the two metrics can mutually limit each other.</p>

<p>Because</p>

\[\rho(\vect{x}, \vect{y}) = \max_{1 \leq i \leq n} \abs{x_i - y_i} = \left( \max_{1 \leq i \leq n} (x_i - y_i)^2 \right)^{\frac{1}{2}}\]

<p>and the scalar function \(f(x) = x^{\frac{1}{2}}\) is increasing when \(x \geq 0\), then from</p>

\[\max_{1 \leq i \leq n} (x_i - y_i)^2 \leq \sum_{i=1}^n (x_i - y_i)^2,\]

<p>we have</p>

\[\left( \max_{1 \leq i \leq n} (x_i - y_i)^2 \right)^{\frac{1}{2}} \leq \left( \sum_{i=1}^n (x_i - y_i)^2 \right)^{\frac{1}{2}}.\]

<p>Hence,</p>

\[\rho(\vect{x}, \vect{y}) \leq d(\vect{x}, \vect{y}).\]

<p>Meanwhile,</p>

\[\left( \sum_{i=1}^n (x_i - y_i)^2 \right)^{\frac{1}{2}} \leq \left( n \max_{1 \leq i \leq n} (x_i - y_i)^2 \right)^{\frac{1}{2}} = \left( n \left( \max_{1 \leq i \leq n} \abs{x_i - y_i} \right)^2 \right)^{\frac{1}{2}}.\]

<p>Therefore,</p>

\[d(\vect{x}, \vect{y}) \leq \sqrt{n} \rho(\vect{x}, \vect{y}).\]

<p>Summarize the above we have</p>

\[\rho(\vect{x}, \vect{y}) \leq d(\vect{x}, \vect{y}) \leq \sqrt{n} \rho(\vect{x}, \vect{y})\]

<p>and its equivalent form</p>

\[\frac{1}{\sqrt{n}} d(\vect{x}, \vect{y}) \leq \rho(\vect{x}, \vect{y}) \leq d(\vect{x}, \vect{y}).\]

<p>b) Prove the two metrics generate the same topology.</p>

<p>For all \(\vect{x} \in \mathbb{R}^n\) and \(\varepsilon &gt; 0\), because \(d(\vect{x}, \vect{y}) \leq \sqrt{n} \rho(\vect{x}, \vect{y})\), if we let \(\sqrt{n} \rho(\vect{x}, \vect{y}) &lt; \varepsilon\), we also have \(d(\vect{x}, \vect{y}) &lt; \varepsilon\). This means the open ball \(B_{\rho}(\vect{x}, \frac{\varepsilon}{\sqrt{n}})\) in the topology induced by \(\rho\) is contained in the open ball \(B_d(\vect{x}, \varepsilon)\) in the topology induced by \(d\). So the square metric topology is finer than the euclidean metric topology according to Lemma 20.2.</p>

<p>Meanwhile, by letting \(\rho(\vect{x}, \vect{y}) \leq d(\vect{x}, \vect{y}) &lt; \varepsilon\), we have the open ball \(B_d(\vect{x}, \varepsilon)\) being contained in the open ball \(B_{\rho}(\vect{x}, \varepsilon)\), which proves the euclidean metric topology is finer than the square metric topology.</p>

<p>Therefore, the two metrics generate the same topology.</p>

<p><strong>Comment</strong> It can be seen that when a certain open ball radius is given, the larger the metric being defined, the smaller the open ball in the sense of set inclusion or cardinality.</p>

<p>c) Prove the topology induced by \(\rho\) is the same as the product topology on \(\mathbb{R}^n\).</p>

<p>Let \(\vect{B} = \prod_{i=1}^n (a_i, b_i)\) be a basis element for \(\mathbb{R}^n\) with the product topology. For all \(\vect{x} \in \vect{B}\) and \(i \in \{1, \cdots ,n\}\), there exists an \(\varepsilon_i &gt; 0\) such that \(x_i \in (x_i - \varepsilon_i, x_i + \varepsilon_i) \subset (a_i, b_i)\). Let \(\varepsilon = \min_{1 \leq i \leq n} \{ \varepsilon_i\}\), we have \(x_i \in (x_i - \varepsilon, x_i + \varepsilon) \subset (a_i, b_i)\). Because \(B_{\rho}(\vect{x}, \varepsilon) = \prod_{i=1}^n (x_i - \varepsilon, x_i + \varepsilon)\), we have \(\vect{x} \in B_{\rho}(\vect{x}, \varepsilon) \subset \vect{B}\). Hence, the square metric topology is finer than the product topology on \(\mathbb{R}^n\).</p>

<p>On the other hand, let \(B_{\rho}(\vect{x}, \varepsilon)\) be an arbitrary open ball in \(\mathbb{R}^n\) with the square metric topology, it is itself a basis element for the product topology. Therefore, the product topology is finer than the square metric topology.</p>

<p>Finally, the two metrics generate the same topology as the product topology on \(\mathbb{R}^n\).</p>

<p><strong>Comment</strong> It should be noted that although \(B_{\rho}(\vect{x}, \varepsilon) = \prod_{i=1}^n (x_i - \varepsilon, x_i + \varepsilon)\), we do not have \(B_{\bar{\rho}}(\vect{x}, \varepsilon) = \prod_{i=1}^{\infty} (x_i - \varepsilon, x_i + \varepsilon)\), where \(\bar{\rho}\) is the uniform metric on \(\mathbb{R}^{\omega}\). This point has been mentioned in <a href="https://www.cnblogs.com/peabody/p/10223052.html">this post</a>.</p>

<p><strong>Remark</strong> This theorem can be generalized as below.</p>

<blockquote>
  <p>If any two metrics \(d_1\) and \(d_2\) on a space \(X\) can be mutually limited, i.e. for all \(x\) and \(y\) in \(X\), there exist positive constants \(C_1\) and \(C_2\) such that \(C_1 d_1(x, y) \leq d_2(x, y) \leq C_2 d_1(x, y)\), then the two metrics induce the same topology on \(X\).</p>
</blockquote>

<p>Then, these two metrics are considered to be equivalent in a topological sense and such “equivalence” can be understood  like this. We have already known in <a href="https://www.cnblogs.com/peabody/p/10125356.html">this post</a> that in a topological space, the concept of convergence is defined based on using a collection of nested open sets as rulers for “distance” measurement, when there is still no metric established. The equivalence of two metrics in a topological sense just means that the convergence behaviors in the topological spaces induced from these two metrics are the same.</p>

<h1 id="examples-of-equivalent-metrics">Examples of equivalent metrics</h1>

<p>In linear algebra, we have already witnessed examples of equivalent metrics, which are induced from corresponding norms for vectors or matrices.</p>

<p>For all \(\vect{x} \in \mathbb{R}^n\), the following is a list of commonly adopted vector norms:</p>

<ol>
  <li>1-norm: \(\norm{\vect{x}}_1 = \sum_{i = 1}^n \abs{x_i}\).</li>
  <li>2-norm: \(\norm{\vect{x}}_2 = \left( \sum_{i=1}^n \abs{x_i}^2 \right)^{\frac{1}{2}}\).</li>
  <li>\(\infty\)-norm: \(\norm{\vect{x}}_{\infty} = \max_{1 \leq i \leq n} \abs{x_i}\).</li>
</ol>

<p>It is easy to prove that these norms are equivalent as below, which implies the equivalence of their induced metrics and also the induced topologies on \(\mathbb{R}^n\).</p>

\[\begin{align*}
\norm{\vect{x}}_{\infty} \leq &amp; \norm{\vect{x}}_1 \leq n \norm{\vect{x}}_{\infty} \\
\norm{\vect{x}}_{\infty} \leq &amp; \norm{\vect{x}}_2 \leq \sqrt{n} \norm{\vect{x}}_{\infty} \\
\frac{1}{\sqrt{n}} \norm{\vect{x}}_2 \leq &amp; \norm{\vect{x}}_1 \leq n \norm{\vect{x}}_2
\end{align*}.\]

<p>Based on the definition of vector norms, the corresponding norms for matrices, which are treated as linear operators on vector space, can also be induced. For all \(A \in \mathbb{R}^{n \times n}\), possible matrix norms are</p>

<ol>
  <li>1-norm: \(\norm{A}_1 = \sup_{\forall \vect{x} \in \mathbb{R}^n, \vect{x} \neq 0} \frac{\norm{A \vect{x}}_1}{\norm{\vect{x}}_1} = \max_{1 \leq j \leq n} \sum_{i=1}^n \abs{a_{ij}}\), which is the maximum column sum;</li>
  <li>2-norm: \(\norm{A}_2 = \sup_{\forall \vect{x} \in \mathbb{R}^n, \vect{x} \neq 0} \frac{\norm{A \vect{x}}_2}{\norm{\vect{x}}_2} = \sqrt{\rho(A^T A)}\), where \(\rho\) represents the spectral radius, i.e. the maximum eigenvalue of \(A^TA\);</li>
  <li>\(\infty\)-norm: \(\norm{A}_{\infty} = \sup_{\forall \vect{x} \in \mathbb{R}^n, \vect{x} \neq 0} \frac{\norm{A \vect{x}}_{\infty}}{\norm{\vect{x}}_{\infty}} = \max_{1 \leq i \leq n} \sum_{j=1}^n \abs{a_{ij}}\), which is the maximum row sum.</li>
</ol>

<p>The equivalence of these matrix norms can be directly derived from the equivalence of vector norms. For example, because \(\norm{A\vect{x}}_1 \leq n \norm{A\vect{x}}_2\) and \(\norm{\vect{x}}_1 \geq \frac{1}{\sqrt{n}} \norm{\vect{x}}_2\), we have</p>

\[\frac{\norm{A\vect{x}}_1}{\norm{\vect{x}}_1} \leq \frac{n \norm{A\vect{x}}_2}{\frac{1}{\sqrt{n}}\norm{\vect{x}}_2} = n\sqrt{n}\frac{\norm{A\vect{x}}_2}{\norm{\vect{x}}_2}.\]

<p>From \(\norm{A\vect{x}}_1 \geq \frac{1}{\sqrt{n}} \norm{A\vect{x}}_2\) and \(\norm{\vect{x}}_1 \leq n \norm{\vect{x}}_2\), we have</p>

\[\frac{1}{n\sqrt{n}}\frac{\norm{A\vect{x}}_2}{\norm{\vect{x}}_2} \leq \frac{\norm{A\vect{x}}_1}{\norm{\vect{x}}_1}.\]

<p>By taking supremum operation on both sides of the two inequalities,</p>

\[\frac{1}{n\sqrt{n}} \norm{A}_2 \leq \norm{A}_1 \leq n\sqrt{n} \norm{A}_2.\]

<p>Similarly, we also have</p>

\[\begin{align*}
\frac{1}{n} \norm{A}_{\infty} \leq &amp; \norm{A}_1 \leq n \norm{A}_{\infty} \\
\frac{1}{\sqrt{n}} \norm{A}_{\infty} \leq &amp; \norm{A}_2 \leq \sqrt{n} \norm{A}_{\infty}
\end{align*}.\]

<p>The equivalence of matrix norms implies the equivalence of their induced metrics and topologies on \(\mathbb{R}^{n \times n}\).</p>

<p>Backlinks: <a href="/math/2019/01/10/matrix-norm.html">《Matrix norm》</a></p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="topology" /><summary type="html"><![CDATA[Proof of Theorem 20.3]]></summary></entry><entry><title type="html">Theorem 20.4 in James Munkres Topology</title><link href="https://jihuan-tian.github.io/math/2019/01/04/munkres-theo20-4.html" rel="alternate" type="text/html" title="Theorem 20.4 in James Munkres Topology" /><published>2019-01-04T00:00:00+08:00</published><updated>2019-01-04T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2019/01/04/munkres-theo20-4</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2019/01/04/munkres-theo20-4.html"><![CDATA[<p><strong>Theorem 20.4</strong> The uniform topology on \(\mathbb{R}^J\) is finer than the product topology and coarser than the box topology; these three topologies are all different if \(J\) is infinite.</p>

<p><strong>Proof:</strong> a) Prove the uniform topology is finer than the product topology.</p>

<p><strong>Analysis:</strong> Look inside an open ball in the product topology for an open ball in the uniform topology and then apply Lemma 20.2. It should be also noted that the product topology on \(\mathbb{R}^J\) has each of its coordinate space assigned the standard topology, which is consistent with both topologies induced from the two metrics \(d\) and \(\bar{d}\) according to example 2 in this section and Theorem 20.1.</p>

<p>According to the second part of Theorem 19.2, let \(\prod_{\alpha \in J} B_{\alpha}\) be an arbitrary basis element for the product topology on \(\mathbb{R}^J\), where only a finite number of \(B_{\alpha}\)s are open intervals in \(\mathbb{R}\) and not equal to \(\mathbb{R}\). Let the indices for these \(B_{\alpha}\)s be \(\{\alpha_1, \cdots, \alpha_n\}\) and for all \(i \in \{1, \cdots, n\}\), \(B_{\alpha_i} = (a_i, b_i)\). Then for all \(\vect{x} \in \prod_{\alpha \in J} B_{\alpha}\) and for all \(\alpha \in J\), \(x_{\alpha} \in B_{\alpha}\). Specifically, for all \(i \in \{1, \cdots, n\}\), \(x_{\alpha_i} \in B_{\alpha_i}\). Let \(\varepsilon_{\alpha_i} = \min \{ x_{\alpha_i} - a_i, b_i - x_{\alpha_i} \}\) and \(\varepsilon = \min_{1 \leq i \leq n} \{\varepsilon_{\alpha_1}, \cdots, \varepsilon_{\alpha_n}\}\). Then we’ll check the open ball \(B_{\bar{\rho}}(\vect{x}, \varepsilon)\) in \(\mathbb{R}^J\) with the uniform topology is contained in the basis element \(\prod_{\alpha \in J} B_{\alpha}\).</p>

<p>For all \(\vect{y} \in B_{\bar{\rho}}(\vect{x}, \varepsilon)\), \(\bar{\rho}(\vect{x}, \vect{y}) &lt; \varepsilon\), i.e. \(\sup_{\forall \alpha \in J} \{\bar{d}(x_{\alpha}, y_{\alpha})\} &lt; \varepsilon\). Therefore, for all \(i \in \{1, \cdots, n\}\), \(\bar{d}(x_{\alpha_i}, y_{\alpha_i}) &lt; \varepsilon\). Note that when \(\varepsilon &gt; 1\), \(B_{\bar{\rho}}(\vect{x}, \varepsilon) = \mathbb{R}^J\), which is not what we desire. Instead, we need to define the open ball’s radius as \(\varepsilon' = \min\{\varepsilon, 1\}\). Then we have for all \(\vect{y} \in B_{\bar{\rho}}(\vect{x}, \varepsilon')\), \(\bar{d}(x_{\alpha_i}, y_{\alpha_i}) = d(x_{\alpha_i}, y_{\alpha_i}) &lt; \varepsilon'\), i.e. \(y_{\alpha_i} \in (x_{\alpha_i} - \varepsilon', x_{\alpha_i} + \varepsilon') \subset B_{\alpha_i}\). For other coordinate indices \(\alpha \notin \{\alpha_1, \cdots, \alpha_n\}\), because \(B_{\alpha} = \mathbb{R}\), \(y_{\alpha} \in (x_{\alpha} - \varepsilon', x_{\alpha} + \varepsilon') \subset B_{\alpha}\) holds trivially.</p>

<p>Therefore, the uniform topology is finer than the product topology.</p>

<p>b) Prove the uniform topology is strictly finer than the product topology, when \(J\) is infinite.</p>

<p>When \(J\) is infinite, for an open ball \(B_{\bar{\rho}}(\vect{x}, \varepsilon)\) with \(\varepsilon \in (0, 1]\), there are infinite number of coordinate components comprising this open ball which are not equal to \(\mathbb{R}\). Therefore, there is no basis element for the product topology which is contained in \(B_{\bar{\rho}}(\vect{x}, \varepsilon)\).</p>

<p>c) Prove the box topology is finer than the uniform topology.</p>

<p>For any basis element \(B_{\bar{\rho}}(\vect{x}, \varepsilon)\) for the uniform topology, when \(\varepsilon &gt; 1\), \(B_{\bar{\rho}}(\vect{x}, \varepsilon) = \mathbb{R}^J\). Then for all \(\vect{y} \in B_{\bar{\rho}}(\vect{x}, \varepsilon)\), any basis element for the box topology containing this \(\vect{y}\) is contained in \(B_{\bar{\rho}}(\vect{x}, \varepsilon)\).</p>

<p>When \(\varepsilon \in (0, 1]\), \(\bar{d}\) is equivalent to \(d\) on \(\mathbb{R}\). Then for all \(\vect{y} \in B_{\bar{\rho}}(\vect{x}, \varepsilon)\), we have</p>

\[\sup_{\alpha \in J} \{ \bar{d}(x_{\alpha}, y_{\alpha}) \} = \sup_{\alpha \in J} \{ d(x_{\alpha}, y_{\alpha}) \} &lt; \varepsilon.\]

<p>Therefore, for all \(\alpha \in J\), \(y_{\alpha} \in (x_{\alpha} - \varepsilon, x_{\alpha} + \varepsilon)\). Then we may tend to say that \(\prod_{\alpha \in J} (x_{\alpha} - \varepsilon, x_{\alpha} + \varepsilon)\) is a basis element for the box topology containing \(\vect{y}\), which is contained in \(B_{\bar{\rho}}(\vect{x}, \varepsilon)\). However, this is not true. Because \(\vect{y}\) can be thus selected such that as \(\alpha\) changes in \(J\), \(\bar{d}(x_{\alpha}, y_{\alpha})\) can be arbitrarily close to \(\varepsilon\), which leads to \(\sup_{\alpha \in J} \{ \bar{d}(x_{\alpha}, y_{\alpha}) \} = \varepsilon\). This makes \(\vect{y} \notin B_{\bar{\rho}}(\vect{x}, \varepsilon)\) and \(\prod_{\alpha \in J} (x_{\alpha} - \varepsilon, x_{\alpha} + \varepsilon)\) is not contained in \(B_{\bar{\rho}}(\vect{x}, \varepsilon)\). Such example can be given for \(\mathbb{R}^{\omega}\), where we let \(\vect{y} = \{y_n = x_n + \varepsilon - \frac{\varepsilon}{n}\}_{n \in \mathbb{Z}_+}\). When \(n \rightarrow \infty\), \(\bar{d}(x_n, y_n) \rightarrow \varepsilon\).</p>

<p>With this point clarified, a smaller basis element should be selected for the box topology, such as \(\prod_{\alpha \in J} (x_{\alpha} - \frac{\varepsilon}{2}, x_{\alpha} + \frac{\varepsilon}{2})\). For all \(\vect{y}\) in this basis element, \(\sup_{\alpha \in J} \{ \bar{d}(x_{\alpha}, y_{\alpha}) \} \leq \frac{\varepsilon}{2} &lt; \varepsilon\). Hence \(\prod_{\alpha \in J} (x_{\alpha} - \frac{\varepsilon}{2}, x_{\alpha} + \frac{\varepsilon}{2}) \subset B_{\bar{\rho}}(\vect{x}, \varepsilon)\) and the box topology is finer than the uniform topology.</p>

<p><strong>Remark:</strong> The proof in the book for this part inherently adopts the definition of open set via topological basis introduced in section 13.</p>

<p>d) Prove the box topology is strictly finer than the uniform topology, when \(J\) is infinite.</p>

<p><strong>Analysis:</strong> Because the open ball in the uniform topology sets an upper bound on the dimension of each coordinate component, it can be envisioned that if we construct a basis element for the box topology with the dimension for each coordinate component approaching to zero, it cannot cover any open ball in the uniform topology with a fixed radius no matter how small it is.</p>

<p>Let’s consider the case in \(\mathbb{R}^{\omega}\). Select a basis element for the box topology as \(\prod_{n = 1}^{\infty} (x_n - \frac{c}{n}, x_n + \frac{c}{n})\) with \((c &gt; 0)\). Then for all \(\varepsilon &gt; 0\), there exists \(\vect{y}_0 \in B_{\bar{\rho}}(\vect{x}, \varepsilon)\) such that \(\vect{y}_0 \notin \prod_{n = 1}^{\infty} (x_n - \frac{c}{n}, x_n + \frac{c}{n})\). For example, we can select \(\vect{y}_0 = (x_n + \frac{\varepsilon}{2})_{n \geq 1}\). Then there exists an \(n_0 \in \mathbb{Z}_+\) such that when \(n &gt; n_0\), \(\frac{c}{n} &lt; \frac{\varepsilon}{n}\) and \(y_n \notin (x_n - \frac{c}{n}, x_n + \frac{c}{n})\). Hence, the box topology is strictly finer than the uniform topology.</p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="topology" /><summary type="html"><![CDATA[Theorem 20.4 The uniform topology on \(\mathbb{R}^J\) is finer than the product topology and coarser than the box topology; these three topologies are all different if \(J\) is infinite.]]></summary></entry><entry><title type="html">A tuple is defined as a function</title><link href="https://jihuan-tian.github.io/math/2018/12/23/a-tuple-is-defined-as-a-function.html" rel="alternate" type="text/html" title="A tuple is defined as a function" /><published>2018-12-23T00:00:00+08:00</published><updated>2018-12-23T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2018/12/23/a-tuple-is-defined-as-a-function</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2018/12/23/a-tuple-is-defined-as-a-function.html"><![CDATA[<p>In James Munkres “Topology”, the concept for a tuple, which can be \(m\)-tuple, \(\omega\)-tuple or \(J\)-tuple, is defined from a <em>function</em> point of view as below.</p>

<p>Let \(X\) be a set.</p>
<ul>
  <li>
    <p>Let \(m\) be a positive integer and \(\{ 1, \cdots, m \}\) be an index set. An \(m\)-tuple of elements in \(X\) is a function</p>

\[\vect{x}: \{ 1, \cdots, m \} \rightarrow X.\]
  </li>
  <li>
    <p>Let \(\mathbb{Z}_+\) be the index set comprised of all positive integers. An \(\omega\)-tuple of elements in \(X\) is a function</p>

\[\vect{x}: \mathbb{Z}_+ \rightarrow X.\]
  </li>
  <li>
    <p>Let \(J\) be an index set, whose cardinality is not limited to be finite or infinite, countable or uncountable. A \(J\)-tuple of elements in \(X\) is a function</p>

\[\vect{x}: J \rightarrow X.\]
  </li>
</ul>

<p>For all these types of tuples, if \(\alpha\) is an index belongs to the index set, the corresponding coordinate component of the tuple is \(\vect{x}(\alpha)\). It is written as \(x_{\alpha}\), which is the form we often use.</p>

<p>From the above it can be seen that a tuple of elements, which are literally tangible <em>data</em>, are viewed as the rule of assignment for a <em>function</em>, which is more abstract. In addition, while we have already been given to the stereotype of a tuple, which is a container holding a list of ordered elements, the function mapping version of a tuple does not require any order relation prescribed for the tuple’s index set.</p>

<p>Considering these concepts in computer programming, a tuple of values or objects can be either stored in an ordered array as in procedural programming. Or the tuple can be stored within a function as in functional programming. Without loss of generality, this functional perspective can be further applied to matrix and tensor, which eliminates or mingles the boundary between data and operation.</p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="topology" /><summary type="html"><![CDATA[In James Munkres “Topology”, the concept for a tuple, which can be \(m\)-tuple, \(\omega\)-tuple or \(J\)-tuple, is defined from a function point of view as below.]]></summary></entry><entry><title type="html">Theorem 19.6 in James Munkres Topology</title><link href="https://jihuan-tian.github.io/math/2018/12/23/munkres-topology-theo19-6.html" rel="alternate" type="text/html" title="Theorem 19.6 in James Munkres Topology" /><published>2018-12-23T00:00:00+08:00</published><updated>2018-12-23T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2018/12/23/munkres-topology-theo19-6</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2018/12/23/munkres-topology-theo19-6.html"><![CDATA[<p><strong>Theorem 19.6</strong> Let \(f: A \rightarrow \prod_{\alpha \in J} X_{\alpha}\) be given by the equation</p>

\[f(a) = (f_{\alpha}(a))_{\alpha \in J},\]

<p>where \(f_{\alpha}: A \rightarrow X_{\alpha}\) for each \(\alpha\). Let \(\prod X_{\alpha}\) have the product topology. Then the function \(f\) is continuous if and only if each function \(f_{\alpha}\) is continuous.</p>

<p><strong>Comment:</strong> This is an extension of Theorem 18.4, where only two component spaces are involved.</p>

<p><strong>Proof:</strong> a) First, we prove the projection map is continuous, which is defined on the Cartesian space constructed from a \(J\)-tuple of component spaces .</p>

<p>For all \(\beta \in J\), let \(\pi_{\beta}: \prod X_{\alpha} \rightarrow X_{\beta}\) be the projection map. For arbitrary open set \(V_{\beta}\) in \(X_{\beta}\), its pre-image under \(\pi_{\beta}\) is \(\pi_{\beta}^{-1}(V_{\beta})\), which is a subbasis element for the product topology on \(\prod X_{\alpha}\). Therefore, \(\pi_{\beta}^{-1}(V_{\beta})\) is open and the projection map \(\pi_{\beta}\) is continuous.</p>

<p>Next, we notice that for all \(\alpha \in J\), the coordinate function \(f_{\alpha}: A \rightarrow X_{\alpha}\) is a composition of the two continuous functions \(f\) and \(\pi_{\alpha}\), i.e. \(f_{\alpha} = \pi_{\alpha} \circ f\). Then according to Theorem 18.2 (c), \(f_{\alpha}\) is continuous.</p>

<p><strong>Remark:</strong> Because the box topology is finer than the product topology, the projection map is also continuous when the box topology is adopted for \(\prod X_{\alpha}\). Therefore, this part of the theorem is true for both product topology and box topology.</p>

<p>b) <strong>Analysis:</strong> To prove the continuity of a function, showing that the pre-image of any subbasis element in the range space is open in the domain space is more efficient than using basis element or <em>raw</em> open set in the range space. In addition, the subbasis element for the product topology on \(\prod X_{\alpha}\) has the form \(\pi_{\beta}^{-1}(U_{\beta})\) with \(U_{\beta}\) being a single coordinate component and open in \(X_{\beta}\). This is the clue of the proof.</p>

<p>For all \(\beta \in J\) and arbitrary open set \(U_{\beta}\) in \(X_{\beta}\), we have \(f_{\beta}^{-1}(U_{\beta}) = f^{-1} \circ \pi_{\beta}^{-1}(U_{\beta})\). Because \(f_{\beta}\) is continuous and \(U_{\beta}\) is open, \(f_{\beta}^{-1}(U_{\beta})\) is open. In addition, \(\pi_{\beta}^{-1}(U_{\beta})\) is an arbitrary subbasis element for \(\prod X_{\alpha}\) with the product topology, whose pre-image under \(f\) is just the open set \(f_{\beta}^{-1}(U_{\beta})\), therefore \(f\) is continuous.</p>

<p><strong>Remark:</strong> Part b) of this theorem really depends on the adopted topology for \(\prod X_{\alpha}\), which can be understood as below.</p>

<p>At first, we will show that for all \(\vect{U} = \prod U_{\alpha}\) being a subset of \(\prod X_{\alpha}\), \(f^{-1}(\vect{U}) = \bigcap_{\alpha \in J} f_{\alpha}^{-1}(U_{\alpha})\).</p>

<p>For all \(x \in f^{-1}(\vect{U})\), because \(f(x) \in \vect{U}\), then for all \(\alpha \in J\), \(f_{\alpha}(x) \in U_{\alpha}\), hence \(x \in \bigcap_{\alpha \in J} f_{\alpha}^{-1}(U_{\alpha})\) and \(f^{-1}(\vect{U}) \subset \bigcap_{\alpha \in J} f_{\alpha}^{-1}(U_{\alpha})\).</p>

<p>On the other hand, for all \(x \in \bigcap_{\alpha \in J} f_{\alpha}^{-1}(U_{\alpha})\), we have for all \(\alpha \in J\), \(f_{\alpha}(x) \in U_{\alpha}\). Therefore, \(f(x) \in \vect{U}\) and \(x \in f^{-1}(\vect{U})\). Hence \(\bigcap_{\alpha \in J} f_{\alpha}^{-1}(U_{\alpha}) \subset f^{-1}(\vect{U})\).</p>

<p>Next, if we assign the product topology to \(\prod X_{\alpha}\), for any \(\vect{U} = \prod U_{\alpha}\) with \(U_{\alpha}\) open in \(X_{\alpha}\) and only a finite number of them not equal to \(X_{\alpha}\), it is a basis element of the product topology. Let the set of all indices with which \(U_{\alpha} \neq X_{\alpha}\) be \(\{\alpha_1, \cdots, \alpha_n\}\) and also notice that when \(U_{\alpha} = X_{\alpha}\), \(f_{\alpha}^{-1}(U_{\alpha}) = A\), we have</p>

\[f^{-1}(\vect{U}) = \bigcap_{\alpha \in J} f_{\alpha}^{-1}(U_{\alpha}) = \bigcap_{i=1}^n f_{\alpha_i}^{-1}(U_{\alpha_i}),
\tag{*}
\label{eq:intersection}\]

<p>where those \(f_{\alpha}^{-1}(U_{\alpha})\) with \(\alpha \notin \{\alpha_1, \cdots, \alpha_n\}\) do not contribute to the intersection. This indicates that \(f^{-1}(\vect{U})\) is a finite intersection of open sets which is still open. Hence \(f\) is continuous.</p>

<p>However, if the box topology is adopted for \(\prod X_{\alpha}\), qualitatively speaking, because the topology for the range space becomes finer, according to our <a href="https://www.cnblogs.com/peabody/p/10125356.html">previous post</a>, it makes a function to be continuous more difficult. Specifically in this theorem, \(f^{-1}(\vect{U})\) in \eqref{eq:intersection} can be an intersection of infinite number of open sets \(U_{\alpha}\) not equal to \(X_{\alpha}\). Thus \(f^{-1}(\vect{U})\) may not be open anymore.</p>

<p>After understanding this point, it is not difficult to construct a counter example for part b) as below.</p>

<p>Let \(f: \mathbb{R} \rightarrow \mathbb{R}^{\omega}\) be defined as \(f(t) = (t, t, \cdots)\). Select a basis element \(\vect{U}\) in \(\mathbb{R}^{\omega}\) such that the intersection of all its coordinate components is not open. For example, \(\vect{U} = \prod_{n=1}^{\infty} (-\frac{1}{n}, \frac{1}{n})\), which is a neighborhood of \(f(0) = (0, 0, \cdots)\).</p>

<p>For any basis element \((a, b)\) in \(\mathbb{R}\) containing \(0\), with \(a &lt; 0\) and \(b &gt; 0\), by letting \(\delta = \min\{-a, b\}\), we have \((-\delta, \delta) \subset (a, b)\) and \(0 \in (-\delta, \delta)\). The image of \((-\delta, \delta)\) under \(f\) is \(\prod_{n=1}^{\infty} (-\delta, \delta)\). Then there exist an \(n_0 \in \mathbb{Z}_+\) such that \((-\delta, \delta)\) is not contained in \((-\frac{1}{n_0}, \frac{1}{n_0})\). Therefore, \(\pi_{n_0}(f((-\delta, \delta)))\) is not contained in \(\pi_{n_0}(\vect{U})\) and \(\pi_{n_0}(f((a, b)))\) is not contained in \(\pi_{n_0}(\vect{U})\). Hence the image of \((a, b)\) under \(f\) is not contained in \(\vect{U}\). This contradicts Theorem 18.1 (4) and \(f\) is not continuous.</p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="topology" /><summary type="html"><![CDATA[Theorem 19.6 Let \(f: A \rightarrow \prod_{\alpha \in J} X_{\alpha}\) be given by the equation]]></summary></entry><entry><title type="html">Different ways of constructing continuous functions</title><link href="https://jihuan-tian.github.io/math/2018/12/19/constructing-continuous-functions.html" rel="alternate" type="text/html" title="Different ways of constructing continuous functions" /><published>2018-12-19T00:00:00+08:00</published><updated>2018-12-19T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2018/12/19/constructing-continuous-functions</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2018/12/19/constructing-continuous-functions.html"><![CDATA[<p>This post summarises different ways of constructing continuous functions, which are introduced in Section 18 of James Munkres “Topology”.</p>

<ol>
  <li>Constant function.</li>
  <li>
    <p>Inclusion function.</p>

    <p>N.B. The function domain should have the subspace topology relative to the range.</p>
  </li>
  <li>Composition of continuous functions. Specifically, composition of continuous real-valued functions via simple arithmetic, i.e. sum, difference, product and quotient. For the case of quotient, the function as the denominator should never be evaluated to 0.</li>
  <li>
    <p>Restricting the domain of a continuous function.</p>

    <p>N.B. The reduced domain should be assigned the subspace topology with respect to the original domain.</p>
  </li>
  <li>
    <p>Restricting or expanding the range of a continuous function.</p>

    <p>N.B. The smaller range should have the subspace topology with respect to the larger range.</p>
  </li>
  <li>Local formulation of continuity,i.e. the function is continuous if it is still continuous after restricting its domain to each open set in an open covering of the original domain.</li>
  <li>
    <p>Pasting continuous functions with their domains on patches of closed sets which cover the whole domain.</p>

    <p>Comment: In the overlapping subdomain, the functions on different patches should be defined consistently. This condition is not required in “local formulation of continuity”, where the covering of the whole domain is made from open sets instead of closed sets. From this difference, we can <em>sense</em> the difference between open set and closed set. The former is intrinsically related to continuity, which can be phenomenologically construed as that the open sets can <strong>infiltrate</strong> into one another infinitely, even though the amount of infiltration is often infinitesimal if a metric is also assigned to the space. On the contrary, the latter has a clearly set demarcation or buffer zone between the functions on different patches without further penetration or interaction. Therefore, it does not intrinsically imply continuity and the function values in the overlapping subdomain must be consistent to ensure the continuity of the fully assembled function.</p>
  </li>
  <li>Maps into products, which ensures the equivalence between the continuity of the original function and that of its coordinate functions.</li>
  <li>
    <p>Uniform limit of a sequence of continuous functions.</p>

    <p>N.B. The range space of these functions should have a metric.</p>
  </li>
</ol>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="topology" /><summary type="html"><![CDATA[This post summarises different ways of constructing continuous functions, which are introduced in Section 18 of James Munkres “Topology”.]]></summary></entry><entry><title type="html">Exercise 12 in Section 18 of James Munkres Topology</title><link href="https://jihuan-tian.github.io/math/2018/12/17/munkres-topology-s18e12.html" rel="alternate" type="text/html" title="Exercise 12 in Section 18 of James Munkres Topology" /><published>2018-12-17T00:00:00+08:00</published><updated>2018-12-17T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2018/12/17/munkres-topology-s18e12</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2018/12/17/munkres-topology-s18e12.html"><![CDATA[<p>Theorem 18.4 in James Munkres “Topology” states that if a function $f : A \rightarrow X \times Y$ is continuous, its coordinate functions $f_1 : A \rightarrow X$ and $f_2 : A \rightarrow Y$ are also continuous, and the converse is also true. This is what we have been familiar with, such as a continuous parametric curve $f: [0, 1] \rightarrow \mathbb{R}^3$ defined as $f(t) = (x(t), y(t), z(t))^T$ with its three components being continuous. However, if a function $g: A \times B \rightarrow X$ is separately continuous in each of its components, i.e. both $g_1: A \rightarrow X$ and $g_2 : B \rightarrow X$ are continuous, $g$ is not necessarily continuous.</p>

<p>Here, the said “separately continuous in each of its components” means arbitrarily selecting the value of one component variable from its domain and fix it, then the original function depending only on the other component is continuous. In the above, the function $g$ can be envisaged as a curved surface in 3D space. With $g_1$ being continuous, the intersection profiles between this curved surface and those planes perpendicular to the coordinate axis for $B$ are continuous. Similarly, because $g_2$ is continuous, the intersection profiles obtained from those planes perpendicular to the coordinate axis for $A$ are also continuous. The continuity of intersection curves is only ensured in these two special directions, so it is not guaranteed that the original function $g$ is continuous.</p>

<p>In Exercise 12 of Section 18, an example is given as
\(F(x \times y) = \begin{cases}
\frac{xy}{x^2 + y^2} &amp; (x \neq 0, y \neq 0) \\
0 &amp; (x = 0, y = 0)
\end{cases},\)
where $F$ is continuous separately in each of its component variables but is not continuous by itself. This is function is visualized below.</p>

<p><img src="/figures/munkres-topology-s18e12.gif" alt="F(x,y)=xy/(x^2+y^2)" /></p>

<p>Fix $y$ at $y_0$, we have $F_{y_0}(x) = F(x \times y_0)$. When $y_0 \neq 0$, $F_{y_0}(x)$ is continuous with respect to $x$ because it is only a composition of continuous real valued functions via simple arithmetic. When $y_0 = 0$, if $x \neq 0$, $F_0(x) = 0$; if $x =0$, $F_0(x)$ is also 0 due to the definition of $F(x \times y)$. Therefore, $F_0(x)$ is a constant function, which is continuous due to Theorem 18.2 (a). Similarly, $F_{x_0}(y)$ is also continuous with respect to $y$.</p>

<p>However, if we let $x = y$ and approach $(x, y) = (x, x)$ to $(0, 0)$, it can be seen that $F(x \times x)$ is not continuous, because</p>

<ul>
  <li>when $x \neq 0$, $F(x \times x) = \frac{x^2}{x^2 + x^2} = \frac{1}{2}$;</li>
  <li>when $x = 0$, $F(x \times x) = 0$.</li>
</ul>

<p>If we let $x = -y$ and approach $(x ,y) = (x, -x)$ to $(0, 0)$, $F = -\frac{1}{2}$ when $x \neq 0$ and $F = 0$ when $x = 0$.</p>

<p>Then, if we select an open set such as $(-\frac{1}{4}, \frac{1}{4})$ around the function value $0$ in $\mathbb{R}$, its pre-image $U$ in $\mathbb{R} \times \mathbb{R}$ should include the point $(0, 0)$ and exclude the rays $(x, x)$ and $(x, -x)$ with $x \in \mathbb{R}$ and $x \neq 0$. Due to these excluded rays, there is no neighborhood of $(0, 0)$ in $\mathbb{R} \times \mathbb{R}$ that is contained completely in $U$. Therefore, $U$ is not an open set and $F(x \times y)$ is not continuous.</p>

<p>From the above analysis, some lessons can be learned.</p>

<ol>
  <li>Pure analysis can be made and general conclusions can be obtained before entering into the real world with a solid example.</li>
  <li>A tangible counter example is a sound proof for negation of a proposition. Just one is enough!</li>
</ol>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="topology" /><summary type="html"><![CDATA[Theorem 18.4 in James Munkres “Topology” states that if a function $f : A \rightarrow X \times Y$ is continuous, its coordinate functions $f_1 : A \rightarrow X$ and $f_2 : A \rightarrow Y$ are also continuous, and the converse is also true. This is what we have been familiar with, such as a continuous parametric curve $f: [0, 1] \rightarrow \mathbb{R}^3$ defined as $f(t) = (x(t), y(t), z(t))^T$ with its three components being continuous. However, if a function $g: A \times B \rightarrow X$ is separately continuous in each of its components, i.e. both $g_1: A \rightarrow X$ and $g_2 : B \rightarrow X$ are continuous, $g$ is not necessarily continuous.]]></summary></entry><entry><title type="html">Understanding of continuity definition in topology</title><link href="https://jihuan-tian.github.io/math/2018/12/15/concept-of-continuity-in-topology.html" rel="alternate" type="text/html" title="Understanding of continuity definition in topology" /><published>2018-12-15T00:00:00+08:00</published><updated>2018-12-15T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2018/12/15/concept-of-continuity-in-topology</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2018/12/15/concept-of-continuity-in-topology.html"><![CDATA[<p>When we learn calculus in university as freshmen, we are usually force-fed with the \(\epsilon-\delta\) language for the definition of a function’s continuity, i.e.</p>

<blockquote>
  <p>A function \(f: A \rightarrow \mathbb{R}\) with \(A \subseteq \mathbb{R}\) as its domain is continuous at \(x_0 \in A\) if for all \(\epsilon &gt; 0\), there exists a \(\delta &gt; 0\) such that whenever \(x \in A\) and \(\left\vert x - x_0 \right\vert &lt; \delta\), there is \(\vert f(x) - f(x_0) \vert &lt; \epsilon\).</p>
</blockquote>

<p>Although the abstract and formal \(\epsilon-\delta\) language is not easy to get used to at first, the idea embodied in the definition is obvious: any small amount of deviation or error around \(f(x_0)\) is procurable by making a corresponding perturbation about \(x_0\), so that the function does not have significant <em>jump</em> at \(x_0\) and is thus considered to be continuous.</p>

<p>In topology, the definition of continuity is based on open sets, which is more abstract compared to the above tangible version, i.e.</p>

<blockquote>
  <p>Let \(X\) and \(Y\) be topological spaces. \(f: X \rightarrow Y\) is a continuous function if for all open set \(V\) in \(Y\), its pre-image \(U = f^{-1}(V)\) is open in \(X\). For a point \(x_0 \in X\) and each neighborhood \(V\) of \(f(x_0)\), there is a neighborhood \(U\) of \(x_0\) such that \(f(U) \subset V\), we say the function is continuous at \(x_0\).</p>
</blockquote>

<p>At first glance, this definition seems merely a product of a mathematician’s endowment, which just works and needs no psychological acceptance. However, it is never a good luck by chance, but requires an understanding with profundity and discernment, which explores the topological essence underpinning the metric space \(\mathbb{R}\).</p>

<p>Take the definition of sequence convergence as an analogy. Its definition in the normal metric space \(\mathbb{R}\), which we are familiar with, is based on the absolute value of real numbers for measuring point proximity, while its definition in a general topological space involves only point inclusion in open sets, i.e.</p>

<blockquote>
  <p>Let \(X\) be a topological space  and \(\{x_n\}_{n \geq 1}\) be a sequence in \(X\). We say \(\{x_n\}_{n \geq 1}\) converges to a point \(x_0\) in \(X\) if for any neighborhood \(U\) of \(x_0\), there exists a \(N\) in \(\mathbb{Z}_+\), such that when \(n &gt; N\), \(x_n\) belongs to \(U\).</p>
</blockquote>

<p>From this it can be seen that the notion of metric or <em>distance</em> is discarded in the topological space, where the <em>rulers</em> for measuring point proximity degenerates to a collection of open sets. Because set inclusion relation establishes a partial order on this open set collection, the meaning of metric or <em>distance</em> is still kept to some extent. Assume that we select a collection of nested open sets (forming a chain in the order relation) as rulers to measure point convergence. The smaller the open set used to circumscribe a segment of the sequence \(\{x_n\}_{n \geq 1}\), the <em>closer</em> they approach the limiting point \(x_0\). This concept is illustrated below.</p>

<p align="center">
<img src="/figures/2018-12-15 Convergence of sequence in topological space.png" width="300px" /></p>
<p align="center">Fig. Convergence of a sequence of points in topological space.</p>

<p>Similarly, for the definition of function continuity in pure topological spaces, the only tool for measuring the amount of deviation or error about a point \(x_0\) and its function value \(f(x_0)\) is using open sets. The smaller the open set, the finer the measuring resolution. If the function value can be limited within any neighborhood of \(f(x_0)\) by confining the variation of \(x\) around \(x_0\) in \(X\), we can say that the set of rulers selected from \(Y\), viz. its topology, has tried its best to ensure the function’s continuity at \(x_0\). It is easy to project that the finer the topology of \(Y\), the more difficult the function to be continuous, due to the improved resolution of the <em>rulers</em>.</p>

<p>Then, looking back at the definition of function continuity in metric space \(\mathbb{R}\), the basis elements of its topology can be infinitesimally small, i.e. \((a, b)\) can have a length approaching zero. Therefore, the continuous functions we meet in college calculus are actually defined in a very strict sense and it’s no doubt that they exhibit elegant silhouettes. Meanwhile, we also see that the continuity of a function not only relies on its rule of assignment, but also on the adopted topologies.</p>

<h2 id="equivalence-between-the-two-definitions">Equivalence between the two definitions</h2>

<p>After clarifying the concepts of function continuity, we will show the equivalence between its two definitions, the \(\epsilon-\delta\) version and the open set version.</p>

<h3 id="open-set-version-longrightarrow-epsilon-delta-version">Open set version \(\Longrightarrow\) \(\epsilon-\delta\) version</h3>

<p>Let \(y = f(x)\) and \(y_0 = f(x_0)\). The condition \(\vert y - y_0 \vert &lt; \epsilon\) forms an open interval in \(Y = \mathbb{R}\), i.e. \((y_0 - \epsilon, y_0 + \epsilon)\) for any \(\epsilon &gt; 0\), which is a basis element of the metric topology on \(Y\) (and also of its order topology). Then, according to the open set version, \(f^{-1} ((y_0 - \epsilon, y_0 + \epsilon))\) is open in \(X\). Because \(y_0 \in (y_0 - \epsilon, y_0 + \epsilon)\), \(x_0 \in f^{-1}((y_0 - \epsilon, y_0 + \epsilon))\). Then there exists a basis element \((a, b)\) around \(x_0\), such that \((a, b) \subset f^{-1}((y_0 - \epsilon, y_0 + \epsilon))\). By letting \(\delta = \min\{x_0 - a, b - x_0\}\), we have \(x \in (x_0 - \delta, x_0 + \delta)\), i.e. \(\vert x - x_0 \vert &lt; \delta\) ensuring \(\vert y - y_0 \vert &lt; \epsilon\).</p>

<h3 id="epsilon-delta-version-longrightarrow-open-set-version">\(\epsilon-\delta\) version \(\Longrightarrow\) open set version</h3>

<p>Let \(V\) be an open set in \(Y = \mathbb{R}\) assigned with the metric topology.Then for all \(y_0 \in V\), there exists an open interval \((c, d)\) containing \(y_0\), such that \((c, d) \subset V\). Let \(\epsilon = \min\{y_0 - c, d - y_0\}\), we have \(y_0 \in (y_0 - \epsilon, y_0 + \epsilon) \subset V\). According to the given \(\epsilon-\delta\) version, there exists a \(\delta &gt; 0\) such that when \({\rm dist}_X (x, x_0) = \vert x - x_0 \vert &lt; \delta\), \({\rm dist}_Y(y, y_0) = \vert y - y_0 \vert &lt; \epsilon\). It should be noted here that because \(f\) may not be injective, there could be more than one element in \(f^{-1}(y_0)\). Then the above \(\epsilon-\delta\) condition holds for any \(x_0\) selected from \(f^{-1}(y_0)\).</p>

<p>Because \(y_0\) is arbitrary in \(V\) and \(x_0\) is arbitrary in \(f^{-1}(y_0)\), taking the union of all such open intervals \((x_0 - \delta, x_0 + \delta)\) will produce \(f^{-1}(V)\), which is also an open set in \(X\).</p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="topology" /><summary type="html"><![CDATA[When we learn calculus in university as freshmen, we are usually force-fed with the \(\epsilon-\delta\) language for the definition of a function’s continuity, i.e.]]></summary></entry><entry><title type="html">Theorem 16.3 in James Munkres Topology</title><link href="https://jihuan-tian.github.io/math/2018/12/13/munkres-topology-theo16-3.html" rel="alternate" type="text/html" title="Theorem 16.3 in James Munkres Topology" /><published>2018-12-13T00:00:00+08:00</published><updated>2018-12-13T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2018/12/13/munkres-topology-theo16-3</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2018/12/13/munkres-topology-theo16-3.html"><![CDATA[<p><strong>Theorem 16.3</strong> If \(A\) is a subspace of \(X\) and \(B\) is a subspace of \(Y\), then the product  topology on \(A \times B\) is the same as the topology \(A \times B\) inherits as a subspace of \(X \times Y\).</p>

<p><strong>Comment:</strong> To prove the identity of two topologies, we can either show they mutually contain each other or prove the equivalence of their bases. Because a topological basis has smaller number of elements or cardinality than the corresponding topology, proof via basis is more efficient.</p>

<p><strong>Proof:</strong> Let \(\mathcal{C}\) be the topological basis of \(X\) and \(\mathcal{D}\) be the basis of \(Y\). Because \(A \subset X\) and \(B \subset Y\), the subspace topological bases of them are \(\mathcal{B}_A = \{C \cap A \vert \forall C \in \mathcal{C} \}\) and \(\mathcal{B}_B = \{D \cap B \vert \forall D \in \mathcal{D} \}\) respectively according to Lemma 16.1.</p>

<p>Due to Lemma 15.1, the basis of the product topology on \(A \times B\) is</p>

\[\mathcal{B}_{A \times B} = \{ (C \cap A) \times (D \cap B) \vert \forall C \in \mathcal{C}, \forall D \in \mathcal{D} \}.\]

<p>Meanwhile, the basis of the product topology on \(X \times Y\) is</p>

\[\mathcal{B}_{X \times Y} = \{ C \times D \vert \forall C \in \mathcal{C}, \forall D \in \mathcal{D} \}.\]

<p>Restricting \(\mathcal{B}_{X \times Y}\) to the subset \(A \times B\), the basis of the subspace topology on \(A \times B\) is</p>

\[\begin{aligned}
\tilde{\mathcal{B}}_{A \times B} &amp;= \{ (C \times D) \cap (A \times B) \vert \forall C \in \mathcal{C}, \forall D \in \mathcal{D} \} \\
&amp;= \{ (C \cap A) \times (D \cap B) \vert \forall C \in \mathcal{C}, \forall D \in \mathcal{D} \},
\end{aligned}\]

<p>which is the same as that of the product topology on \(A \times B\). Hence, this theorem is proved.</p>

<p>The above process of proof can be illustrated as below.</p>

<p><img src="/figures/15447141961713.jpg" alt="" /></p>

<p><strong>Remark:</strong> The above two routes for generating topology on \(A \times B\) must lead to the same result, otherwise, the theory itself is inappropriately proposed. A theory must be at least self-consistent before its debut in reality.</p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="topology" /><summary type="html"><![CDATA[Theorem 16.3 If \(A\) is a subspace of \(X\) and \(B\) is a subspace of \(Y\), then the product topology on \(A \times B\) is the same as the topology \(A \times B\) inherits as a subspace of \(X \times Y\).]]></summary></entry><entry><title type="html">Barber paradox</title><link href="https://jihuan-tian.github.io/math/2018/12/12/barber-paradox.html" rel="alternate" type="text/html" title="Barber paradox" /><published>2018-12-12T00:00:00+08:00</published><updated>2018-12-12T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2018/12/12/barber-paradox</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2018/12/12/barber-paradox.html"><![CDATA[<p>According to <a href="https://en.wikipedia.org/wiki/Barber_paradox?wprov=sfti1">Wikipedia</a>, the well known barber paradox states like this:</p>

<blockquote>
  <p>The barber is the “one who shaves all those, and those only, who do not shave themselves.” The question is, does the barber shave himself?</p>
</blockquote>

<p>Actually, this paradox is directly related to the second part of Theorem 7.8 in James Munkres “Topology”. This theorem says:</p>

<blockquote>
  <p>Let $A$ be a set. There is no injective map $f: \mathcal{P}(A) \rightarrow A$, and there is no surjective map $g: A \rightarrow \mathcal{P}(A)$.</p>
</blockquote>

<p>Here $\mathcal{P}(A)$ represents the power set of $A$.</p>

<p>Mapped to the barber paradox, this theorem can be dissected as below:</p>

<p>Let the set $A$ represent all the people involved in the paradox. Let $a$ be any one of the barbers and the surjective map $g$ associate $a$ with a group of people $C \in \mathcal{P}(A)$, who do not shave themselves and are $a$’s customers. Then, let $B$ be a subset of $A$ including all the barbers. Because $g$ is surjective, this group of barbers $B$ must also have its own pre-image, which is a singleton ${a_0}$ in $A$. According to the definition of $g$, all the barbers in group $B$ do not shave themselves and the only people $a_0$ in the singleton is also a barber who provides service to all barbers in $B$. And here we have the paradox: on one hand, because the barber $a_0$ belongs to the subset $B$ so $a_0$ does not shave himself; on the other hand, the rule of assignment for the surjective map $g$ ensures $a_0$ really shaves himself.</p>

<p>Although we have an unsolvable paradox here, there is no need to bear any qualms. In reality, the barbers in $B$ do not need a high-level <strong>barber’s barber</strong> or a barber from another city as the $a_0$. They can simply provide mutual help to each other.</p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="set-theory" /><summary type="html"><![CDATA[According to Wikipedia, the well known barber paradox states like this:]]></summary></entry><entry><title type="html">Metaphor of topological basis and open set</title><link href="https://jihuan-tian.github.io/math/2018/12/03/metaphor-of-topological-basis-and-open-set.html" rel="alternate" type="text/html" title="Metaphor of topological basis and open set" /><published>2018-12-03T00:00:00+08:00</published><updated>2018-12-03T00:00:00+08:00</updated><id>https://jihuan-tian.github.io/math/2018/12/03/metaphor-of-topological-basis-and-open-set</id><content type="html" xml:base="https://jihuan-tian.github.io/math/2018/12/03/metaphor-of-topological-basis-and-open-set.html"><![CDATA[<p>The definition of topological basis for a space $X$ requires that each point $x$ in $X$ is contained in one of the said topological bases. Meanwhile, the definition of open set also claims that a topological basis can be <em>drawn</em> around each point of the set. Because the topological basis itself is also an open set, such operation of drawing <em>neighborhood</em> around each point can be continued forever.</p>

<p>The abstract concept on topological basis and open set can be likened to throwing pebbles into a water pond without physical boundary, so that no matter where a pebble is dropped into the pond, there will be ripples produced, which are embodied as concentric rings. Then, an open set is similar to such a pond without boundary and ripples in the form of concentric rings are just those local topological bases drawn around the pebble’s incidence point. An example of such concentric rings is the usually adopted topological bases for $\mathbb{R}^n$ which are defined as open balls.</p>

<p>Another point to be noted is since the concept of metric has not come into play yet, the said “without boundary” implies some kind of infinity, which does not mean a distance with infinite length, but drawing local topological bases one inside another <em>ad infinitum</em>. In addition, because such kind of never-stopping self-nesting operation can be applied to each point in an open set, some sense of continuity can bud from here. And it is natural to see that the definition of continuity relies on open set.</p>

<p><img src="/figures/Throwing pebbles into a pond.png" alt="" /></p>]]></content><author><name>Jihuan Tian</name></author><category term="math" /><category term="topology" /><summary type="html"><![CDATA[The definition of topological basis for a space $X$ requires that each point $x$ in $X$ is contained in one of the said topological bases. Meanwhile, the definition of open set also claims that a topological basis can be drawn around each point of the set. Because the topological basis itself is also an open set, such operation of drawing neighborhood around each point can be continued forever.]]></summary></entry></feed>