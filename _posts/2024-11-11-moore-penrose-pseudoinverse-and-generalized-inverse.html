---
layout: post
title: Moore-Penrose pseudoinverse and generalized inverse
date: 2024-11-11
categories: [math]
tags: [BEM]
mathjax: true
custom_css: /assets/css/make4ht.css
---

<!-- l. 20 --><p class='noindent'>When a boundary integral operator \(B\) in BEM to be used as a preconditioner is not elliptic on its <span class='p1xb-x-x-109'>whole </span>domain,
such as the hypersingular operator \(D\), generalized inverse operator \(\dot {B}^{-1}\) is needed (at least theoretically), which is
spectrally equivalent to the original operator \(A\). In (<a href='#XSteinbachConstruction1998'>Steinbach and Wendland</a>), the preconditioning operator
<span class='footnote-mark'><a href='#fn1x0' id='fn1x0-bk'><sup class='textsuperscript'>1</sup></a></span><a id='x1-2f1'></a> is \(B: H^{s-2\alpha }(\Gamma ) \rightarrow H^s(\Gamma )\). Its
generalized inverse is \begin{equation}  \dot {B}^{-1}: V^{s,0}(\Gamma ,B) \rightarrow V^{s-2\alpha ,0}(\Gamma ,B).  \end{equation}<a id='x1-4r1'></a>
</p><!-- l. 25 --><p class='indent'>   Because the generalized inverse is an extension of the Moore-Penrose pseudoinverse, we’ll first introduce the
latter concept. We’ve already met pseudoinverse matrices in linear algebra. For a matrix equation \(Ax=b\), when \(A\) has full
column rank, it has a unique Moore-Penrose pseudoinverse matrix \begin{equation}  A^{\dagger } = (A^{\ast }A)^{-1}A^{\ast },  \end{equation}<a id='x1-5r2'></a> where \(A^{\ast }\) is the Hermite transpose of \(A\). \(A^{\dagger }\) satisfies
the four Penrose conditions (<a href='#XWangGeneralized2018'>Wang et al.</a>):
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-7x1'>
     <!-- l. 31 --><p class='noindent'>\(AA^{\dagger }A=A\)
     </p></li>
<li class='enumerate' id='x1-9x2'>
     <!-- l. 32 --><p class='noindent'>\(A^{\dagger }AA^{\dagger }=A^{\dagger }\)
     </p></li>
<li class='enumerate' id='x1-11x3'>
     <!-- l. 33 --><p class='noindent'>\((AA^{\dagger })^{\ast }=AA^{\dagger }\)
     </p></li>
<li class='enumerate' id='x1-13x4'>
     <!-- l. 34 --><p class='noindent'>\((A^{\dagger }A)^{\ast }=A^{\dagger }A\)</p></li></ol>
<!-- l. 36 --><p class='noindent'>From these conditions, we know that \(A^{\dagger }\) is just the left inverse of \(A\). According to our previous knowledge about the
<a href='{% post_url 2024-11-03-kernel-and-range-of-a-matrix-and-its-transpose %}'>kernel and range spaces of a matrix</a>, when the matrix has full column rank, it is an injective map which should
have a left inverse.
</p><!-- l. 38 --><p class='indent'>   The basic idea behind Moore-Penrose pseudoinverse is simple. Assume \(A\) maps from \(V\) to \(W\). Let \(y\) belongs to \(W\) and
we want to find its pre-image \(x\) in \(V\) in the sense of pseudoinverse. When \(\mathrm {ker}(A)\) is not \(\{ 0 \}\), \(A\) is not surjective. So we first apply \(A^{\ast }\)
to \(y\), which maps \(y\) back into \(( \mathrm {ker}(A) )^{\perp }\). In this smaller subspace of \(V\), \(A^{\ast }A\) is bijective and the pre-image of \(A^{\ast }y\) can be found by
applying its inverse.
</p><!-- l. 40 --><p class='indent'>   Before the study on matrix pseudoinverse by Penrose, there had been research on the generalized inverse
of integral or differential operators by Hilbert, Fredholm et al. Let \(A\) be a bounded linear operator
from Hilbert space \(V\) to \(W\). The operator equation is \(Ax=b\), where \(x\in V\) and \(b\in W\). If the range \(\mathrm {Im}(A)\) of \(A\) is closed in \(W\), the
following generalized solutions are equivalent (<a href='#XWangGeneralized2018'>Wang et al.</a>), which are called the least square
solution:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-15x1'>
                                                                                               
                                                                                               
     <!-- l. 42 --><p class='noindent'>\(Ax=Pb\), where \(P\) is the projection operator maps onto \(\mathrm {Im}(A)\).
     </p></li>
<li class='enumerate' id='x1-17x2'>
     <!-- l. 43 --><p class='noindent'>\(\argmin _{x\in V} \lVert Ax-b \rVert _{W}\).
     </p></li>
<li class='enumerate' id='x1-19x3'>
     <!-- l. 44 --><p class='noindent'>\(A^{\ast }Ax=A^{\ast }b\).</p></li></ol>
<!-- l. 47 --><p class='indent'>   If we loosen the condition by assuming \(V\) and \(W\) are Banach spaces instead of Hilbert spaces, according to the
closed range theorem in (<a href='#XSteinbachNumerical2007'>Steinbach</a>, page 48), when \(A\) has a closed range, \(\mathrm {Im}(A)\) is the annihilator of the kernel of the
adjoint operator \(A': W' \rightarrow V'\), i.e. \begin{equation}  \mathrm {Im}(A)=(\mathrm {ker}(A'))^{\circ },  \end{equation}<a id='x1-20r3'></a> and for any \(y\in \mathrm {Im}(A)\) and \(x\in \mathrm {ker}(A')\), the duality pairing \(\langle y,x \rangle \) is zero. Because there are no inner product structures
on \(V\) and \(W\), we do not have the concepts of orthogonal complement space and Hilbert-adjoint anymore. Then the
above Moore-Penrose pseudoinverse cannot be used. But still the domain \(V\) of \(A\) can be decomposed as \begin{equation}  V = \mathrm {ker}(A) \oplus Z,  \end{equation}<a id='x1-21r4'></a> where \(Z\) is a
closed subspace of \(V\) such that \(\mathrm {ker}(A) \cap Z = \{ 0 \}\). If we restrict the domain of \(A\) to \(Z\), the map \(A\big \vert _Z: Z \rightarrow \mathrm {Im}(A)\) is bijective, which of course has an
inverse. If the range space \(W\) of \(A\) is decomposed as \begin{equation}  W = \mathrm {Im}(A) \oplus Y = (\mathrm {ker}(A'))^{\circ } \oplus Y,  \end{equation}<a id='x1-22r5'></a> the generalized inverse \(A^+\) of \(A\) can be defined as \begin{equation}  A^{+}(y) = \begin {cases} A\big \vert _Z^{-1}(y) &amp; y\in \mathrm {Im}(A) = (\mathrm {ker}(A'))^{\circ } \\ 0 &amp; y\in Y \end {cases}.  \end{equation}<a id='x1-23r6'></a> It
is easy to know that such generalized pseudoinverse only satisfies the first two Moore-Penrose
conditions:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-25x1'>
     <!-- l. 69 --><p class='noindent'>\(AA^{+}A=A\)
     </p></li>
<li class='enumerate' id='x1-27x2'>
     <!-- l. 70 --><p class='noindent'>\(A^{+}AA^{+}=A^{+}\)</p></li></ol>
   <h3 class='likesectionHead'><a id='x1-1000'></a>References</h3>
<!-- l. 1 --><p class='noindent'>
  </p><div class='thebibliography'>
  <p class='bibitem'><span class='biblabel'>
<a id='XSteinbachNumerical2007'></a><span class='bibsp'>   </span></span>Olaf  Steinbach.    <span class='p1xi-x-x-109'>Numerical  Approximation  Methods  for  Elliptic  Boundary  Value  Problems:  Finite  and
  </span><span class='p1xi-x-x-109'>Boundary Elements</span>. Springer Science &amp; Business Media. ISBN 978-0-387-31312-2.
  </p>
  <p class='bibitem'><span class='biblabel'>
<a id='XSteinbachConstruction1998'></a><span class='bibsp'>   </span></span>Olaf               Steinbach               and               Wolfgang L.               Wendland.                                      The
  construction of some efficient preconditioners in the boundary element method.  9(1-2):191–216.  URL
  <a class='url' href='http://link.springer.com/article/10.1023/A:1018937506719'><span class='t1xtt-x-x-109'>http://link.springer.com/article/10.1023/A:1018937506719</span></a>.
                                                                                               
                                                                                               
  </p>
  <p class='bibitem'><span class='biblabel'>
<a id='XWangGeneralized2018'></a><span class='bibsp'>   </span></span>Wang,  Yimin  Wei,  and  Sanzheng  Qiao.   <span class='p1xi-x-x-109'>Generalized  Inverses:  Theory  and  Computations</span>,  volume 53
  of  <span class='p1xi-x-x-109'>Developments  in  Mathematics</span>.     Springer.     ISBN  9789811301452  9789811301469.     doi:  10.1007/
  978-981-13-0146-9.
</p>
  </div>
   <div class='footnotes'><a id='x1-3x'></a>
<!-- l. 20 --><p class='indent'>      <span class='footnote-mark'><a href='#fn1x0-bk' id='fn1x0'><sup class='textsuperscript'>1</sup></a></span><span class='p1xr-x-x-90'>Here we explicitly say “preconditioning operator” not simply “preconditioner”, because we want to distinguish it from
</span><span class='p1xr-x-x-90'>“preconditioning matrix”. While a preconditioning operator such as </span>\(B\) <span class='p1xr-x-x-90'>is an approximate inverse of the original operator </span>\(A\)<span class='p1xr-x-x-90'>, a
</span><span class='p1xr-x-x-90'>preconditioning matrix is the discretized Galerkin matrix associated with </span>\(\dot {B}^{-1}\)<span class='p1xr-x-x-90'>, not </span>\(B\)<span class='p1xr-x-x-90'>. To apply a preconditioning matrix to a discretized linear
</span><span class='p1xr-x-x-90'>system, we need to multiply its approximate inverse matrix to both sides of the equation. For simplicity, we will say “preconditioner”
</span><span class='p1xr-x-x-90'>instead of “preconditioning operator” from now on.</span></p>                                                                                                                                  </div>

{{ "2024-11-11-moore-penrose-pseudoinverse-and-generalized-inverse" | backlink }}
